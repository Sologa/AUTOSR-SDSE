{
  "query_title": "{{LauraGPT}: Listen, attend, understand, and regenerate audio with {GPT}}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/l+BkdmISCLe5lh0eUjw6ufS7vtk</id>\n  <title>arXiv Query: search_query=ti:\"lauragpt listen attend understand and regenerate audio with gpt\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:22:55Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22lauragpt+listen+attend+understand+and+regenerate+audio+with+gpt%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2310.04673v4</id>\n    <title>LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT</title>\n    <updated>2024-07-03T02:38:03Z</updated>\n    <link href=\"https://arxiv.org/abs/2310.04673v4\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2310.04673v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.</summary>\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2023-10-07T03:17:59Z</published>\n    <arxiv:comment>10 pages, work in progress</arxiv:comment>\n    <arxiv:primary_category term=\"cs.SD\"/>\n    <author>\n      <name>Zhihao Du</name>\n    </author>\n    <author>\n      <name>Jiaming Wang</name>\n    </author>\n    <author>\n      <name>Qian Chen</name>\n    </author>\n    <author>\n      <name>Yunfei Chu</name>\n    </author>\n    <author>\n      <name>Zhifu Gao</name>\n    </author>\n    <author>\n      <name>Zerui Li</name>\n    </author>\n    <author>\n      <name>Kai Hu</name>\n    </author>\n    <author>\n      <name>Xiaohuan Zhou</name>\n    </author>\n    <author>\n      <name>Jin Xu</name>\n    </author>\n    <author>\n      <name>Ziyang Ma</name>\n    </author>\n    <author>\n      <name>Wen Wang</name>\n    </author>\n    <author>\n      <name>Siqi Zheng</name>\n    </author>\n    <author>\n      <name>Chang Zhou</name>\n    </author>\n    <author>\n      <name>Zhijie Yan</name>\n    </author>\n    <author>\n      <name>Shiliang Zhang</name>\n    </author>\n  </entry>\n</feed>\n"
}