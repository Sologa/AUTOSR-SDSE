{
  "query_title": "{Addressing Index Collapse of Large-Codebook Speech Tokenizer With Dual-Decoding Product-Quantized Variational Auto-Encoder}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/L3oFXr9L1aBDwJXSTrXYXy8Et/A</id>\n  <title>arXiv Query: search_query=ti:\"addressing index collapse of large codebook speech tokenizer with dual decoding product quantized variational auto encoder\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:21:46Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22addressing+index+collapse+of+large+codebook+speech+tokenizer+with+dual+decoding+product+quantized+variational+auto+encoder%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2406.02940v1</id>\n    <title>Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder</title>\n    <updated>2024-06-05T04:54:49Z</updated>\n    <link href=\"https://arxiv.org/abs/2406.02940v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2406.02940v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.</summary>\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2024-06-05T04:54:49Z</published>\n    <arxiv:primary_category term=\"cs.SD\"/>\n    <author>\n      <name>Haohan Guo</name>\n    </author>\n    <author>\n      <name>Fenglong Xie</name>\n    </author>\n    <author>\n      <name>Dongchao Yang</name>\n    </author>\n    <author>\n      <name>Hui Lu</name>\n    </author>\n    <author>\n      <name>Xixin Wu</name>\n    </author>\n    <author>\n      <name>Helen Meng</name>\n    </author>\n  </entry>\n</feed>\n"
}