{
  "query_title": "{UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/kAxgHMZhnta+VIkpwey7Dxb/CSI</id>\n  <title>arXiv Query: search_query=ti:\"unity two pass direct speech to speech translation with discrete units\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:24:15Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22unity+two+pass+direct+speech+to+speech+translation+with+discrete+units%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2212.08055v2</id>\n    <title>UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units</title>\n    <updated>2023-05-26T16:07:54Z</updated>\n    <link href=\"https://arxiv.org/abs/2212.08055v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2212.08055v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2022-12-15T18:58:28Z</published>\n    <arxiv:comment>ACL 2023 (main conference)</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Hirofumi Inaguma</name>\n    </author>\n    <author>\n      <name>Sravya Popuri</name>\n    </author>\n    <author>\n      <name>Ilia Kulikov</name>\n    </author>\n    <author>\n      <name>Peng-Jen Chen</name>\n    </author>\n    <author>\n      <name>Changhan Wang</name>\n    </author>\n    <author>\n      <name>Yu-An Chung</name>\n    </author>\n    <author>\n      <name>Yun Tang</name>\n    </author>\n    <author>\n      <name>Ann Lee</name>\n    </author>\n    <author>\n      <name>Shinji Watanabe</name>\n    </author>\n    <author>\n      <name>Juan Pino</name>\n    </author>\n  </entry>\n</feed>\n"
}