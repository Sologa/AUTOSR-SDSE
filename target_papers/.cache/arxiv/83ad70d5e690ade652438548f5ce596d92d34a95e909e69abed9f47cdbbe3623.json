{
  "query_title": "{Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/OSojtgrY19lGffLBrICadmxZKrY</id>\n  <title>arXiv Query: search_query=ti:\"towards efficient speech text jointly decoding within one speech language model\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:22:31Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22towards+efficient+speech+text+jointly+decoding+within+one+speech+language+model%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2506.04518v2</id>\n    <title>Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model</title>\n    <updated>2025-06-13T03:55:18Z</updated>\n    <link href=\"https://arxiv.org/abs/2506.04518v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2506.04518v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.</summary>\n    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-06-04T23:53:49Z</published>\n    <arxiv:comment>Our company need to do internal review</arxiv:comment>\n    <arxiv:primary_category term=\"eess.AS\"/>\n    <author>\n      <name>Haibin Wu</name>\n    </author>\n    <author>\n      <name>Yuxuan Hu</name>\n    </author>\n    <author>\n      <name>Ruchao Fan</name>\n    </author>\n    <author>\n      <name>Xiaofei Wang</name>\n    </author>\n    <author>\n      <name>Kenichi Kumatani</name>\n    </author>\n    <author>\n      <name>Bo Ren</name>\n    </author>\n    <author>\n      <name>Jianwei Yu</name>\n    </author>\n    <author>\n      <name>Heng Lu</name>\n    </author>\n    <author>\n      <name>Lijuan Wang</name>\n    </author>\n    <author>\n      <name>Yao Qian</name>\n    </author>\n    <author>\n      <name>Jinyu Li</name>\n    </author>\n  </entry>\n</feed>\n"
}