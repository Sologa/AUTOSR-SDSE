{
  "query_title": "{{SpeechTokenizer}: Unified speech tokenizer for speech large language models}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/GKjQLHh/BWYTEaHbhuMU1+UTZY4</id>\n  <title>arXiv Query: search_query=ti:\"speechtokenizer unified speech tokenizer for speech large language models\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:22:35Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22speechtokenizer+unified+speech+tokenizer+for+speech+large+language+models%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2308.16692v2</id>\n    <title>SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models</title>\n    <updated>2024-01-23T01:56:57Z</updated>\n    <link href=\"https://arxiv.org/abs/2308.16692v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2308.16692v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2023-08-31T12:53:09Z</published>\n    <arxiv:comment>Accepted by ICLR 2024. Project page is at https://0nutation.github.io/SpeechTokenizer.github.io/</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Xin Zhang</name>\n    </author>\n    <author>\n      <name>Dong Zhang</name>\n    </author>\n    <author>\n      <name>Shimin Li</name>\n    </author>\n    <author>\n      <name>Yaqian Zhou</name>\n    </author>\n    <author>\n      <name>Xipeng Qiu</name>\n    </author>\n  </entry>\n</feed>\n"
}