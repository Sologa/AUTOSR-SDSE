{
  "query_title": "{{AudioLM}: A Language Modeling Approach to Audio Generation}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/1xYVr/iFTpqR8aRJHA8ipy021Js</id>\n  <title>arXiv Query: search_query=ti:\"audiolm a language modeling approach to audio generation\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:22:44Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22audiolm+a+language+modeling+approach+to+audio+generation%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2209.03143v2</id>\n    <title>AudioLM: a Language Modeling Approach to Audio Generation</title>\n    <updated>2023-07-26T03:52:36Z</updated>\n    <link href=\"https://arxiv.org/abs/2209.03143v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2209.03143v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.</summary>\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2022-09-07T13:40:08Z</published>\n    <arxiv:primary_category term=\"cs.SD\"/>\n    <author>\n      <name>Zal\u00e1n Borsos</name>\n    </author>\n    <author>\n      <name>Rapha\u00ebl Marinier</name>\n    </author>\n    <author>\n      <name>Damien Vincent</name>\n    </author>\n    <author>\n      <name>Eugene Kharitonov</name>\n    </author>\n    <author>\n      <name>Olivier Pietquin</name>\n    </author>\n    <author>\n      <name>Matt Sharifi</name>\n    </author>\n    <author>\n      <name>Dominik Roblek</name>\n    </author>\n    <author>\n      <name>Olivier Teboul</name>\n    </author>\n    <author>\n      <name>David Grangier</name>\n    </author>\n    <author>\n      <name>Marco Tagliasacchi</name>\n    </author>\n    <author>\n      <name>Neil Zeghidour</name>\n    </author>\n  </entry>\n</feed>\n"
}