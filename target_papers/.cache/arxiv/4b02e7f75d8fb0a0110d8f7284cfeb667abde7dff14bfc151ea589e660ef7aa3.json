{
  "query_title": "{Neural discrete representation learning}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/GD5sHuDUbAH2CApMPAVi0vse1o8</id>\n  <title>arXiv Query: search_query=ti:\"neural discrete representation learning\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:19:30Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22neural+discrete+representation+learning%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>4</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/1711.00937v2</id>\n    <title>Neural Discrete Representation Learning</title>\n    <updated>2018-05-30T14:58:27Z</updated>\n    <link href=\"https://arxiv.org/abs/1711.00937v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1711.00937v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2017-11-02T21:14:44Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Aaron van den Oord</name>\n    </author>\n    <author>\n      <name>Oriol Vinyals</name>\n    </author>\n    <author>\n      <name>Koray Kavukcuoglu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2302.07950v2</id>\n    <title>Self-Organising Neural Discrete Representation Learning \u00e0 la Kohonen</title>\n    <updated>2024-07-08T19:47:40Z</updated>\n    <link href=\"https://arxiv.org/abs/2302.07950v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2302.07950v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Unsupervised learning of discrete representations in neural networks (NNs) from continuous ones is essential for many modern applications. Vector Quantisation (VQ) has become popular for this, in particular in the context of generative models, such as Variational Auto-Encoders (VAEs), where the exponential moving average-based VQ (EMA-VQ) algorithm is often used. Here, we study an alternative VQ algorithm based on Kohonen's learning rule for the Self-Organising Map (KSOM; 1982). EMA-VQ is a special case of KSOM. KSOM is known to offer two potential benefits: empirically, it converges faster than EMA-VQ, and KSOM-generated discrete representations form a topological structure on the grid whose nodes are the discrete symbols, resulting in an artificial version of the brain's topographic map. We revisit these properties by using KSOM in VQ-VAEs for image processing. In our experiments, the speed-up compared to well-configured EMA-VQ is only observable at the beginning of training, but KSOM is generally much more robust, e.g., w.r.t. the choice of initialisation schemes.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2023-02-15T21:04:04Z</published>\n    <arxiv:comment>Two first authors. Accepted to ICANN 2024. The Version of Record of this contribution is published by Springer. An earlier version was presented at ICML 2023 Workshop on Sampling and Optimization in Discrete Space (SODS)</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Kazuki Irie</name>\n    </author>\n    <author>\n      <name>R\u00f3bert Csord\u00e1s</name>\n    </author>\n    <author>\n      <name>J\u00fcrgen Schmidhuber</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2310.13344v2</id>\n    <title>DeepFracture: A Generative Approach for Predicting Brittle Fractures with Neural Discrete Representation Learning</title>\n    <updated>2025-02-20T07:06:03Z</updated>\n    <link href=\"https://arxiv.org/abs/2310.13344v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2310.13344v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>In the field of brittle fracture animation, generating realistic destruction animations using physics-based simulation methods is computationally expensive. While techniques based on Voronoi diagrams or pre-fractured patterns are effective for real-time applications, they fail to incorporate collision conditions when determining fractured shapes during runtime. This paper introduces a novel learning-based approach for predicting fractured shapes based on collision dynamics at runtime. Our approach seamlessly integrates realistic brittle fracture animations with rigid body simulations, utilising boundary element method (BEM) brittle fracture simulations to generate training data. To integrate collision scenarios and fractured shapes into a deep learning framework, we introduce generative geometric segmentation, distinct from both instance and semantic segmentation, to represent 3D fragment shapes. We propose an eight-dimensional latent code to address the challenge of optimising multiple discrete fracture pattern targets that share similar continuous collision latent codes. This code will follow a discrete normal distribution corresponding to a specific fracture pattern within our latent impulse representation design. This adaptation enables the prediction of fractured shapes using neural discrete representation learning. Our experimental results show that our approach generates considerably more detailed brittle fractures than existing techniques, while the computational time is typically reduced compared to traditional simulation methods at comparable resolutions.</summary>\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2023-10-20T08:15:13Z</published>\n    <arxiv:comment>This is a preprint of an article published in the Computer Graphics Forum. The final authenticated version is available at (https://doi.org/10.1111/cgf.70002). Please also check the project page: https://nikoloside.github.io/deepfracture/</arxiv:comment>\n    <arxiv:primary_category term=\"cs.GR\"/>\n    <arxiv:journal_ref>Computer Graphics Forum, 15 pages, e70002, 2025</arxiv:journal_ref>\n    <author>\n      <name>Yuhang Huang</name>\n    </author>\n    <author>\n      <name>Takashi Kanai</name>\n    </author>\n    <arxiv:doi>10.1111/cgf.70002</arxiv:doi>\n    <link rel=\"related\" href=\"https://doi.org/10.1111/cgf.70002\" title=\"doi\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2512.00873v1</id>\n    <title>Neural Discrete Representation Learning for Sparse-View CBCT Reconstruction: From Algorithm Design to Prospective Multicenter Clinical Evaluation</title>\n    <updated>2025-11-30T12:45:02Z</updated>\n    <link href=\"https://arxiv.org/abs/2512.00873v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2512.00873v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Cone beam computed tomography (CBCT)-guided puncture has become an established approach for diagnosing and treating early- to mid-stage thoracic tumours, yet the associated radiation exposure substantially elevates the risk of secondary malignancies. Although multiple low-dose CBCT strategies have been introduced, none have undergone validation using large-scale multicenter retrospective datasets, and prospective clinical evaluation remains lacking. Here, we propose DeepPriorCBCT - a three-stage deep learning framework that achieves diagnostic-grade reconstruction using only one-sixth of the conventional radiation dose. 4102 patients with 8675 CBCT scans from 12 centers were included to develop and validate DeepPriorCBCT. Additionally, a prospective cross-over trial (Registry number: NCT07035977) which recruited 138 patients scheduled for percutaneous thoracic puncture was conducted to assess the model's clinical applicability. Assessment by 11 physicians confirmed that reconstructed images were indistinguishable from original scans. Moreover, diagnostic performance and overall image quality were comparable to those generated by standard reconstruction algorithms. In the prospective trial, five radiologists reported no significant differences in image quality or lesion assessment between DeepPriorCBCT and the clinical standard (all P&gt;0.05). Likewise, 25 interventionalists expressed no preference between model-based and full-sampling images for surgical guidance (Kappa&lt;0.2). Radiation exposure with DeepPriorCBCT was reduced to approximately one-sixth of that with the conventional approach, and collectively, the findings confirm that it enables high-quality CBCT reconstruction under sparse sampling conditions while markedly decreasing intraoperative radiation risk.</summary>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-30T12:45:02Z</published>\n    <arxiv:primary_category term=\"cs.CV\"/>\n    <author>\n      <name>Haoshen Wang</name>\n    </author>\n    <author>\n      <name>Lei Chen</name>\n    </author>\n    <author>\n      <name>Wei-Hua Zhang</name>\n    </author>\n    <author>\n      <name>Linxia Wu</name>\n    </author>\n    <author>\n      <name>Yong Luo</name>\n    </author>\n    <author>\n      <name>Zengmao Wang</name>\n    </author>\n    <author>\n      <name>Yuan Xiong</name>\n    </author>\n    <author>\n      <name>Chengcheng Zhu</name>\n    </author>\n    <author>\n      <name>Wenjuan Tang</name>\n    </author>\n    <author>\n      <name>Xueyi Zhang</name>\n    </author>\n    <author>\n      <name>Wei Zhou</name>\n    </author>\n    <author>\n      <name>Xuhua Duan</name>\n    </author>\n    <author>\n      <name>Lefei Zhang</name>\n    </author>\n    <author>\n      <name>Gao-Jun Teng</name>\n    </author>\n    <author>\n      <name>Bo Du</name>\n    </author>\n    <author>\n      <name>Huangxuan Zhao</name>\n    </author>\n  </entry>\n</feed>\n"
}