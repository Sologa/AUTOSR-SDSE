{
  "query_title": "{Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/Hz7WsMKkktKcLI8UOJDpNonObZE</id>\n  <title>arXiv Query: search_query=ti:\"pre train prompt and predict a systematic survey of prompting methods in natural language processing\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:24:33Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22pre+train+prompt+and+predict+a+systematic+survey+of+prompting+methods+in+natural+language+processing%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2107.13586v1</id>\n    <title>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>\n    <updated>2021-07-28T18:09:46Z</updated>\n    <link href=\"https://arxiv.org/abs/2107.13586v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2107.13586v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub \"prompt-based learning\". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2021-07-28T18:09:46Z</published>\n    <arxiv:comment>Website: http://pretrain.nlpedia.ai/</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Pengfei Liu</name>\n    </author>\n    <author>\n      <name>Weizhe Yuan</name>\n    </author>\n    <author>\n      <name>Jinlan Fu</name>\n    </author>\n    <author>\n      <name>Zhengbao Jiang</name>\n    </author>\n    <author>\n      <name>Hiroaki Hayashi</name>\n    </author>\n    <author>\n      <name>Graham Neubig</name>\n    </author>\n  </entry>\n</feed>\n"
}