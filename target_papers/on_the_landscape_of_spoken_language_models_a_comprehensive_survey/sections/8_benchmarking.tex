\section{Benchmarking and Evaluating \slms}
\label{sec:evaluation}
Like text language models, \slms\ may possess a broad array of capabilities.  In addition (and in contrast to text LMs), they also involve a variety of design choices that differ significantly across models. 
These properties can make it difficult to compare \slms, and more important to assess them 
from multiple angles and training stages.
Below, we outline commonly used evaluation methods and benchmarks,
categorized into three primary subgroups: (1) likelihood-based evaluation metrics; (2) generative metrics; and (3) trustworthiness evaluations. Traditional task-specific speech evaluation methods (e.g., ASR, TTS, ST) are also frequently used to assess \slm\ performance. However, we exclude them from this survey, as they are well-established and extensively covered in the existing literature.


\subsection{Likelihood-Based Evaluation}
\label{ssec:likelihood_eval}
As described above, the general \slm\ architecture
comprises three main components: (1) speech (and optionally text) encoding; (2) sequence modeling of speech, text, or both; and (3) speech and/or text decoding. Likelihood-based evaluation metrics~\citep{dunbar2021zero} are 
designed to assess the sequential speech modeling capabilities of \slms.\footnote{Such evaluation metrics can be considered as an analogue to perplexity (PPL) for text LLMs. Since there is no standard method for speech tokenization, the speech ``vocabulary'' may change between pure \slms. Therefore, PPL cannot be directly applied. Instead, likelihood-based metrics introduce an alternative approach of comparing sequence likelihoods.}
Such evaluation metrics are therefore more suitable for the pre-training stage of pure speech LMs and joint speech+text LMs~\citep{GSLM, twist, audiolm, defossezmoshi}.

It is crucial to emphasize that speech encoding (i.e., speech tokenization), an active area of research~\citep{messica2024nast, turetzky2024last, speechtokenizer}, plays a significant role in shaping speech modeling capabilities and therefore cannot be disregarded. For instance, utilizing tokens generated by a self-supervised learning model such as HuBERT 
significantly outperforms the quantization of log mel spectrograms~\citep{GSLM}. Likewise, employing speech tokenizers with higher compression rates (i.e., sampled at lower frequency in the latent space) yields better performance in speech modeling~\citep{audiolm, twist}. While this subsection discusses speech modeling tasks with a focus on the LM, we believe that in the context of \slms, the research community should evaluate both components collectively rather than treating them as separate entities. 

In likelihood-based evaluations, the pre-trained \slm\ is typically provided with two
input sequences:
one representing a natural speech sequence (positive example) and the other an unnatural sequence relative to a specific property (negative example). We then calculate the percentage of instances where the \slm\ assigns a higher likelihood to the positive example than to the negative one. 
This type of evalution has also been popular in the NLP community until very recently~\citep{zellers2019hellaswag, mostafazadeh2016corpus, touvron2023llama2, team2024gemma, the2024large}. Due to significant model improvements this type of NLP benchmark has saturated and significantly more complex ones have emerged~\citep{jimenez2023swe,bigbench}. 

Metrics in this category are designed to evaluate a model’s capacity to represent various speech characteristics. For example, sWUGGY~\citep{dunbar2021zero} evaluates the lexical capabilities of models by presenting them with pairs of utterances, one consisting of a real word and the other a similar non-word (e.g., `brick' vs.~`blick’), and measuring the model’s ability to assign a higher probability to the real word (where nonwords were obtained from the text WUGGY task~\citep{keuleers2010wuggy}). Similarly, sBLIMP~\citep{dunbar2021zero} (which was adapted from the text-based BLIMP~\citep{warstadt2020blimp}) evaluates the ability of the network to model syntactic properties. The network is presented with a matched pair of grammatically correct and incorrect sentences. \citet{twist} expanded this method to evaluate semantic understanding of spoken text. They generated two spoken adaptations of the StoryCloze text paragraph story completion task~\citep{mostafazadeh2016corpus}. In the first adaptation, they adhered to the original StoryCloze setup, while in the second, they randomly sampled negative examples, resulting in a simplified version primarily %
requiring an understanding of the general topic of the paragraph. ProsAudit~\citep{de2023prosaudit} constructs negative examples by inserting pauses in unnatural positions in utterances to evaluate \slms' sensitivity to speech 
prosody. Lastly,~\citet{maimon2024suite} proposed the SALMon evaluation suite, which, unlike the previously mentioned benchmarks, focuses on a variety of non-lexical acoustic and prosodic elements (e.g., speaker identity, speaker sentiment, background noise and room acoustics)
at two levels of complexity: One measures the \textit{consistency} across time of a given acoustic element, whereas the other measures \textit{alignment} between the acoustic details and the semantic, spoken content. 

\subsection{Generative Metrics} 
Unlike likelihood-based metrics, which assess a model’s capacity to assign higher likelihoods to in-domain samples than out-of-domain ones, generative metrics focus on evaluating the model’s ability to produce meaningful continuations or responses based on a given prompt or instruction. Such evaluation methods can be suitable for all \slm\ types (pure speech LMs, speech+text LMs, or speech-aware text LMs). 

\paragraph{Intrinsic quality metrics} 
 For \slms\ that generate spoken output, the quality of the generated response can be evaluated along several axes: (1) speech quality, using subjective tests or objective metrics such as MOSNet~\citep{lo2019mosnet}; (2) speaker, acoustic, and/or sentiment consistency using human judges or pre-trained classifiers~\citep{nguyen2024spirit}; and (3) quality of the spoken content, evaluated by either comparing to a target response or transcribing the speech and using a judge or text LLM to score the content.

For pure \slms, a common evaluation metric is the \emph{generative perplexity}, in which we transcribe the generated speech and measure its perplexity (PPL) using a pre-trained text LLM~\citep{GSLM}. However, a known problem with this metric is that text LLMs tend to assign high probability to repeated content~\citep{holtzman2020the}. To mitigate this issue, \citet{GSLM} propose the VERT metric, which balances between quality and diversity of the generated response. VERT uses a linear combination of the text PPL and the auto-BLEU metric of within-sentence diversity~\citep{GSLM}.

\paragraph{Task-based evaluation}  For joint speech+text LMs and speech-aware text LMs, a common approach is to evaluate the model's ability to perform question answering and follow instructions. \citet{nachmani2023spoken} proposed two spoken question answering benchmarks: LLaMA-Questions, derived from a text LLM, and 
a synthesized version of the WebQuestions~\citep{berant2013semantic} textual benchmark. \citet{defossezmoshi} built upon this approach and developed a synthesized version of TriviaQA \citep{joshi2017triviaqa}. To evaluate \slm\ responses on these question-answering benchmarks, the generated speech is transcribed and the accuracy is measured against the target answer. \citet{chen2024voicebench} proposed VoiceBench, an extended question-answering dataset consisting of (1) open-ended QA; (2) reference-based QA; (3) multiple-choice QA; and (4) instruction-following evaluations. VoiceBench also evaluates the model's ability to handle background noise and speaker variability. \citet{fang2024llama} proposed to evaluate the instruction-following capabilities of \slms\ using \emph{LLM-as-a-judge} methods. In this approach, an external text LLM rates the quality of the responses considering both content and style.

Recently, there have been several efforts to develop evaluations of speech-aware text LMs on a broader 
range of tasks and instruction data.  In these evaluations, each task
consists of text instructions, speech utterances, and text labels. 
\citet{huang2024dynamic} introduced the Dynamic-SUPERB evaluation suite, designed as a dynamic bechmark allowing for community contributions of tasks, analogously to BIG-bench~\citep{bigbench} for text LMs.
The initial phase of Dynamic-SUPERB focused on classification tasks related to content, speaker characteristics, semantics, degradation, paralinguistics, and non-speech audio information. 
Phase 2 of Dynamic-SUPERB~\citep{huang2025dynamicsuperb} significantly expanded the task set to include regression and sequence generation tasks, making it the largest benchmark for speech and audio evaluation.
\citet{airbench} proposed AIR-Bench, 
which includes both classification and open-ended chat-style questions. 
Both Dynamic-SUPERB Phase-2 and AIR-Bench use
LLM-as-a-judge evaluation. \citet{sakshi2024mmau} introduced MMAU, a benchmark for audio understanding and reasoning using natural language. MMAU includes both information extraction tasks (e.g., ``Which word appears first?'') and reasoning tasks (e.g., ``What are the roles of the first and second speakers in the conversation?''), formulated as multiple-choice questions. 
Finally, AudioBench~\citep{wang2024audiobench} 
includes eight speech and audio tasks 
based on 26 datasets,
focusing on speech understanding (e.g., ASR, spoken QA, speech instruction), audio scene comprehension (e.g., audio captioning, audio scene question
answering), and voice analysis (e.g., accent recognition, gender recognition, emotion recognition).

While the benchmarks above %
include a large variety of tasks,
some recent benchmarks focus specifically on 
speech characteristics that exploit the new abilities of \slms.  For example, StyleTalk~\citep{lin2024advancing} 
assesses \slms' 
ability to generate text responses that align with specific speaking styles, such as intonation and emotion. The evaluation compares the generated output with a target response using standard text generation metrics (BLEU, ROUGE, and others).
Similarly, the E-chat200 dataset~\citep{xue2024chat} %
also assesses a model’s ability to generate responses that align with the speaker’s emotion. The dataset was synthetically created using an expressive TTS, while the text-based questions and responses were generated by an external LLM. 
SD-Eval~\citep{ao2024sd} %
goes beyond emotion alignment to include alignment with paralinguistic features (e.g., accent, age, prosody, timbre, volume) as well as environmental context. 

\subsection{Evaluating Trustworthiness}

The 
benchmarks discussed thus far focus on the \emph{performance} of \slms. 
Next, we consider evaluations that measure their \emph{trustworthiness}. 
Since \slms\ generate word sequences (whether directly as text or within the spoken output), all trustworthiness considerations related to text-based LLMs also apply to \slms. However, in addition to this content information, non-content information in the generated speech creates additional aspects of trustworthiness that need to be considered. 

\subsubsection{Hallucination}
Text-based LLMs 
often %
suffer from hallucinations, or outputs that are inconsistent with the given context or other knowledge considered to be fact.  %
In addition to such content hallucinations, \slms\ may also generate audio hallucinations. 
For example, consider an audio clip without a dog barking. If you ask an \slm\ to describe it, it would most likely not mention anything related to a dog barking. However, if you directly ask the model, ``Is there a dog barking?'' it might answer, ``Yes,'' even though it can provide accurate audio captions when explicitly prompted to do so~\citep{kuan2024largeaudiolanguagemodelstruly}.

\citet{kuan2024largeaudiolanguagemodelstruly} %
study whether \slms\ can accurately perform three types of tasks without hallucination: 
 (1) identify the presence of an object (e.g., ``Is there a dog barking?''), (2) determine the temporal order of sound events (e.g., ``Is the dog barking before someone laughs?''), and (3) recognize the source of a sound event (e.g., ``Who is laughing, a man or a woman?''). %
The main finding is that all of the %
evaluated 
\slms\ %
hallucinate more than a simple cascade model that combines audio captioning with text LLMs.
This issue may arise because, while \slms\ %
can generate accurate audio captions,
they struggle to understand specific questions~\cite{kuan2024audiohallucination}.
The authors also proposed a method that mitigates hallucination by prompting 
the model to produce output in multiple steps (similarly to chain-of-thought prompting~\citep{CoT}), by first describing the audio and then responding to the %
instruction based on that description. %



\subsubsection{Toxicity}
Prevention of harmful, offensive, or inappropriate output is a crucial issue for text LLMs, and the same challenge also applies to \slms. Both Meta's SpiRit-LM~\citep{nguyen2024spirit} and OpenAI's GPT-4o voice mode~\citep{gpt4osafety} evaluate the toxicity of their models' responses. These evaluations typically involve using specific prompts to elicit speech responses from the \slms\ and assessing the toxicity of the text transcriptions of those responses. %
These evaluations therefore consider only verbal toxicity, i.e.~toxicity within the word sequence.
The evaluation of non-verbal toxicity %
(e.g., toxic sarcasm) has yet to be widely studied.

\subsubsection{Bias}
Similarly to toxicity, 
all bias measures %
used for text LLMs can also be applied to \slms. 
For \slms\ that produce spoken output, text-based methods can still be used to analyze their transcriptions~\citep{biascontentSLT24}. %
In addition, because speech conveys information beyond the textual content, %
there may be factors outside the text that %
indicate bias. \citet{biasspeechSLT24} %
investigate such biases %
by providing an \slm\ with input utterances that share identical content but differ in speaker characteristics (such as gender and age) to assess whether these traits influence its responses. %
The results suggest that the tested \slms\ 
exhibit minimal bias. However, this may be because current models lack sufficient sophistication to recognize differences in speaker characteristics, and therefore do not respond differently based on them.  OpenAI report in a blog post~\citep{gpt4osafety} on their efforts to ensure consistency across user voices by post-training GPT-4o with a diverse set of voices and on a bias evaluation, which found no significant variation in model behavior across voices.  While such findings are encouraging, regular and thorough bias benchmarking is needed to track the evolution of \slm\ bias as this field matures.


\subsubsection{Deepfakes}
Current \slms\ can mimic %
a variety of voices to a level indistinguishable from human speech \citep{wang2023neural}. 
Unfortunately, malicious actors may exploit this technology, leading to misuse and security concerns, such as deepfake attacks (for example, creating fake news using the voice of a public figure). 
To address this issue, \citep{wu2024codecfake,du2025codecfake} introduced CodecFake, a deepfake audio dataset, and found that 
detection models trained on CodecFake can %
counter fake audio generated by existing \slms.



