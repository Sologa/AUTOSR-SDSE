{"key": "agostinelli2023musiclm", "query_title": "{MusicLM}: Generating music from text", "normalized_title": "musiclm generating music from text", "title": "MusicLM: Generating Music From Text", "abstract": "We introduce MusicLM, a model generating high-fidelity music from text descriptions such as \"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.", "source": "arxiv", "source_id": "2301.11325v1", "match_status": "exact_title", "missing_reason": null}
{"key": "algayres2022speech", "query_title": "Speech sequence embeddings using nearest neighbors contrastive learning", "normalized_title": "speech sequence embeddings using nearest neighbors contrastive learning", "title": "Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning", "abstract": "We introduce a simple neural encoder architecture that can be trained using an unsupervised contrastive learning objective which gets its positive samples from data-augmented k-Nearest Neighbors search. We show that when built on top of recent self-supervised audio representations, this method can be applied iteratively and yield competitive SSE as evaluated on two tasks: query-by-example of random sequences of speech, and spoken term discovery. On both tasks our method pushes the state-of-the-art by a significant margin across 5 different languages. Finally, we establish a benchmark on a query-by-example task on the LibriSpeech dataset to monitor future improvements in the field.", "source": "arxiv", "source_id": "2204.05148v2", "match_status": "exact_title", "missing_reason": null}
{"key": "tGSLM", "query_title": "Generative spoken language model based on continuous word-sized audio tokens", "normalized_title": "generative spoken language model based on continuous word sized audio tokens", "title": "Generative Spoken Language Model based on continuous word-sized audio tokens", "abstract": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.", "source": "arxiv", "source_id": "2310.05224v1", "match_status": "exact_title", "missing_reason": null}
{"key": "an2024funaudiollm", "query_title": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms", "normalized_title": "funaudiollm voice understanding and generation foundation models for natural interaction between humans and llms", "title": "FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs", "abstract": "This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual speech recognition, emotion recognition, and audio event detection; and CosyVoice, which facilitates natural speech generation with control over multiple languages, timbre, speaking style, and speaker identity. SenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and SenseVoice-Large supports high-precision ASR for over 50 languages, while CosyVoice excels in multi-lingual voice generation, zero-shot in-context learning, cross-lingual voice cloning, and instruction-following capabilities. The models related to SenseVoice and CosyVoice have been open-sourced on Modelscope and Huggingface, along with the corresponding training, inference, and fine-tuning codes released on GitHub. By integrating these models with LLMs, FunAudioLLM enables applications such as speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration, thereby pushing the boundaries of voice interaction technology. Demos are available at https://fun-audio-llm.github.io, and the code can be accessed at https://github.com/FunAudioLLM.", "source": "arxiv", "source_id": "2407.04051v3", "match_status": "exact_title", "missing_reason": null}
{"key": "ao2024sd", "query_title": "{SD-Eval}: A benchmark dataset for spoken dialogue understanding beyond words", "normalized_title": "sd eval a benchmark dataset for spoken dialogue understanding beyond words", "title": "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words", "abstract": "Speech encompasses a wealth of information, including but not limited to content, paralinguistic, and environmental information. This comprehensive nature of speech significantly impacts communication and is crucial for human-computer interaction. Chat-Oriented Large Language Models (LLMs), known for their general-purpose assistance capabilities, have evolved to handle multi-modal inputs, including speech. Although these models can be adept at recognizing and analyzing speech, they often fall short of generating appropriate responses. We argue that this is due to the lack of principles on task definition and model development, which requires open-source datasets and metrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a benchmark dataset aimed at multidimensional evaluation of spoken dialogue understanding and generation. SD-Eval focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound. To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a process similar to that of SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. We also conduct a comprehensive evaluation using objective evaluation methods (e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated responses. Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures. Moreover, experiments demonstrate that LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics. We open-source SD-Eval at https://github.com/amphionspace/SD-Eval.", "source": "arxiv", "source_id": "2406.13340v2", "match_status": "exact_title", "missing_reason": null}
{"key": "arora2023universlu", "query_title": "{U}niver{SLU}: Universal spoken language understanding for diverse tasks with natural language instructions", "normalized_title": "universlu universal spoken language understanding for diverse tasks with natural language instructions", "title": "UniverSLU: Universal Spoken Language Understanding for Diverse Tasks with Natural Language Instructions", "abstract": "Recent studies leverage large language models with multi-tasking capabilities, using natural language prompts to guide the model's behavior and surpassing performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly performs various spoken language understanding (SLU) tasks? We start by adapting a pre-trained automatic speech recognition model to additional tasks using single-token task specifiers. We enhance this approach through instruction tuning, i.e., finetuning by describing the task using natural language instructions followed by the list of label options. Our approach can generalize to new task descriptions for the seen tasks during inference, thereby enhancing its user-friendliness. We demonstrate the efficacy of our single multi-task learning model \"UniverSLU\" for 12 speech classification and sequence generation task types spanning 17 datasets and 9 languages. On most tasks, UniverSLU achieves competitive performance and often even surpasses task-specific models. Additionally, we assess the zero-shot capabilities, finding that the model generalizes to new datasets and languages for seen task types.", "source": "arxiv", "source_id": "2310.02973v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wav2vec2", "query_title": "wav2vec 2.0: A framework for self-supervised learning of speech representations", "normalized_title": "wav2vec 2 0 a framework for self supervised learning of speech representations", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.", "source": "arxiv", "source_id": "2006.11477v3", "match_status": "exact_title", "missing_reason": null}
{"key": "bai2023qwen", "query_title": "Qwen technical report", "normalized_title": "qwen technical report", "title": "Qwen Technical Report", "abstract": "Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.", "source": "arxiv", "source_id": "2309.16609v1", "match_status": "exact_title", "missing_reason": null}
{"key": "berant2013semantic", "query_title": "Semantic parsing on {F}reebase from question-answer pairs", "normalized_title": "semantic parsing on freebase from question answer pairs", "title": "Semantic Parsing on Freebase from Question-Answer Pairs", "abstract": "In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.", "source": "semantic_scholar", "source_id": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "match_status": "exact_title", "missing_reason": null}
{"key": "audiolm", "query_title": "{AudioLM}: A language modeling approach to audio generation", "normalized_title": "audiolm a language modeling approach to audio generation", "title": "AudioLM: a Language Modeling Approach to Audio Generation", "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.", "source": "arxiv", "source_id": "2209.03143v2", "match_status": "exact_title", "missing_reason": null}
{"key": "borsos2023soundstorm", "query_title": "{SoundStorm}: Efficient parallel audio generation", "normalized_title": "soundstorm efficient parallel audio generation", "title": "SoundStorm: Efficient Parallel Audio Generation", "abstract": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.", "source": "arxiv", "source_id": "2305.09636v1", "match_status": "exact_title", "missing_reason": null}
{"key": "brown2020language", "query_title": "Language models are few-shot learners", "normalized_title": "language models are few shot learners", "title": "Language Models are Few-Shot Learners", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "source": "arxiv", "source_id": "2005.14165v4", "match_status": "exact_title", "missing_reason": null}
{"key": "chan2016listen", "query_title": "Listen, attend and spell", "normalized_title": "listen attend and spell", "title": "Listen, Attend and Spell", "abstract": "We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.", "source": "arxiv", "source_id": "1508.01211v2", "match_status": "exact_title", "missing_reason": null}
{"key": "chang2022maskgit", "query_title": "{MaskGIT}: Masked generative image transformer", "normalized_title": "maskgit masked generative image transformer", "title": "MaskGIT: Masked Generative Image Transformer", "abstract": "Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation.", "source": "arxiv", "source_id": "2202.04200v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chang2024speechprompt", "query_title": "{SpeechPrompt}: Prompting speech language models for speech processing tasks", "normalized_title": "speechprompt prompting speech language models for speech processing tasks", "title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks", "abstract": "Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.", "source": "arxiv", "source_id": "2408.13040v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2023x", "query_title": "{X-LLM}: Bootstrapping advanced large language models by treating multi-modalities as foreign languages", "normalized_title": "x llm bootstrapping advanced large language models by treating multi modalities as foreign languages", "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages", "abstract": "Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.", "source": "arxiv", "source_id": "2305.04160v3", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2024next", "query_title": "Next token prediction towards multimodal intelligence: A comprehensive survey", "normalized_title": "next token prediction towards multimodal intelligence a comprehensive survey", "title": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey", "abstract": "Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets \\& evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction", "source": "arxiv", "source_id": "2412.18619v2", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2025minmo", "query_title": "Minmo: A multimodal large language model for seamless voice interaction", "normalized_title": "minmo a multimodal large language model for seamless voice interaction", "title": "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction", "abstract": "Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations. Previous models for voice interactions are categorized as native and aligned. Native models integrate speech and text processing in one framework but struggle with issues like differing sequence lengths and insufficient pre-training. Aligned models maintain text LLM capabilities but are often limited by small datasets and a narrow focus on speech tasks. In this work, we introduce MinMo, a Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction. We address the main limitations of prior aligned multimodal models. We train MinMo through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and a broad range of speech tasks. After the multi-stage training, MinMo achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system. Moreover, we propose a novel and simple voice decoder that outperforms prior models in voice generation. The enhanced instruction-following capabilities of MinMo supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices. For MinMo, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice. The MinMo project web page is https://funaudiollm.github.io/minmo, and the code and models will be released soon.", "source": "arxiv", "source_id": "2501.06282v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wavlm", "query_title": "{WavLM}: Large-scale self-supervised pre-training for full stack speech processing", "normalized_title": "wavlm large scale self supervised pre training for full stack speech processing", "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing", "abstract": "Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture the sequence ordering of input speech. We also scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pre-trained models are available at https://aka.ms/wavlm.", "source": "arxiv", "source_id": "2110.13900v5", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2023neural", "query_title": "Neural codec language models are zero-shot text to speech synthesizers", "normalized_title": "neural codec language models are zero shot text to speech synthesizers", "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.", "source": "arxiv", "source_id": "2301.02111v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2024slam", "query_title": "Slam-omni: Timbre-controllable voice interaction system with single-stage training", "normalized_title": "slam omni timbre controllable voice interaction system with single stage training", "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training", "abstract": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.", "source": "arxiv", "source_id": "2412.15649v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2024voicebench", "query_title": "{VoiceBench}: Benchmarking {LLM}-based voice assistants", "normalized_title": "voicebench benchmarking llm based voice assistants", "title": "VoiceBench: Benchmarking LLM-Based Voice Assistants", "abstract": "Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field.", "source": "arxiv", "source_id": "2410.17196v3", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2024salm", "query_title": "{SALM}: Speech-augmented language model with in-context learning for speech recognition and translation", "normalized_title": "salm speech augmented language model with in context learning for speech recognition and translation", "title": "SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation", "abstract": "We present a novel Speech Augmented Language Model (SALM) with {\\em multitask} and {\\em in-context} learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, {\\em speech supervised in-context training} is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.", "source": "arxiv", "source_id": "2310.09424v1", "match_status": "exact_title", "missing_reason": null}
{"key": "bestow2024chen", "query_title": "Bestow: Efficient and streamable speech language model with the best of two worlds in {GPT} and {T5}", "normalized_title": "bestow efficient and streamable speech language model with the best of two worlds in gpt and t5", "title": "BESTOW: Efficient and Streamable Speech Language Model with the Best of Two Worlds in GPT and T5", "abstract": "Incorporating speech understanding capabilities into pretrained large-language models has become a vital research direction (SpeechLLM). The previous architectures can be categorized as: i) GPT-style, prepend speech prompts to the text prompts as a sequence of LLM inputs like a decoder-only model; ii) T5-style, introduce speech cross-attention to each layer of the pretrained LLMs. We propose BESTOW architecture to bring the BESt features from TwO Worlds into a single model that is highly efficient and has strong multitask capabilities. Moreover, there is no clear streaming solution for either style, especially considering the solution should generalize to speech multitask. We reformulate streamable SpeechLLM as a read-write policy problem and unifies the offline and streaming research with BESTOW architecture. Hence we demonstrate the first open-source SpeechLLM solution that enables Streaming and Multitask at scale (beyond ASR) at the same time. This streamable solution achieves very strong performance on a wide range of speech tasks (ASR, AST, SQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower training/inference cost, and demonstrates LLM knowledge transferability to speech.", "source": "arxiv", "source_id": "2406.19954v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chou2023toward", "query_title": "Toward joint language modeling for speech units and text", "normalized_title": "toward joint language modeling for speech units and text", "title": "Toward Joint Language Modeling for Speech Units and Text", "abstract": "Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model's learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.", "source": "arxiv", "source_id": "2310.08715v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chu2023qwen", "query_title": "{Qwen-Audio}: Advancing universal audio understanding via unified large-scale audio-language models", "normalized_title": "qwen audio advancing universal audio understanding via unified large scale audio language models", "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models", "abstract": "Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios.", "source": "arxiv", "source_id": "2311.07919v2", "match_status": "exact_title", "missing_reason": null}
{"key": "chu2024qwen2audio", "query_title": "{Qwen2-Audio} technical report", "normalized_title": "qwen2 audio technical report", "title": "Qwen2-Audio Technical Report", "abstract": "We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model's performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community.", "source": "arxiv", "source_id": "2407.10759v1", "match_status": "exact_title", "missing_reason": null}
{"key": "copet2024simple", "query_title": "Simple and controllable music generation", "normalized_title": "simple and controllable music generation", "title": "Simple and Controllable Music Generation", "abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft", "source": "arxiv", "source_id": "2306.05284v3", "match_status": "exact_title", "missing_reason": null}
{"key": "scaling-slms", "query_title": "Scaling properties of speech language models", "normalized_title": "scaling properties of speech language models", "title": "Scaling Properties of Speech Language Models", "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.", "source": "arxiv", "source_id": "2404.00685v2", "match_status": "exact_title", "missing_reason": null}
{"key": "cui2024recentadvancesspeechlanguage", "query_title": "Recent advances in speech language models: A survey", "normalized_title": "recent advances in speech language models a survey", "title": "Recent Advances in Speech Language Models: A Survey", "abstract": "Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\", where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) -- end-to-end models that generate speech without converting from text -- have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at https://github.com/dreamtheater123/Awesome-SpeechLM-Survey", "source": "arxiv", "source_id": "2410.03751v4", "match_status": "exact_title", "missing_reason": null}
{"key": "das2024speechverse", "query_title": "{SpeechVerse}: A large-scale generalizable audio language model", "normalized_title": "speechverse a large scale generalizable audio language model", "title": "SpeechVerse: A Large-scale Generalizable Audio Language Model", "abstract": "Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions. Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic speech recognition and translation. We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained speech and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training. The models are instruction finetuned using continuous latent representations extracted from the speech foundation model to achieve optimal zero-shot performance on a diverse range of speech processing tasks using natural language instructions. We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks. Furthermore, we evaluate the model's capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks. Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks.", "source": "arxiv", "source_id": "2405.08295v3", "match_status": "exact_title", "missing_reason": null}
{"key": "de2023prosaudit", "query_title": "Prosaudit, a prosodic benchmark for self-supervised speech models", "normalized_title": "prosaudit a prosodic benchmark for self supervised speech models", "title": "ProsAudit, a prosodic benchmark for self-supervised speech models", "abstract": "We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, and an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when evaluated on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks.", "source": "arxiv", "source_id": "2302.12057v3", "match_status": "exact_title", "missing_reason": null}
{"key": "encodec", "query_title": "High fidelity neural audio compression", "normalized_title": "high fidelity neural audio compression", "title": "High Fidelity Neural Audio Compression", "abstract": "We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.", "source": "arxiv", "source_id": "2210.13438v1", "match_status": "exact_title", "missing_reason": null}
{"key": "defossezmoshi", "query_title": "Moshi: A speech-text foundation model for real-time dialogue", "normalized_title": "moshi a speech text foundation model for real time dialogue", "title": "Moshi: a speech-text foundation model for real-time dialogue", "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.", "source": "arxiv", "source_id": "2410.00037v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wav2prompt", "query_title": "{Wav2Prompt}: End-to-end speech prompt generation and tuning for {LLM} in zero and few-shot learning", "normalized_title": "wav2prompt end to end speech prompt generation and tuning for llm in zero and few shot learning", "title": "Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in Zero and Few-shot Learning", "abstract": "Wav2Prompt is proposed which allows straightforward integration between spoken input and a text-based large language model (LLM). Wav2Prompt uses a simple training process with only the same data used to train an automatic speech recognition (ASR) model. After training, Wav2Prompt learns continuous representations from speech and uses them as LLM prompts. To avoid task over-fitting issues found in prior work and preserve the emergent abilities of LLMs, Wav2Prompt takes LLM token embeddings as the training targets and utilises a continuous integrate-and-fire mechanism for explicit speech-text alignment. Therefore, a Wav2Prompt-LLM combination can be applied to zero-shot spoken language tasks such as speech translation (ST), speech understanding (SLU), speech question answering (SQA) and spoken-query-based QA (SQQA). It is shown that for these zero-shot tasks, Wav2Prompt performs similarly to an ASR-LLM cascade and better than recent prior work. If relatively small amounts of task-specific paired data are available in few-shot scenarios, the Wav2Prompt-LLM combination can be end-to-end (E2E) fine-tuned. The Wav2Prompt-LLM combination then yields greatly improved results relative to an ASR-LLM cascade for the above tasks. For instance, for English-French ST with the BLOOMZ-7B1 LLM, a Wav2Prompt-LLM combination gave a 8.5 BLEU point increase over an ASR-LLM cascade.", "source": "arxiv", "source_id": "2406.00522v1", "match_status": "exact_title", "missing_reason": null}
{"key": "bert", "query_title": "{BERT}: Pre-training of deep bidirectional transformers for language understanding", "normalized_title": "bert pre training of deep bidirectional transformers for language understanding", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "source": "arxiv", "source_id": "1810.04805v2", "match_status": "exact_title", "missing_reason": null}
{"key": "usm-lite", "query_title": "{USM-Lite}: Quantization and sparsity aware fine-tuning for speech recognition with universal speech models", "normalized_title": "usm lite quantization and sparsity aware fine tuning for speech recognition with universal speech models", "title": "USM-Lite: Quantization and Sparsity Aware Fine-tuning for Speech Recognition with Universal Speech Models", "abstract": "End-to-end automatic speech recognition (ASR) models have seen revolutionary quality gains with the recent development of large-scale universal speech models (USM). However, deploying these massive USMs is extremely expensive due to the enormous memory usage and computational cost. Therefore, model compression is an important research topic to fit USM-based ASR under budget in real-world scenarios. In this study, we propose a USM fine-tuning approach for ASR, with a low-bit quantization and N:M structured sparsity aware paradigm on the model weights, reducing the model complexity from parameter precision and matrix topology perspectives. We conducted extensive experiments with a 2-billion parameter USM on a large-scale voice search dataset to evaluate our proposed method. A series of ablation studies validate the effectiveness of up to int4 quantization and 2:4 sparsity. However, a single compression technique fails to recover the performance well under extreme setups including int2 quantization and 1:4 sparsity. By contrast, our proposed method can compress the model to have 9.4% of the size, at the cost of only 7.3% relative word error rate (WER) regressions. We also provided in-depth analyses on the results and discussions on the limitations and potential solutions, which would be valuable for future studies.", "source": "arxiv", "source_id": "2312.08553v3", "match_status": "exact_title", "missing_reason": null}
{"key": "du2025codecfake", "query_title": "{CodecFake-Omni}: A large-scale codec-based deepfake speech dataset", "normalized_title": "codecfake omni a large scale codec based deepfake speech dataset", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "du2024cosyvoice", "query_title": "{CosyVoice}: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens", "normalized_title": "cosyvoice a scalable multilingual zero shot text to speech synthesizer based on supervised semantic tokens", "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens", "abstract": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.", "source": "arxiv", "source_id": "2407.05407v2", "match_status": "exact_title", "missing_reason": null}
{"key": "dunbar2021zero", "query_title": "The zero resource speech challenge 2021: Spoken language modelling", "normalized_title": "the zero resource speech challenge 2021 spoken language modelling", "title": "The Zero Resource Speech Challenge 2021: Spoken language modelling", "abstract": "We present the Zero Resource Speech Challenge 2021, which asks participants to learn a language model directly from audio, without any text or labels. The challenge is based on the Libri-light dataset, which provides up to 60k hours of audio from English audio books without any associated text. We provide a pipeline baseline system consisting on an encoder based on contrastive predictive coding (CPC), a quantizer ($k$-means) and a standard language model (BERT or LSTM). The metrics evaluate the learned representations at the acoustic (ABX discrimination), lexical (spot-the-word), syntactic (acceptability judgment) and semantic levels (similarity judgment). We present an overview of the eight submitted systems from four groups and discuss the main results.", "source": "arxiv", "source_id": "2104.14700v2", "match_status": "exact_title", "missing_reason": null}
{"key": "ekstedt-skantze-2020-turngpt", "query_title": "{T}urn{GPT}: A transformer-based language model for predicting turn-taking in spoken dialog", "normalized_title": "turngpt a transformer based language model for predicting turn taking in spoken dialog", "title": "TurnGPT: a Transformer-based Language Model for Predicting Turn-taking in Spoken Dialog", "abstract": "Syntactic and pragmatic completeness is known to be important for turn-taking prediction, but so far machine learning models of turn-taking have used such linguistic information in a limited way. In this paper, we introduce TurnGPT, a transformer-based language model for predicting turn-shifts in spoken dialog. The model has been trained and evaluated on a variety of written and spoken dialog datasets. We show that the model outperforms two baselines used in prior work. We also report on an ablation study, as well as attention and gradient analyses, which show that the model is able to utilize the dialog context and pragmatic completeness for turn-taking prediction. Finally, we explore the model's potential in not only detecting, but also projecting, turn-completions.", "source": "arxiv", "source_id": "2010.10874v1", "match_status": "exact_title", "missing_reason": null}
{"key": "fan2024alignformer", "query_title": "{AlignFormer}: Modality matching can achieve better zero-shot instruction-following speech-{LLM}", "normalized_title": "alignformer modality matching can achieve better zero shot instruction following speech llm", "title": "AlignFormer: Modality Matching Can Achieve Better Zero-shot Instruction-Following Speech-LLM", "abstract": "Integrating speech into LLM (speech-LLM) has gaining increased attention recently. The mainstream solution is to connect a well-trained speech encoder and LLM with a neural adapter. However, the length mismatch between the speech and text sequences are not well handled, leading to imperfect modality matching between the speech and text. In this work, we propose a novel neural adapter, AlignFormer, to reduce the length gap between the two modalities. AlignFormer consists of CTC and dynamic-window QFormer layers, where the CTC alignment provides the dynamic window information for QFormer. The LLM backbone is frozen in training to preserve its text capability, especially the instruction following capability. When training with ASR data only, the proposed AlignFormer unlocks the instruction following capability for speech-LLM and the model can perform zero-shot speech translation (ST) and speech question answering (SQA) tasks. In fact, speech-LLM with AlignFormer can theoretically perform any tasks that the LLM backbone can deal with in the speech version. To evaluate the effectiveness of the instruction-following speech-LLM, we propose to use instruction following rate (IFR) and offer a systematic perspective for the IFR evaluation. In addition, we find that the audio position in training would affect the instruction following capability of speech-LLM and conduct an in-depth study on it. Our findings show that audio-first training achieves higher IFR than instruction-first training. The AlignFormer can achieve a near 100% IFR with audio-first training and game-changing improvements from zero to non-zero IFR on some evaluation data with instruction-first training. We believe that this study is a big step towards the perfect speech and text modality matching in the LLM embedding space.", "source": "arxiv", "source_id": "2412.01145v2", "match_status": "exact_title", "missing_reason": null}
{"key": "fang2024llama", "query_title": "{LL}a{MA}-{Omni}: Seamless speech interaction with large language models", "normalized_title": "llama omni seamless speech interaction with large language models", "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models", "abstract": "Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.", "source": "semantic_scholar", "source_id": "218ddd5ca9e959c91ad6b48a379397e4cb0d47d8", "match_status": "exact_title", "missing_reason": null}
{"key": "audiochatllama", "query_title": "{AudioChatLlama}: Towards general-purpose speech abilities for {LLM}s", "normalized_title": "audiochatllama towards general purpose speech abilities for llms", "title": "AudioChatLlama: Towards General-Purpose Speech Abilities for LLMs", "abstract": "In this work, we extend the instruction-tuned Llama-2 model with end-to-end general-purpose speech processing and reasoning abilities while maintaining the wide range of original LLM capabilities, without using any carefully curated paired data. The resulting end-to-end model, named AudioChatLlama, can utilize audio prompts as a replacement for text and sustain a conversation. Such a model also has extended cross-modal capabilities such as being able to perform spoken question answering (QA), speech translation, and audio summarization amongst many other closed and open-domain tasks. This is unlike prior approaches in speech, in which LLMs are extended to handle audio for a limited number of pre-designated tasks. On both synthesized and recorded speech QA test sets, evaluations show that our end-to-end approach is on par with or outperforms cascaded systems (speech recognizer + LLM) in terms of modelling the response to a prompt. Furthermore, unlike cascades, our approach can interchange text and audio modalities and intrinsically utilize prior context in a conversation to provide better results.", "source": "semantic_scholar", "source_id": "e4a9a026a40a58cee4d6509d2ef817a86a11a760", "match_status": "exact_title", "missing_reason": null}
{"key": "bpe", "query_title": "A new algorithm for data compression", "normalized_title": "a new algorithm for data compression", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "gaido2021ctc", "query_title": "{CTC}-based compression for direct speech translation", "normalized_title": "ctc based compression for direct speech translation", "title": "CTC-based Compression for Direct Speech Translation", "abstract": "Previous studies demonstrated that a dynamic phone-informed compression of the input audio is beneficial for speech translation (ST). However, they required a dedicated model for phone recognition and did not test this solution for direct ST, in which a single model translates the input audio into the target language without intermediate representations. In this work, we propose the first method able to perform a dynamic compression of the input in direct ST models. In particular, we exploit the Connectionist Temporal Classification (CTC) to compress the input sequence according to its phonetic characteristics. Our experiments demonstrate that our solution brings a 1.3-1.5 BLEU improvement over a strong baseline on two language pairs (English-Italian and English-German), contextually reducing the memory footprint by more than 10%.", "source": "semantic_scholar", "source_id": "62316c538d3d9dcb6ae8a3567433f7417eab1f32", "match_status": "exact_title", "missing_reason": null}
{"key": "gao2022wavprompt", "query_title": "{WavPrompt}: Towards few-shot spoken language understanding with frozen language models", "normalized_title": "wavprompt towards few shot spoken language understanding with frozen language models", "title": "WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models", "abstract": "Large-scale auto-regressive language models pretrained on massive text have demonstrated their impressive ability to perform new natural language tasks with only a few text examples, without the need for fine-tuning. Recent studies further show that such a few-shot learning ability can be extended to the text-image setting by training an encoder to encode the images into embeddings functioning like the text embeddings of the language model. Interested in exploring the possibility of transferring the few-shot learning ability to the audio-text setting, we propose a novel speech understanding framework, WavPrompt, where we finetune a wav2vec model to generate a sequence of audio embeddings understood by the language model. We show that WavPrompt is a few-shot learner that can perform speech understanding tasks better than a naive text baseline. We conduct detailed ablation studies on different components and hyperparameters to empirically identify the best model configuration. In addition, we conduct a non-speech understanding experiment to show WavPrompt can extract more information than just the transcriptions. Code is available at https://github.com/Hertin/WavPrompt", "source": "arxiv", "source_id": "2203.15863v2", "match_status": "exact_title", "missing_reason": null}
{"key": "geminiteam2024gemini15unlockingmultimodal", "query_title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", "normalized_title": "gemini 1 5 unlocking multimodal understanding across millions of tokens of context", "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.", "source": "arxiv", "source_id": "2403.05530v5", "match_status": "exact_title", "missing_reason": null}
{"key": "team2024gemma", "query_title": "Gemma: Open models based on gemini research and technology", "normalized_title": "gemma open models based on gemini research and technology", "title": "Gemma: Open Models Based on Gemini Research and Technology", "abstract": "This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.", "source": "arxiv", "source_id": "2403.08295v4", "match_status": "exact_title", "missing_reason": null}
{"key": "gong2023whisperat", "query_title": "{Whisper-AT}: Noise-robust automatic speech recognizers are also strong general audio event taggers", "normalized_title": "whisper at noise robust automatic speech recognizers are also strong general audio event taggers", "title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers", "abstract": "In this paper, we focus on Whisper, a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions. We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type. With this finding, we build a unified audio tagging and speech recognition model Whisper-AT by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it. With <1% extra computational cost, Whisper-AT can recognize audio events, in addition to spoken text, in a single forward pass.", "source": "arxiv", "source_id": "2307.03183v1", "match_status": "exact_title", "missing_reason": null}
{"key": "gong2023joint", "query_title": "Joint audio and speech understanding", "normalized_title": "joint audio and speech understanding", "title": "Joint Audio and Speech Understanding", "abstract": "Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper as a perception module and LLaMA as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.", "source": "arxiv", "source_id": "2309.14405v3", "match_status": "exact_title", "missing_reason": null}
{"key": "gong2023listen", "query_title": "Listen, think, and understand", "normalized_title": "listen think and understand", "title": "Listen, Think, and Understand", "abstract": "The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.", "source": "arxiv", "source_id": "2305.10790v3", "match_status": "exact_title", "missing_reason": null}
{"key": "llama3", "query_title": "The {Llama} 3 herd of models", "normalized_title": "the llama 3 herd of models", "title": "The Llama 3 Herd of Models", "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.", "source": "arxiv", "source_id": "2407.21783v3", "match_status": "exact_title", "missing_reason": null}
{"key": "graves2006connectionist", "query_title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "normalized_title": "connectionist temporal classification labelling unsegmented sequence data with recurrent neural networks", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "VQ", "query_title": "Vector quantization", "normalized_title": "vector quantization", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "gu2021efficiently", "query_title": "Efficiently modeling long sequences with structured state spaces", "normalized_title": "efficiently modeling long sequences with structured state spaces", "title": "Efficiently Modeling Long Sequences with Structured State Spaces", "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.", "source": "arxiv", "source_id": "2111.00396v3", "match_status": "exact_title", "missing_reason": null}
{"key": "guo2025recent", "query_title": "Recent advances in discrete speech tokens: A review", "normalized_title": "recent advances in discrete speech tokens a review", "title": "Recent Advances in Discrete Speech Tokens: A Review", "abstract": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.", "source": "arxiv", "source_id": "2502.06490v4", "match_status": "exact_title", "missing_reason": null}
{"key": "twist", "query_title": "Textually pretrained speech language models", "normalized_title": "textually pretrained speech language models", "title": "Textually Pretrained Speech Language Models", "abstract": "Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .", "source": "arxiv", "source_id": "2305.13009v3", "match_status": "exact_title", "missing_reason": null}
{"key": "diva", "query_title": "Distilling an end-to-end voice assistant without instruction training data", "normalized_title": "distilling an end to end voice assistant without instruction training data", "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data", "abstract": "Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT) have led to models ``forgetting\" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using $>$100x less training compute.", "source": "arxiv", "source_id": "2410.02678v1", "match_status": "exact_title", "missing_reason": null}
{"key": "hernandez2018ted", "query_title": "Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation", "normalized_title": "ted lium 3 twice as much data and corpus repartition for experiments on speaker adaptation", "title": "TED-LIUM 3: twice as much data and corpus repartition for experiments on speaker adaptation", "abstract": "In this paper, we present TED-LIUM release 3 corpus dedicated to speech recognition in English, that multiplies by more than two the available data to train acoustic models in comparison with TED-LIUM 2. We present the recent development on Automatic Speech Recognition (ASR) systems in comparison with the two previous releases of the TED-LIUM Corpus from 2012 and 2014. We demonstrate that, passing from 207 to 452 hours of transcribed speech training data is really more useful for end-to-end ASR systems than for HMM-based state-of-the-art ones, even if the HMM-based ASR system still outperforms end-to-end ASR system when the size of audio training data is 452 hours, with respectively a Word Error Rate (WER) of 6.6% and 13.7%. Last, we propose two repartitions of the TED-LIUM release 3 corpus: the legacy one that is the same as the one existing in release 2, and a new one, calibrated and designed to make experiments on speaker adaptation. Like the two first releases, TED-LIUM 3 corpus will be freely available for the research community.", "source": "arxiv", "source_id": "1805.04699v4", "match_status": "exact_title", "missing_reason": null}
{"key": "holtzman2020the", "query_title": "The curious case of neural text degeneration", "normalized_title": "the curious case of neural text degeneration", "title": "The Curious Case of Neural Text Degeneration", "abstract": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.", "source": "arxiv", "source_id": "1904.09751v2", "match_status": "exact_title", "missing_reason": null}
{"key": "houlsby2019parameter", "query_title": "Parameter-efficient transfer learning for nlp", "normalized_title": "parameter efficient transfer learning for nlp", "title": "Parameter-Efficient Transfer Learning for NLP", "abstract": "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.", "source": "arxiv", "source_id": "1902.00751v2", "match_status": "exact_title", "missing_reason": null}
{"key": "hubert", "query_title": "{HuBERT}: Self-supervised speech representation learning by masked prediction of hidden units", "normalized_title": "hubert self supervised speech representation learning by masked prediction of hidden units", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units", "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.", "source": "arxiv", "source_id": "2106.07447v1", "match_status": "exact_title", "missing_reason": null}
{"key": "lora", "query_title": "{LoRA}: Low-rank adaptation of large language models", "normalized_title": "lora low rank adaptation of large language models", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.", "source": "arxiv", "source_id": "2106.09685v2", "match_status": "exact_title", "missing_reason": null}
{"key": "hu2024chain", "query_title": "Chain-of-thought prompting for speech translation", "normalized_title": "chain of thought prompting for speech translation", "title": "Chain-of-Thought Prompting for Speech Translation", "abstract": "Large language models (LLMs) have demonstrated remarkable advancements in language understanding and generation. Building on the success of text-based LLMs, recent research has adapted these models to use speech embeddings for prompting, resulting in Speech-LLM models that exhibit strong performance in automatic speech recognition (ASR) and automatic speech translation (AST). In this work, we propose a novel approach to leverage ASR transcripts as prompts for AST in a Speech-LLM built on an encoder-decoder text LLM. The Speech-LLM model consists of a speech encoder and an encoder-decoder structure Megatron-T5. By first decoding speech to generate ASR transcripts and subsequently using these transcripts along with encoded speech for prompting, we guide the speech translation in a two-step process like chain-of-thought (CoT) prompting. Low-rank adaptation (LoRA) is used for the T5 LLM for model adaptation and shows superior performance to full model fine-tuning. Experimental results show that the proposed CoT prompting significantly improves AST performance, achieving an average increase of 2.4 BLEU points across 6 En->X or X->En AST tasks compared to speech prompting alone. Additionally, compared to a related CoT prediction method that predicts a concatenated sequence of ASR and AST transcripts, our method performs better by an average of 2 BLEU points.", "source": "arxiv", "source_id": "2409.11538v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wavllm", "query_title": "{W}av{LLM}: Towards robust and adaptive speech large language model", "normalized_title": "wavllm towards robust and adaptive speech large language model", "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "abstract": "The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\url{aka.ms/wavllm}.", "source": "semantic_scholar", "source_id": "1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4", "match_status": "exact_title", "missing_reason": null}
{"key": "huang2024dynamic", "query_title": "{Dynamic-SUPERB}: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech", "normalized_title": "dynamic superb towards a dynamic collaborative and comprehensive instruction tuning benchmark for speech", "title": "Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech", "abstract": "Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.", "source": "arxiv", "source_id": "2309.09510v2", "match_status": "exact_title", "missing_reason": null}
{"key": "huang2025dynamicsuperb", "query_title": "{Dynamic-SUPERB} phase-2: A collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks", "normalized_title": "dynamic superb phase 2 a collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks", "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks", "abstract": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.", "source": "arxiv", "source_id": "2411.05361v2", "match_status": "exact_title", "missing_reason": null}
{"key": "huang2001spoken", "query_title": "\\emph{Spoken language processing: A guide to theory, algorithm, and system development}", "normalized_title": "spoken language processing a guide to theory algorithm and system development", "title": "Spoken Language Processing: A Guide to Theory, Algorithm, and System Development", "abstract": "From the Publisher: New advances in spoken language processing: theory and practice In-depth coverage of speech processing, speech recognition, speech synthesis, spoken language understanding, and speech interface design Many case studies from state-of-the-art systems, including examples from Microsoft's advanced research labs Spoken Language Processing draws on the latest advances and techniques from multiple fields: computer science, electrical engineering, acoustics, linguistics, mathematics, psychology, and beyond. Starting with the fundamentals, it presents all this and more: Essential background on speech production and perception, probability and information theory, and pattern recognition Extracting information from the speech signal: useful representations and practical compression solutions Modern speech recognition techniques: hidden Markov models, acoustic and language modeling, improving resistance to environmental noises, search algorithms, and large vocabulary speech recognition Text-to-speech: analyzing documents, pitch and duration controls; trainable synthesis, and more Spoken language understanding: dialog management, spoken language applications, and multimodal interfaces To illustrate the book's methods, the authors present detailed case studies based on state-of-the-art systems, including Microsoft's Whisper speech recognizer, Whistler text-to-speech system, Dr. Who dialog system, and the MiPad handheld device. Whether you're planning, designing, building, or purchasing spoken language technology, this is the state of the artfromalgorithms through business productivity.", "source": "openalex", "source_id": "https://openalex.org/W1578856370", "match_status": "exact_title", "missing_reason": null}
{"key": "ji2024wavchat", "query_title": "{WavChat}: A survey of spoken dialogue models", "normalized_title": "wavchat a survey of spoken dialogue models", "title": "WavChat: A Survey of Spoken Dialogue Models", "abstract": "Recent advancements in spoken dialogue models, exemplified by systems like GPT-4o, have captured significant attention in the speech domain. Compared to traditional three-tier cascaded spoken dialogue models that comprise speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS), modern spoken dialogue models exhibit greater intelligence. These advanced spoken dialogue models not only comprehend audio, music, and other speech-related features, but also capture stylistic and timbral characteristics in speech. Moreover, they generate high-quality, multi-turn speech responses with low latency, enabling real-time interaction through simultaneous listening and speaking capability. Despite the progress in spoken dialogue systems, there is a lack of comprehensive surveys that systematically organize and analyze these systems and the underlying technologies. To address this, we have first compiled existing spoken dialogue systems in the chronological order and categorized them into the cascaded and end-to-end paradigms. We then provide an in-depth overview of the core technologies in spoken dialogue models, covering aspects such as speech representation, training paradigm, streaming, duplex, and interaction capabilities. Each section discusses the limitations of these technologies and outlines considerations for future research. Additionally, we present a thorough review of relevant datasets, evaluation metrics, and benchmarks from the perspectives of training and evaluating spoken dialogue systems. We hope this survey will contribute to advancing both academic research and industrial applications in the field of spoken dialogue systems. The related material is available at https://github.com/jishengpeng/WavChat.", "source": "arxiv", "source_id": "2411.13577v2", "match_status": "exact_title", "missing_reason": null}
{"key": "jimenez2023swe", "query_title": "Swe-bench: Can language models resolve real-world github issues?", "normalized_title": "swe bench can language models resolve real world github issues", "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.", "source": "arxiv", "source_id": "2310.06770v3", "match_status": "exact_title", "missing_reason": null}
{"key": "joshi2017triviaqa", "query_title": "{T}rivia{QA}: A large scale distantly supervised challenge dataset for reading comprehension", "normalized_title": "triviaqa a large scale distantly supervised challenge dataset for reading comprehension", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/", "source": "arxiv", "source_id": "1705.03551v2", "match_status": "exact_title", "missing_reason": null}
{"key": "ke2023continual", "query_title": "Continual pre-training of language models", "normalized_title": "continual pre training of language models", "title": "Continual Pre-training of Language Models", "abstract": "Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method.", "source": "arxiv", "source_id": "2302.03241v4", "match_status": "exact_title", "missing_reason": null}
{"key": "keuleers2010wuggy", "query_title": "Wuggy: A multilingual pseudoword generator", "normalized_title": "wuggy a multilingual pseudoword generator", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "kharitonov2022textless", "query_title": "textless-lib: A library for textless spoken language processing", "normalized_title": "textless lib a library for textless spoken language processing", "title": "textless-lib: a Library for Textless Spoken Language Processing", "abstract": "Textless spoken language processing research aims to extend the applicability of standard NLP toolset onto spoken language and languages with few or no textual resources. In this paper, we introduce textless-lib, a PyTorch-based library aimed to facilitate research in this research area. We describe the building blocks that the library provides and demonstrate its usability by discuss three different use-case examples: (i) speaker probing, (ii) speech resynthesis and compression, and (iii) speech continuation. We believe that textless-lib substantially simplifies research the textless setting and will be handful not only for speech researchers but also for the NLP community at large. The code, documentation, and pre-trained models are available at https://github.com/facebookresearch/textlesslib/ .", "source": "arxiv", "source_id": "2202.07359v1", "match_status": "exact_title", "missing_reason": null}
{"key": "pGSLM", "query_title": "Text-free prosody-aware generative spoken language modeling", "normalized_title": "text free prosody aware generative spoken language modeling", "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "source": "arxiv", "source_id": "2109.03264v2", "match_status": "exact_title", "missing_reason": null}
{"key": "kharitonov2023speak", "query_title": "Speak, read and prompt: High-fidelity text-to-speech with minimal supervision", "normalized_title": "speak read and prompt high fidelity text to speech with minimal supervision", "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision", "abstract": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.", "source": "arxiv", "source_id": "2302.03540v1", "match_status": "exact_title", "missing_reason": null}
{"key": "Kirkpatrick2017overcoming", "query_title": "Overcoming catastrophic forgetting in neural networks", "normalized_title": "overcoming catastrophic forgetting in neural networks", "title": "Overcoming catastrophic forgetting in neural networks", "abstract": "The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.", "source": "arxiv", "source_id": "1612.00796v2", "match_status": "exact_title", "missing_reason": null}
{"key": "koizumi2022wavefit", "query_title": "Wavefit: An iterative and non-autoregressive neural vocoder based on fixed-point iteration", "normalized_title": "wavefit an iterative and non autoregressive neural vocoder based on fixed point iteration", "title": "WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration", "abstract": "Denoising diffusion probabilistic models (DDPMs) and generative adversarial networks (GANs) are popular generative models for neural vocoders. The DDPMs and GANs can be characterized by the iterative denoising framework and adversarial training, respectively. This study proposes a fast and high-quality neural vocoder called \\textit{WaveFit}, which integrates the essence of GANs into a DDPM-like iterative framework based on fixed-point iteration. WaveFit iteratively denoises an input signal, and trains a deep neural network (DNN) for minimizing an adversarial loss calculated from intermediate outputs at all iterations. Subjective (side-by-side) listening tests showed no statistically significant differences in naturalness between human natural speech and those synthesized by WaveFit with five iterations. Furthermore, the inference speed of WaveFit was more than 240 times faster than WaveRNN. Audio demos are available at \\url{google.github.io/df-conformer/wavefit/}.", "source": "arxiv", "source_id": "2210.01029v1", "match_status": "exact_title", "missing_reason": null}
{"key": "kong2020hifi", "query_title": "{HiFi-GAN}: Generative adversarial networks for efficient and high fidelity speech synthesis", "normalized_title": "hifi gan generative adversarial networks for efficient and high fidelity speech synthesis", "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis", "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.", "source": "arxiv", "source_id": "2010.05646v2", "match_status": "exact_title", "missing_reason": null}
{"key": "kreuk2022textless", "query_title": "Textless speech emotion conversion using discrete \\& decomposed representations", "normalized_title": "textless speech emotion conversion using discrete and decomposed representations", "title": "Textless Speech Emotion Conversion using Discrete & Decomposed Representations", "abstract": "Speech emotion conversion is the task of modifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity. In this study, we cast the problem of emotion conversion as a spoken language translation task. We use a decomposition of the speech signal into discrete learned representations, consisting of phonetic-content units, prosodic features, speaker, and emotion. First, we modify the speech content by translating the phonetic-content units to a target emotion, and then predict the prosodic features based on these units. Finally, the speech waveform is generated by feeding the predicted representations into a neural vocoder. Such a paradigm allows us to go beyond spectral and parametric changes of the signal, and model non-verbal vocalizations, such as laughter insertion, yawning removal, etc. We demonstrate objectively and subjectively that the proposed method is vastly superior to current approaches and even beats text-based systems in terms of perceived emotion and audio quality. We rigorously evaluate all components of such a complex system and conclude with an extensive model analysis and ablation study to better emphasize the architectural choices, strengths and weaknesses of the proposed method. Samples are available under the following link: https://speechbot.github.io/emotion", "source": "semantic_scholar", "source_id": "44ff1432414805bbbff7b7c814c758989d3028c9", "match_status": "exact_title", "missing_reason": null}
{"key": "kuan2024largeaudiolanguagemodelstruly", "query_title": "Can large audio-language models truly hear? {Tackling} hallucinations with multi-task assessment and stepwise audio reasoning", "normalized_title": "can large audio language models truly hear tackling hallucinations with multi task assessment and stepwise audio reasoning", "title": "Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with Multi-Task Assessment and Stepwise Audio Reasoning", "abstract": "Recent advancements in large audio-language models (LALMs) have shown impressive capabilities in understanding and reasoning about audio and speech information. However, these models still face challenges, including hallucinating non-existent sound events, misidentifying the order of sound events, and incorrectly attributing sound sources, which undermine their reliability and real-world application. To systematically evaluate these issues, we propose three distinct tasks: object existence, temporal order, and object attribute within audio. These tasks assess the models' comprehension of critical audio information aspects. Our experimental results reveal limitations in these fundamental tasks, underscoring the need for better models in recognizing specific sound events, determining event sequences, and identifying sound sources. To improve performance in these areas, we introduce a multi-turn chain-of-thought approach, which demonstrates significantly improved model performance across the proposed tasks.", "source": "arxiv", "source_id": "2410.16130v2", "match_status": "exact_title", "missing_reason": null}
{"key": "kuan2024audiohallucination", "query_title": "Understanding sounds, missing the questions: The challenge of object hallucination in large audio-language models", "normalized_title": "understanding sounds missing the questions the challenge of object hallucination in large audio language models", "title": "Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models", "abstract": "Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions.", "source": "arxiv", "source_id": "2406.08402v1", "match_status": "exact_title", "missing_reason": null}
{"key": "parp", "query_title": "{PARP}: Prune, adjust and re-prune for self-supervised speech recognition", "normalized_title": "parp prune adjust and re prune for self supervised speech recognition", "title": "PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition", "abstract": "Self-supervised speech representation learning (speech SSL) has demonstrated the benefit of scale in learning rich representations for Automatic Speech Recognition (ASR) with limited paired data, such as wav2vec 2.0. We investigate the existence of sparse subnetworks in pre-trained speech SSL models that achieve even better low-resource ASR results. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, we show that the discovered subnetworks yield minimal performance gain compared to the original dense network. We present Prune-Adjust-Re-Prune (PARP), which discovers and finetunes subnetworks for much better performance, while only requiring a single downstream ASR finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks need merely a slight adjustment to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource ASR verify (1) sparse subnetworks exist in mono-lingual/multi-lingual pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods. In particular, on the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We further demonstrate the effectiveness of PARP via: cross-lingual pruning without any phone recognition degradation, the discovery of a multi-lingual subnetwork for 10 spoken languages in 1 finetuning run, and its applicability to pre-trained BERT/XLNet for natural language tasks.", "source": "arxiv", "source_id": "2106.05933v2", "match_status": "exact_title", "missing_reason": null}
{"key": "lai2023instruction", "query_title": "Instruction-following speech recognition", "normalized_title": "instruction following speech recognition", "title": "Instruction-Following Speech Recognition", "abstract": "Conventional end-to-end Automatic Speech Recognition (ASR) models primarily focus on exact transcription tasks, lacking flexibility for nuanced user interactions. With the advent of Large Language Models (LLMs) in speech processing, more organic, text-prompt-based interactions have become possible. However, the mechanisms behind these models' speech understanding and \"reasoning\" capabilities remain underexplored. To study this question from the data perspective, we introduce instruction-following speech recognition, training a Listen-Attend-Spell model to understand and execute a diverse set of free-form text instructions. This enables a multitude of speech recognition tasks -- ranging from transcript manipulation to summarization -- without relying on predefined command sets. Remarkably, our model, trained from scratch on Librispeech, interprets and executes simple instructions without requiring LLMs or pre-trained speech modules. It also offers selective transcription options based on instructions like \"transcribe first half and then turn off listening,\" providing an additional layer of privacy and safety compared to existing LLMs. Our findings highlight the significant potential of instruction-following training to advance speech foundation models.", "source": "arxiv", "source_id": "2309.09843v1", "match_status": "exact_title", "missing_reason": null}
{"key": "GSLM", "query_title": "On generative spoken language modeling from raw audio", "normalized_title": "on generative spoken language modeling from raw audio", "title": "On Generative Spoken Language Modeling from Raw Audio", "abstract": "Abstract We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1", "source": "semantic_scholar", "source_id": "7c39adb2049e79951dd6b92c970abaa4d81819b1", "match_status": "exact_title", "missing_reason": null}
{"key": "latif2023sparks", "query_title": "Sparks of large audio models: A survey and outlook", "normalized_title": "sparks of large audio models a survey and outlook", "title": "Sparks of Large Audio Models: A Survey and Outlook", "abstract": "This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \\textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \\textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \\textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.", "source": "arxiv", "source_id": "2308.12792v3", "match_status": "exact_title", "missing_reason": null}
{"key": "the2024large", "query_title": "Large concept models: Language modeling in a sentence representation space", "normalized_title": "large concept models language modeling in a sentence representation space", "title": "Large Concept Models: Language Modeling in a Sentence Representation Space", "abstract": "LLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for many tasks. The current established technology of LLMs is to process input and generate output at the token level. This is in sharp contrast to humans who operate at multiple levels of abstraction, well beyond single words, to analyze information and to generate creative content. In this paper, we present an attempt at an architecture which operates on an explicit higher-level semantic representation, which we name a concept. Concepts are language- and modality-agnostic and represent a higher level idea or action in a flow. Hence, we build a \"Large Concept Model\". In this study, as proof of feasibility, we assume that a concept corresponds to a sentence, and use an existing sentence embedding space, SONAR, which supports up to 200 languages in both text and speech modalities. The Large Concept Model is trained to perform autoregressive sentence prediction in an embedding space. We explore multiple approaches, namely MSE regression, variants of diffusion-based generation, and models operating in a quantized SONAR space. These explorations are performed using 1.6B parameter models and training data in the order of 1.3T tokens. We then scale one architecture to a model size of 7B parameters and training data of about 2.7T tokens. We perform an experimental evaluation on several generative tasks, namely summarization and a new task of summary expansion. Finally, we show that our model exhibits impressive zero-shot generalization performance to many languages, outperforming existing LLMs of the same size. The training code of our models is freely available.", "source": "arxiv", "source_id": "2412.08821v2", "match_status": "exact_title", "missing_reason": null}
{"key": "le2024voicebox", "query_title": "Voicebox: Text-guided multilingual universal speech generation at scale", "normalized_title": "voicebox text guided multilingual universal speech generation at scale", "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale", "abstract": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}.", "source": "arxiv", "source_id": "2306.15687v2", "match_status": "exact_title", "missing_reason": null}
{"key": "lee2022direct", "query_title": "Direct speech-to-speech translation with discrete units", "normalized_title": "direct speech to speech translation with discrete units", "title": "Direct speech-to-speech translation with discrete units", "abstract": "We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequence-to-sequence speech-to-unit translation (S2UT) model to predict the discrete representations of the target speech. When target text transcripts are available, we design a joint speech and text training framework that enables the model to generate dual modality output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features. When trained without any text transcripts, our model performance is comparable to models that predict spectrograms and are trained with text supervision, showing the potential of our system for translation between unwritten languages. Audio samples are available at https://facebookresearch.github.io/speech_translation/direct_s2st_units/index.html .", "source": "arxiv", "source_id": "2107.05604v2", "match_status": "exact_title", "missing_reason": null}
{"key": "lee2022textless", "query_title": "Textless speech-to-speech translation on real data", "normalized_title": "textless speech to speech translation on real data", "title": "Textless Speech-to-Speech Translation on Real Data", "abstract": "We present a textless speech-to-speech translation (S2ST) system that can translate speech from one language into another language and can be built without the need of any text data. Different from existing work in the literature, we tackle the challenge in modeling multi-speaker target speech and train the systems with real-world S2ST data. The key to our approach is a self-supervised unit-based speech normalization technique, which finetunes a pre-trained speech encoder with paired audios from multiple speakers and a single reference speaker to reduce the variations due to accents, while preserving the lexical content. With only 10 minutes of paired data for speech normalization, we obtain on average 3.2 BLEU gain when training the S2ST model on the VoxPopuli S2ST dataset, compared to a baseline trained on un-normalized speech target. We also incorporate automatically mined S2ST data and show an additional 2.0 BLEU gain. To our knowledge, we are the first to establish a textless S2ST technique that can be trained with real-world data and works for multiple language pairs. Audio samples are available at https://facebookresearch.github.io/speech_translation/textless_s2st_real_data/index.html .", "source": "arxiv", "source_id": "2112.08352v2", "match_status": "exact_title", "missing_reason": null}
{"key": "lee2022autoregressive", "query_title": "Autoregressive image generation using residual quantization", "normalized_title": "autoregressive image generation using residual quantization", "title": "Autoregressive Image Generation using Residual Quantization", "abstract": "For autoregressive (AR) modeling of high-resolution images, vector quantization (VQ) represents an image as a sequence of discrete codes. A short sequence length is important for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code sequence and generate high-fidelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes. Then, RQ-Transformer learns to predict the quantized feature vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256$\\times$256 image as 8$\\times$8 resolution of the feature map, and RQ-Transformer can efficiently reduce the computational costs. Consequently, our framework outperforms the existing AR models on various benchmarks of unconditional and conditional image generation. Our approach also has a significantly faster sampling speed than previous AR models to generate high-quality images.", "source": "arxiv", "source_id": "2203.01941v2", "match_status": "exact_title", "missing_reason": null}
{"key": "q-former", "query_title": "{BLIP-2}: Bootstrapping language-image pre-training with frozen image encoders and large language models", "normalized_title": "blip 2 bootstrapping language image pre training with frozen image encoders and large language models", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.", "source": "arxiv", "source_id": "2301.12597v3", "match_status": "exact_title", "missing_reason": null}
{"key": "lin2024advancing", "query_title": "Advancing large language models to capture varied speaking styles and respond properly in spoken conversations", "normalized_title": "advancing large language models to capture varied speaking styles and respond properly in spoken conversations", "title": "Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations", "abstract": "In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that \"even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different\". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different. To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles. Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech LLMs methods.", "source": "arxiv", "source_id": "2402.12786v2", "match_status": "exact_title", "missing_reason": null}
{"key": "lin2024alignslmtextlessspokenlanguage", "query_title": "{Align-SLM}: Textless spoken language models with reinforcement learning from {AI} feedback", "normalized_title": "align slm textless spoken language models with reinforcement learning from ai feedback", "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback", "abstract": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.", "source": "arxiv", "source_id": "2411.01834v2", "match_status": "exact_title", "missing_reason": null}
{"key": "biasspeechSLT24", "query_title": "Spoken stereoset: On evaluating social bias toward speaker in speech large language models", "normalized_title": "spoken stereoset on evaluating social bias toward speaker in speech large language models", "title": "Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models", "abstract": "Warning: This paper may contain texts with uncomfortable content. Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.", "source": "arxiv", "source_id": "2408.07665v1", "match_status": "exact_title", "missing_reason": null}
{"key": "biascontentSLT24", "query_title": "Listen and speak fairly: A study on semantic gender bias in speech integrated large language models", "normalized_title": "listen and speak fairly a study on semantic gender bias in speech integrated large language models", "title": "Listen and Speak Fairly: A Study on Semantic Gender Bias in Speech Integrated Large Language Models", "abstract": "Speech Integrated Large Language Models (SILLMs) combine large language models with speech perception to perform diverse tasks, such as emotion recognition to speaker verification, demonstrating universal audio understanding capability. However, these models may amplify biases present in training data, potentially leading to biased access to information for marginalized groups. This work introduces a curated spoken bias evaluation toolkit and corresponding dataset. We evaluate gender bias in SILLMs across four semantic-related tasks: speech-to-text translation (STT), spoken coreference resolution (SCR), spoken sentence continuation (SSC), and spoken question answering (SQA). Our analysis reveals that bias levels are language-dependent and vary with different evaluation methods. Our findings emphasize the necessity of employing multiple approaches to comprehensively assess biases in SILLMs, providing insights for developing fairer SILLM systems.", "source": "arxiv", "source_id": "2407.06957v1", "match_status": "exact_title", "missing_reason": null}
{"key": "prompt1", "query_title": "Pre-train, prompt, and predict: {A} systematic survey of prompting methods in natural language processing", "normalized_title": "pre train prompt and predict a systematic survey of prompting methods in natural language processing", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "abstract": "This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub \"prompt-based learning\". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.", "source": "arxiv", "source_id": "2107.13586v1", "match_status": "exact_title", "missing_reason": null}
{"key": "lo2019mosnet", "query_title": "{MOSNet}: Deep learning-based objective assessment for voice conversion", "normalized_title": "mosnet deep learning based objective assessment for voice conversion", "title": "MOSNet: Deep Learning based Objective Assessment for Voice Conversion", "abstract": "Existing objective evaluation metrics for voice conversion (VC) are not always correlated with human perception. Therefore, training VC models with such criteria may not effectively improve naturalness and similarity of converted speech. In this paper, we propose deep learning-based assessment models to predict human ratings of converted speech. We adopt the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor, termed as MOSNet. The proposed models are tested on large-scale listening test results of the Voice Conversion Challenge (VCC) 2018. Experimental results show that the predicted scores of the proposed MOSNet are highly correlated with human MOS ratings at the system level while being fairly correlated with human MOS ratings at the utterance level. Meanwhile, we have modified MOSNet to predict the similarity scores, and the preliminary results show that the predicted scores are also fairly correlated with human ratings. These results confirm that the proposed models could be used as a computational evaluator to measure the MOS of VC systems to reduce the need for expensive human rating.", "source": "arxiv", "source_id": "1904.08352v3", "match_status": "exact_title", "missing_reason": null}
{"key": "desta", "query_title": "{DeSTA}: Enhancing speech language models through descriptive speech-text alignment", "normalized_title": "desta enhancing speech language models through descriptive speech text alignment", "title": "DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment", "abstract": "Recent speech language models (SLMs) typically incorporate pre-trained speech models to extend the capabilities from large language models (LLMs). In this paper, we propose a Descriptive Speech-Text Alignment approach that leverages speech captioning to bridge the gap between speech and text modalities, enabling SLMs to interpret and generate comprehensive natural language descriptions, thereby facilitating the capability to understand both linguistic and non-linguistic features in speech. Enhanced with the proposed approach, our model demonstrates superior performance on the Dynamic-SUPERB benchmark, particularly in generalizing to unseen tasks. Moreover, we discover that the aligned model exhibits a zero-shot instruction-following capability without explicit speech instruction tuning. These findings highlight the potential to reshape instruction-following SLMs by incorporating rich, descriptive speech captions.", "source": "arxiv", "source_id": "2406.18871v1", "match_status": "exact_title", "missing_reason": null}
{"key": "desta2", "query_title": "Developing instruction-following speech language model without speech instruction-tuning data", "normalized_title": "developing instruction following speech language model without speech instruction tuning data", "title": "DeSTA2: Developing Instruction-Following Speech Language Model Without Speech Instruction-Tuning Data", "abstract": "Recent end-to-end speech language models (SLMs) have expanded upon the capabilities of large language models (LLMs) by incorporating pre-trained speech models. However, these SLMs often undergo extensive speech instruction-tuning to bridge the gap between speech and text modalities. This requires significant annotation efforts and risks catastrophic forgetting of the original language capabilities. In this work, we present a simple yet effective automatic process for creating speech-text pair data that carefully injects speech paralinguistic understanding abilities into SLMs while preserving the inherent language capabilities of the text-based LLM. Our model demonstrates general capabilities for speech-related tasks without the need for speech instruction-tuning data, achieving impressive performance on Dynamic-SUPERB and AIR-Bench-Chat benchmarks. Furthermore, our model exhibits the ability to follow complex instructions derived from LLMs, such as specific output formatting and chain-of-thought reasoning. Our approach not only enhances the versatility and effectiveness of SLMs but also reduces reliance on extensive annotated datasets, paving the way for more efficient and capable speech understanding systems.", "source": "arxiv", "source_id": "2409.20007v2", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "ma2024languagemodellistenspeaking", "query_title": "Language model can listen while speaking", "normalized_title": "language model can listen while speaking", "title": "Language Model Can Listen While Speaking", "abstract": "Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM), have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategiesearly fusion, middle fusion, and late fusionare explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLMs robustness to noise and sensitivity to diverse instructions. Our results highlight LSLMs capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.", "source": "openalex", "source_id": "https://openalex.org/W4409348167", "match_status": "exact_title", "missing_reason": null}
{"key": "maimon2023speaking", "query_title": "Speaking style conversion in the waveform domain using discrete self-supervised units", "normalized_title": "speaking style conversion in the waveform domain using discrete self supervised units", "title": "Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units", "abstract": "We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people's unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.", "source": "arxiv", "source_id": "2212.09730v2", "match_status": "exact_title", "missing_reason": null}
{"key": "maimon2024suite", "query_title": "A suite for acoustic language model evaluation", "normalized_title": "a suite for acoustic language model evaluation", "title": "Salmon: A Suite for Acoustic Language Model Evaluation", "abstract": "Speech language models have recently demonstrated great potential as universal speech processing systems. Such models have the ability to model the rich acoustic information existing in audio signals, beyond spoken content, such as emotion, background noise, etc. Despite this, evaluation benchmarks which evaluate awareness to a wide range of acoustic aspects, are lacking. To help bridge this gap, we introduce SALMon, a novel evaluation suite encompassing background noise, emotion, speaker identity and room impulse response. The proposed benchmarks both evaluate the consistency of the inspected element and how much it matches the spoken text. We follow a modelling based approach, measuring whether a model gives correct samples higher scores than incorrect ones. This approach makes the benchmark fast to compute even for large models. We evaluated several speech language models on SALMon, thus highlighting the strengths and weaknesses of each evaluated method. We make the code and data publicly available at https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .", "source": "arxiv", "source_id": "2409.07437v3", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "maimon2025slamming", "query_title": "Slamming: Training a speech language model on one gpu in a day", "normalized_title": "slamming training a speech language model on one gpu in a day", "title": "Slamming: Training a Speech Language Model on One GPU in a Day", "abstract": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .", "source": "arxiv", "source_id": "2502.15814v2", "match_status": "exact_title", "missing_reason": null}
{"key": "maiti2024voxtlm", "query_title": "{VoxtLM}: Unified decoder-only models for consolidating speech recognition, synthesis and speech, text continuation tasks", "normalized_title": "voxtlm unified decoder only models for consolidating speech recognition synthesis and speech text continuation tasks", "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks", "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.", "source": "arxiv", "source_id": "2309.07937v3", "match_status": "exact_title", "missing_reason": null}
{"key": "masumura-etal-2018-neural", "query_title": "Neural dialogue context online end-of-turn detection", "normalized_title": "neural dialogue context online end of turn detection", "title": "Neural Dialogue Context Online End-of-Turn Detection", "abstract": "This paper proposes a fully neural network based dialogue-context online end-of-turn detection method that can utilize long-range interactive information extracted from both speakers utterances and collocutors utterances. The proposed method combines multiple time-asynchronous long short-term memory recurrent neural networks, which can capture speakers and collocutors multiple sequential features, and their interactions. On the assumption of applying the proposed method to spoken dialogue systems, we introduce speakers acoustic sequential features and collocutors linguistic sequential features, each of which can be extracted in an online manner. Our evaluation confirms the effectiveness of taking dialogue context formed by the speakers utterances and collocutors utterances into consideration.", "source": "semantic_scholar", "source_id": "31e63d59c8b86c2b153b7e57711efc4da2e06913", "match_status": "exact_title", "missing_reason": null}
{"key": "MEENA2014903", "query_title": "Data-driven models for timing feedback responses in a map task dialogue system", "normalized_title": "data driven models for timing feedback responses in a map task dialogue system", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "Parrot2024", "query_title": "Parrot: Autoregressive spoken dialogue language modeling with decoder-only transformers", "normalized_title": "parrot autoregressive spoken dialogue language modeling with decoder only transformers", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "messica2024nast", "query_title": "{NAST}: Noise aware speech tokenization for speech language models", "normalized_title": "nast noise aware speech tokenization for speech language models", "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models", "abstract": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.", "source": "arxiv", "source_id": "2406.11037v1", "match_status": "exact_title", "missing_reason": null}
{"key": "phi4mini", "query_title": "Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras", "normalized_title": "phi 4 mini technical report compact yet powerful multimodal language models via mixture of loras", "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.", "source": "arxiv", "source_id": "2503.01743v2", "match_status": "exact_title", "missing_reason": null}
{"key": "mohamed2022self", "query_title": "Self-supervised speech representation learning: A review", "normalized_title": "self supervised speech representation learning a review", "title": "Self-Supervised Speech Representation Learning: A Review", "abstract": "Although supervised deep learning has revolutionized speech and audio processing, it has necessitated the building of specialist models for individual tasks and application scenarios. It is likewise difficult to apply this to dialects and languages for which only limited labeled data is available. Self-supervised representation learning methods promise a single universal model that would benefit a wide variety of tasks and domains. Such methods have shown success in natural language processing and computer vision domains, achieving new levels of performance while reducing the number of labels required for many downstream scenarios. Speech representation learning is experiencing similar progress in three main categories: generative, contrastive, and predictive methods. Other approaches rely on multi-modal data for pre-training, mixing text or visual data streams with speech. Although self-supervised speech representation is still a nascent research area, it is closely related to acoustic word embedding and learning with zero lexical resources, both of which have seen active research for many years. This review presents approaches for self-supervised speech representation learning and their connection to other research areas. Since many current methods focus solely on automatic speech recognition as a downstream task, we review recent efforts on benchmarking learned representations to extend the application beyond speech recognition.", "source": "arxiv", "source_id": "2205.10643v3", "match_status": "exact_title", "missing_reason": null}
{"key": "mostafazadeh2016corpus", "query_title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "normalized_title": "a corpus and cloze evaluation for deeper understanding of commonsense stories", "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.", "source": "semantic_scholar", "source_id": "85b68477a6e031d88b963833e15a4b4fc6855264", "match_status": "exact_title", "missing_reason": null}
{"key": "mousavi2024dasb", "query_title": "{DASB}--discrete audio and speech benchmark", "normalized_title": "dasb discrete audio and speech benchmark", "title": "DASB - Discrete Audio and Speech Benchmark", "abstract": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.", "source": "arxiv", "source_id": "2406.14294v2", "match_status": "exact_title", "missing_reason": null}
{"key": "mousavi2024should", "query_title": "How should we extract discrete audio tokens from self-supervised models?", "normalized_title": "how should we extract discrete audio tokens from self supervised models", "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?", "abstract": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.", "source": "arxiv", "source_id": "2406.10735v1", "match_status": "exact_title", "missing_reason": null}
{"key": "nachmani2023spoken", "query_title": "Spoken question answering and speech continuation using spectrogram-powered {LLM}", "normalized_title": "spoken question answering and speech continuation using spectrogram powered llm", "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM", "abstract": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).", "source": "arxiv", "source_id": "2305.15255v4", "match_status": "exact_title", "missing_reason": null}
{"key": "dGSLM", "query_title": "Generative spoken dialogue language modeling", "normalized_title": "generative spoken dialogue language modeling", "title": "Generative Spoken Dialogue Language Modeling", "abstract": "We introduce dGSLM, the first \"textless\" model able to generate audio samples of naturalistic spoken dialogues. It uses recent work on unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio (Fisher dataset) without any text or labels. We show that our model is able to generate speech, laughter and other paralinguistic signals in the two channels simultaneously and reproduces more naturalistic and fluid turn-taking compared to a text-based cascaded model.", "source": "arxiv", "source_id": "2203.16502v2", "match_status": "exact_title", "missing_reason": null}
{"key": "nguyen2024spirit", "query_title": "{S}pi{R}it-{LM}: Interleaved spoken and written language model", "normalized_title": "spirit lm interleaved spoken and written language model", "title": "Spirit LM: Interleaved Spoken and Written Language Model", "abstract": "We introduce Spirit LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. Spirit LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that Spirit LM can learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification). We make available model weights and inference code.", "source": "arxiv", "source_id": "2402.05755v2", "match_status": "exact_title", "missing_reason": null}
{"key": "gpt4osafety", "query_title": "Gpt-4o system card, 2024", "normalized_title": "gpt 4o system card 2024", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "gpt4", "query_title": "{GPT-4} technical report", "normalized_title": "gpt 4 technical report", "title": "GPT-4 Technical Report", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "source": "arxiv", "source_id": "2303.08774v6", "match_status": "exact_title", "missing_reason": null}
{"key": "ouyang2022training", "query_title": "Training language models to follow instructions with human feedback", "normalized_title": "training language models to follow instructions with human feedback", "title": "Training language models to follow instructions with human feedback", "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.", "source": "arxiv", "source_id": "2203.02155v1", "match_status": "exact_title", "missing_reason": null}
{"key": "pan2023cosmic", "query_title": "{COSMIC}: Data efficient instruction-tuning for speech in-context learning", "normalized_title": "cosmic data efficient instruction tuning for speech in context learning", "title": "COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning", "abstract": "We present a cost-effective method to integrate speech into a large language model (LLM), resulting in a Contextual Speech Model with Instruction-following/in-context-learning Capabilities (COSMIC) multi-modal LLM. Using GPT-3.5, we generate Speech Comprehension Test Question-Answer (SQA) pairs from speech transcriptions for supervised instruction tuning. With under 30 million trainable parameters and only 450 hours of English speech data, COSMIC demonstrates emerging capabilities in instruction-following and in-context learning. Equipped with such capabilities, COSMIC achieves a maximum 33.18 BLEU score in 0-shot EN-to-X speech to text translation (S2TT) and a significant boost in the 1-shot setting. Additionally, there is an average 25.8\\% relative Word Error Rate (WER) reduction for 1-shot cross-domain adaptation. COSMIC exhibits a significant automatic speech recognition (ASR) accuracy gain in contextual biasing tasks due to its instruction-following capability.", "source": "arxiv", "source_id": "2311.02248v2", "match_status": "exact_title", "missing_reason": null}
{"key": "park2024long", "query_title": "Long-form speech generation with spoken language models", "normalized_title": "long form speech generation with spoken language models", "title": "Long-Form Speech Generation with Spoken Language Models", "abstract": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.", "source": "arxiv", "source_id": "2412.18603v2", "match_status": "exact_title", "missing_reason": null}
{"key": "pasad2023comparative", "query_title": "Comparative layer-wise analysis of self-supervised speech models", "normalized_title": "comparative layer wise analysis of self supervised speech models", "title": "Comparative layer-wise analysis of self-supervised speech models", "abstract": "Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose layers of interest for downstream tasks and that single-layer performance often matches or improves upon using all layers, suggesting implications for more efficient use of pre-trained models.", "source": "arxiv", "source_id": "2211.03929v3", "match_status": "exact_title", "missing_reason": null}
{"key": "peng2024surveyspeechlargelanguage", "query_title": "A survey on speech large language models", "normalized_title": "a survey on speech large language models", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "dphubert", "query_title": "{DPHuBERT}: Joint distillation and pruning of self-supervised speech models", "normalized_title": "dphubert joint distillation and pruning of self supervised speech models", "title": "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models", "abstract": "Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.", "source": "arxiv", "source_id": "2305.17651v1", "match_status": "exact_title", "missing_reason": null}
{"key": "owsm", "query_title": "Reproducing {W}hisper-style training using an open-source toolkit and publicly available data", "normalized_title": "reproducing whisper style training using an open source toolkit and publicly available data", "title": "Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data", "abstract": "Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet", "source": "semantic_scholar", "source_id": "d43338451cd8676548811e1ff8f9c92ea987c5bd", "match_status": "exact_title", "missing_reason": null}
{"key": "peng2024voicetextblenderaugmentinglargelanguage", "query_title": "{VoiceTextBlender}: Augmenting large language models with speech capabilities via single-stage joint speech-text supervised fine-tuning", "normalized_title": "voicetextblender augmenting large language models with speech capabilities via single stage joint speech text supervised fine tuning", "title": "VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning", "abstract": "Recent studies have augmented large language models (LLMs) with speech capabilities, leading to the development of speech language models (SpeechLMs). Earlier SpeechLMs focused on single-turn speech-based question answering (QA), where user input comprised a speech context and a text question. More recent studies have extended this to multi-turn conversations, though they often require complex, multi-stage supervised fine-tuning (SFT) with diverse data. Another critical challenge with SpeechLMs is catastrophic forgetting, where models optimized for speech tasks suffer significant degradation in text-only performance. To mitigate these issues, we propose a novel single-stage joint speech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone. Our joint SFT combines text-only SFT data with three types of speech-related data: speech recognition and translation, speech-based QA, and mixed-modal SFT. Compared to previous SpeechLMs with 7B or 13B parameters, our 3B model demonstrates superior performance across various speech benchmarks while preserving the original capabilities on text-only tasks. Furthermore, our model shows emergent abilities of effectively handling previously unseen prompts and tasks, including multi-turn, mixed-modal inputs.", "source": "arxiv", "source_id": "2410.17485v2", "match_status": "exact_title", "missing_reason": null}
{"key": "owsm-ctc", "query_title": "{OWSM}-{CTC}: An open encoder-only speech foundation model for speech recognition, translation, and language identification", "normalized_title": "owsm ctc an open encoder only speech foundation model for speech recognition translation and language identification", "title": "OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification", "abstract": "There has been an increasing interest in large speech models that can perform multiple tasks in a single model. Such models usually adopt an encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). Compared to encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up to 24% relative improvement on ST, while it is more robust and 3 to 4 times faster for inference. OWSM-CTC also improves the long-form ASR result with 20x speed-up. We will publicly release our code, pre-trained model, and training logs to promote open science in speech foundation models.", "source": "arxiv", "source_id": "2402.12654v3", "match_status": "exact_title", "missing_reason": null}
{"key": "polyak21_interspeech", "query_title": "Speech resynthesis from discrete disentangled self-supervised representations", "normalized_title": "speech resynthesis from discrete disentangled self supervised representations", "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations", "abstract": "We propose using self-supervised discrete representations for the task of speech resynthesis. To generate disentangled representation, we separately extract low-bitrate representations for speech content, prosodic information, and speaker identity. This allows to synthesize speech in a controllable manner. We analyze various state-of-the-art, self-supervised representation learning methods and shed light on the advantages of each method while considering reconstruction quality and disentanglement properties. Specifically, we evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings' intelligibility, and overall quality using subjective human evaluation. Lastly, we demonstrate how these representations can be used for an ultra-lightweight speech codec. Using the obtained representations, we can get to a rate of 365 bits per second while providing better speech quality than the baseline methods. Audio samples can be found under the following link: speechbot.github.io/resynthesis.", "source": "arxiv", "source_id": "2104.00355v3", "match_status": "exact_title", "missing_reason": null}
{"key": "puvvada2024less", "query_title": "Less is more: Accurate speech recognition \\& translation without web-scale data", "normalized_title": "less is more accurate speech recognition and translation without web scale data", "title": "Less is More: Accurate Speech Recognition & Translation without Web-Scale Data", "abstract": "Recent advances in speech recognition and translation rely on hundreds of thousands of hours of Internet speech data. We argue that state-of-the art accuracy can be reached without relying on web-scale data. Canary - multilingual ASR and speech translation model, outperforms current state-of-the-art models - Whisper, OWSM, and Seamless-M4T on English, French, Spanish, and German languages, while being trained on an order of magnitude less data than these models. Three key factors enables such data-efficient model: (1) a FastConformer-based attention encoder-decoder architecture (2) training on synthetic data generated with machine translation and (3) advanced training techniques: data-balancing, dynamic data blending, dynamic bucketing and noise-robust fine-tuning. The model, weights, and training code will be open-sourced.", "source": "arxiv", "source_id": "2406.19674v1", "match_status": "exact_title", "missing_reason": null}
{"key": "radford2019language", "query_title": "Language models are unsupervised multitask learners, 2019", "normalized_title": "language models are unsupervised multitask learners 2019", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "whisper", "query_title": "Robust speech recognition via large-scale weak supervision", "normalized_title": "robust speech recognition via large scale weak supervision", "title": "Robust Speech Recognition via Large-Scale Weak Supervision", "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.", "source": "arxiv", "source_id": "2212.04356v1", "match_status": "exact_title", "missing_reason": null}
{"key": "rafailov2023direct", "query_title": "Direct preference optimization: Your language model is secretly a reward model", "normalized_title": "direct preference optimization your language model is secretly a reward model", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.", "source": "arxiv", "source_id": "2305.18290v3", "match_status": "exact_title", "missing_reason": null}
{"key": "turntakeingIS18", "query_title": "Investigating speech features for continuous turn-taking prediction using {LSTM}s", "normalized_title": "investigating speech features for continuous turn taking prediction using lstms", "title": "Investigating Speech Features for Continuous Turn-Taking Prediction Using LSTMs", "abstract": "For spoken dialog systems to conduct fluid conversational interactions with users, the systems must be sensitive to turn-taking cues produced by a user. Models should be designed so that effective decisions can be made as to when it is appropriate, or not, for the system to speak. Traditional end-of-turn models, where decisions are made at utterance end-points, are limited in their ability to model fast turn-switches and overlap. A more flexible approach is to model turn-taking in a continuous manner using RNNs, where the system predicts speech probability scores for discrete frames within a future window. The continuous predictions represent generalized turn-taking behaviors observed in the training data and can be applied to make decisions that are not just limited to end-of-turn detection. In this paper, we investigate optimal speech-related feature sets for making predictions at pauses and overlaps in conversation. We find that while traditional acoustic features perform well, part-of-speech features generally perform worse than word features. We show that our current models outperform previously reported baselines.", "source": "semantic_scholar", "source_id": "ede28e0b075c294c5d66a75711c960c84f2fd234", "match_status": "exact_title", "missing_reason": null}
{"key": "sakshi2024mmau", "query_title": "{MMAU}: A massive multi-task audio understanding and reasoning benchmark", "normalized_title": "mmau a massive multi task audio understanding and reasoning benchmark", "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark", "abstract": "The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.", "source": "arxiv", "source_id": "2410.19168v1", "match_status": "exact_title", "missing_reason": null}
{"key": "schatz2013evaluating", "query_title": "Evaluating speech features with the minimal-pair {ABX} task: Analysis of the classical {MFC}/{PLP} pipeline", "normalized_title": "evaluating speech features with the minimal pair abx task analysis of the classical mfc plp pipeline", "title": "Evaluating speech features with the minimal-pair ABX task: analysis of the classical MFC/PLP pipeline", "abstract": "We present a new framework for the evaluation of speech representations in zero-resource settings, that extends and complements previous work by Carlin, Jansen and Hermansky [1].In particular, we replace their Same/Different discrimination task by several Minimal-Pair ABX (MP-ABX) tasks.We explain the analytical advantages of this new framework and apply it to decompose the standard signal processing pipelines for computing PLP and MFC coefficients.This method enables us to confirm and quantify a variety of well-known and not-so-well-known results in a single framework.", "source": "openalex", "source_id": "https://openalex.org/W2395899413", "match_status": "exact_title", "missing_reason": null}
{"key": "aBPE", "query_title": "Acoustic {BPE} for speech generation with discrete tokens", "normalized_title": "acoustic bpe for speech generation with discrete tokens", "title": "Acoustic BPE for Speech Generation with Discrete Tokens", "abstract": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.", "source": "arxiv", "source_id": "2310.14580v4", "match_status": "exact_title", "missing_reason": null}
{"key": "shen2018natural", "query_title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions", "normalized_title": "natural tts synthesis by conditioning wavenet on mel spectrogram predictions", "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions", "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and $F_0$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.", "source": "arxiv", "source_id": "1712.05884v2", "match_status": "exact_title", "missing_reason": null}
{"key": "shi2024espnet", "query_title": "{ESPnet-Codec}: Comprehensive training and evaluation of neural codecs for audio, music, and speech", "normalized_title": "espnet codec comprehensive training and evaluation of neural codecs for audio music and speech", "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech", "abstract": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.", "source": "arxiv", "source_id": "2409.15897v2", "match_status": "exact_title", "missing_reason": null}
{"key": "shon2022slue", "query_title": "{SLUE}: New benchmark tasks for spoken language understanding evaluation on natural speech", "normalized_title": "slue new benchmark tasks for spoken language understanding evaluation on natural speech", "title": "SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech", "abstract": "Progress in speech processing has been facilitated by shared datasets and benchmarks. Historically these have focused on automatic speech recognition (ASR), speaker identification, or other lower-level tasks. Interest has been growing in higher-level spoken language understanding tasks, including using end-to-end models, but there are fewer annotated datasets for such tasks. At the same time, recent work shows the possibility of pre-training generic representations and then fine-tuning for several tasks using relatively little labeled data. We propose to create a suite of benchmark tasks for Spoken Language Understanding Evaluation (SLUE) consisting of limited-size labeled training sets and corresponding evaluation sets. This resource would allow the research community to track progress, evaluate pre-trained representations for higher-level tasks, and study open questions such as the utility of pipeline versus end-to-end approaches. We present the first phase of the SLUE benchmark suite, consisting of named entity recognition, sentiment analysis, and ASR on the corresponding datasets. We focus on naturally produced (not read or synthesized) speech, and freely available datasets. We provide new transcriptions and annotations on subsets of the VoxCeleb and VoxPopuli datasets, evaluation metrics and results for baseline models, and an open-source toolkit to reproduce the baselines and evaluate new models.", "source": "arxiv", "source_id": "2111.10367v3", "match_status": "exact_title", "missing_reason": null}
{"key": "shon2024discreteslu", "query_title": "Discrete{SLU}: A large language model with self-supervised discrete speech units for spoken language understanding", "normalized_title": "discreteslu a large language model with self supervised discrete speech units for spoken language understanding", "title": "DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding", "abstract": "The integration of pre-trained text-based large language models (LLM) with speech input has enabled instruction-following capabilities for diverse speech tasks. This integration requires the use of a speech encoder, a speech adapter, and an LLM, trained on diverse tasks. We propose the use of discrete speech units (DSU), rather than continuous-valued speech encoder outputs, that are converted to the LLM token embedding space using the speech adapter. We generate DSU using a self-supervised speech encoder followed by k-means clustering. The proposed model shows robust performance on speech inputs from seen/unseen domains and instruction-following capability in spoken question answering. We also explore various types of DSU extracted from different layers of the self-supervised speech encoder, as well as Mel frequency Cepstral Coefficients (MFCC). Our findings suggest that the ASR task and datasets are not crucial in instruction-tuning for spoken question answering tasks.", "source": "semantic_scholar", "source_id": "4626971b4b3a7ae5cdae61ca81ff97e06fc365eb", "match_status": "exact_title", "missing_reason": null}
{"key": "shu2023llasm", "query_title": "{LLaSM}: Large language and speech model", "normalized_title": "llasm large language and speech model", "title": "LLaSM: Large Language and Speech Model", "abstract": "Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions dataset is available at https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.", "source": "arxiv", "source_id": "2308.15930v3", "match_status": "exact_title", "missing_reason": null}
{"key": "sicherman2023analysing", "query_title": "Analysing discrete self supervised speech representation for spoken language modeling", "normalized_title": "analysing discrete self supervised speech representation for spoken language modeling", "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling", "abstract": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations", "source": "arxiv", "source_id": "2301.00591v3", "match_status": "exact_title", "missing_reason": null}
{"key": "siuzdak2024snac", "query_title": "Snac: Multi-scale neural audio codec", "normalized_title": "snac multi scale neural audio codec", "title": "SNAC: Multi-Scale Neural Audio Codec", "abstract": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.", "source": "arxiv", "source_id": "2410.14411v1", "match_status": "exact_title", "missing_reason": null}
{"key": "skantze-2017-towards", "query_title": "Towards a general, continuous model of turn-taking in spoken dialogue using {LSTM} recurrent neural networks", "normalized_title": "towards a general continuous model of turn taking in spoken dialogue using lstm recurrent neural networks", "title": "Towards a General, Continuous Model of Turn-taking in Spoken Dialogue using LSTM Recurrent Neural Networks", "abstract": "Previous models of turn-taking have mostly been trained for specific turn-taking decisions, such as discriminating between turn shifts and turn retention in pauses. In this paper, we present a predictive, continuous model of turn-taking using Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN). The model is trained on human-human dialogue data to predict upcoming speech activity in a future time window. We show how this general model can be applied to two different tasks that it was not specifically trained for. First, to predict whether a turn-shift will occur or not in pauses, where the model achieves a better performance than human observers, and better than results achieved with more traditional models. Second, to make a prediction at speech onset whether the utterance will be a short backchannel or a longer utterance. Finally, we show how the hidden layer in the network can be used as a feature vector for turn-taking decisions in a human-robot interaction scenario.", "source": "semantic_scholar", "source_id": "04af90e58c35798bf7c0f80c9f5457bb06fe01f5", "match_status": "exact_title", "missing_reason": null}
{"key": "bigbench", "query_title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models", "normalized_title": "beyond the imitation game quantifying and extrapolating the capabilities of language models", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models", "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.", "source": "arxiv", "source_id": "2206.04615v3", "match_status": "exact_title", "missing_reason": null}
{"key": "tang2023salmonn", "query_title": "{SALMONN}: Towards generic hearing abilities for large language models", "normalized_title": "salmonn towards generic hearing abilities for large language models", "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models", "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.", "source": "arxiv", "source_id": "2310.13289v2", "match_status": "exact_title", "missing_reason": null}
{"key": "tian2025espnet", "query_title": "Espnet-speechlm: An open speech language model toolkit", "normalized_title": "espnet speechlm an open speech language model toolkit", "title": "ESPnet-SpeechLM: An Open Speech Language Model Toolkit", "abstract": "We present ESPnet-SpeechLM, an open toolkit designed to democratize the development of speech language models (SpeechLMs) and voice-driven agentic applications. The toolkit standardizes speech processing tasks by framing them as universal sequential modeling problems, encompassing a cohesive workflow of data preprocessing, pre-training, inference, and task evaluation. With ESPnet-SpeechLM, users can easily define task templates and configure key settings, enabling seamless and streamlined SpeechLM development. The toolkit ensures flexibility, efficiency, and scalability by offering highly configurable modules for every stage of the workflow. To illustrate its capabilities, we provide multiple use cases demonstrating how competitive SpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter model pre-trained on both text and speech tasks, across diverse benchmarks. The toolkit and its recipes are fully transparent and reproducible at: https://github.com/espnet/espnet/tree/speechlm.", "source": "arxiv", "source_id": "2502.15218v2", "match_status": "exact_title", "missing_reason": null}
{"key": "touvron2023llama", "query_title": "{LLaMA}: Open and efficient foundation language models", "normalized_title": "llama open and efficient foundation language models", "title": "LLaMA: Open and Efficient Foundation Language Models", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "source": "arxiv", "source_id": "2302.13971v1", "match_status": "exact_title", "missing_reason": null}
{"key": "touvron2023llama2", "query_title": "{LLaMA} 2: Open foundation and fine-tuned chat models", "normalized_title": "llama 2 open foundation and fine tuned chat models", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "source": "arxiv", "source_id": "2307.09288v2", "match_status": "exact_title", "missing_reason": null}
{"key": "tsunoo24_interspeech", "query_title": "Decoder-only architecture for streaming end-to-end speech recognition", "normalized_title": "decoder only architecture for streaming end to end speech recognition", "title": "Decoder-only Architecture for Streaming End-to-end Speech Recognition", "abstract": "Decoder-only language models (LMs) have been successfully adopted for speech-processing tasks including automatic speech recognition (ASR). The LMs have ample expressiveness and perform efficiently. This efficiency is a suitable characteristic for streaming applications of ASR. In this work, we propose to use a decoder-only architecture for blockwise streaming ASR. In our approach, speech features are compressed using CTC output and context embedding using blockwise speech subnetwork, and are sequentially provided as prompts to the decoder. The decoder estimates the output tokens promptly at each block. To this end, we also propose a novel training scheme using random-length prefix prompts to make the model robust to the truncated prompts caused by blockwise processing. An experimental comparison shows that our proposed decoder-only streaming ASR achieves 8% relative word error rate reduction in the LibriSpeech test-other set while being twice as fast as the baseline model.", "source": "arxiv", "source_id": "2406.16107v2", "match_status": "exact_title", "missing_reason": null}
{"key": "turetzky2024last", "query_title": "{LAST}: Language model aware speech tokenization", "normalized_title": "last language model aware speech tokenization", "title": "LAST: Language Model Aware Speech Tokenization", "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "source": "arxiv", "source_id": "2409.03701v2", "match_status": "exact_title", "missing_reason": null}
{"key": "van2017neural", "query_title": "Neural discrete representation learning", "normalized_title": "neural discrete representation learning", "title": "Neural Discrete Representation Learning", "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.", "source": "arxiv", "source_id": "1711.00937v2", "match_status": "exact_title", "missing_reason": null}
{"key": "veluri2024turnbasedinterfacessynchronousllms", "query_title": "Beyond turn-based interfaces: Synchronous {LLM}s as full-duplex dialogue agents", "normalized_title": "beyond turn based interfaces synchronous llms as full duplex dialogue agents", "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents", "abstract": "Despite broad interest in modeling spoken dialogue agents, most approaches are inherently half-duplex  restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is full-duplex allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of time. To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the models ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms.", "source": "semantic_scholar", "source_id": "77c12b7565774bc18e420f91336d02ba5bd6309f", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2024audiobench", "query_title": "Audio{B}ench: A universal benchmark for audio large language models", "normalized_title": "audiobench a universal benchmark for audio large language models", "title": "AudioBench: A Universal Benchmark for Audio Large Language Models", "abstract": "We introduce AudioBench, a universal benchmark designed to evaluate Audio Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, among which, 7 are newly proposed datasets. The evaluation targets three main aspects: speech understanding, audio scene understanding, and voice understanding (paralinguistic). Despite recent advancements, there lacks a comprehensive benchmark for AudioLLMs on instruction following capabilities conditioned on audio signals. AudioBench addresses this gap by setting up datasets as well as desired evaluation metrics. Besides, we also evaluated the capabilities of five popular models and found that no single model excels consistently across all tasks. We outline the research outlook for AudioLLMs and anticipate that our open-sourced evaluation toolkit, data, and leaderboard will offer a robust testbed for future model developments.", "source": "arxiv", "source_id": "2406.16020v5", "match_status": "exact_title", "missing_reason": null}
{"key": "blsp", "query_title": "{BLSP}: Bootstrapping language-speech pre-training via behavior alignment of continuation writing", "normalized_title": "blsp bootstrapping language speech pre training via behavior alignment of continuation writing", "title": "BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing", "abstract": "The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios.", "source": "arxiv", "source_id": "2309.00916v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2023slm", "query_title": "{SLM}: Bridge the thin gap between speech and text foundation models", "normalized_title": "slm bridge the thin gap between speech and text foundation models", "title": "SLM: Bridge the thin gap between speech and text foundation models", "abstract": "We present a joint Speech and Language Model (SLM), a multitask, multilingual, and dual-modal model that takes advantage of pretrained foundational speech and language models. SLM freezes the pretrained foundation models to maximally preserves their capabilities, and only trains a simple adapter with just 1\\% (156M) of the foundation models' parameters. This adaptation not only leads SLM to achieve strong performance on conventional tasks such as speech recognition (ASR) and speech translation (AST), but also introduces the novel capability of zero-shot instruction-following for more diverse tasks: given a speech input and a text instruction, SLM is able to perform unseen generation tasks including contextual biasing ASR using real-time context, dialog generation, speech continuation, and question answering, etc. Our approach demonstrates that the representational gap between pretrained speech and language models might be narrower than one would expect, and can be bridged by a simple adaptation mechanism. As a result, SLM is not only efficient to train, but also inherits strong capabilities already acquired in foundation models of different modalities.", "source": "arxiv", "source_id": "2310.00230v1", "match_status": "exact_title", "missing_reason": null}
{"key": "duplextimeNuerIPS24", "query_title": "A full-duplex speech dialogue scheme based on large language models", "normalized_title": "a full duplex speech dialogue scheme based on large language models", "title": "A Full-duplex Speech Dialogue Scheme Based On Large Language Models", "abstract": "We present a generative dialogue system capable of operating in a full-duplex manner, allowing for seamless interaction. It is based on a large language model (LLM) carefully aligned to be aware of a perception module, a motor function module, and the concept of a simple finite state machine (called neural FSM) with two states. The perception and motor function modules operate in tandem, allowing the system to speak and listen to the user simultaneously. The LLM generates textual tokens for inquiry responses and makes autonomous decisions to start responding to, wait for, or interrupt the user by emitting control tokens to the neural FSM. All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time. In automatic quality evaluations simulating real-life interaction, the proposed system reduces the average conversation response latency by more than threefold compared with LLM-based half-duplex dialogue systems while responding within less than 500 milliseconds in more than 50% of evaluated interactions. Running an LLM with only 8 billion parameters, our system exhibits an 8% higher interruption precision rate than the best available commercial LLM for voice-based dialogue.", "source": "arxiv", "source_id": "2405.19487v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2024freezeomnismartlowlatency", "query_title": "{Freeze-Omni}: A smart and low latency speech-to-speech dialogue model with frozen {LLM}", "normalized_title": "freeze omni a smart and low latency speech to speech dialogue model with frozen llm", "title": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM", "abstract": "Rapidly developing large language models (LLMs) have brought tremendous intelligent applications. Especially, the GPT-4o's excellent duplex speech interaction ability has brought impressive experience to users. Researchers have recently proposed several multi-modal LLMs in this direction that can achieve user-agent speech-to-speech conversations. This paper proposes a novel speech-text multimodal LLM architecture called Freeze-Omni. Our main contribution is that the speech input and output modalities can be easily connected to a textual LLM while keeping the LLM's parameters frozen throughout the training process. We design a three-stage training strategy for modeling both the speech input and output, enabling Freeze-Omni to obtain speech-to-speech conversation ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs. Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while achieving low latency end-to-end spoken response. In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, giving Freeze-Omni a more natural style of dialogue ability between users and agents. In summary, Freeze-Omni holds great potential to conduct speech-to-speech dialogue based on a multimodal LLM under the condition of a frozen LLM, avoiding the catastrophic forgetting problem caused by limited data and training resources.", "source": "arxiv", "source_id": "2411.00774v5", "match_status": "exact_title", "missing_reason": null}
{"key": "warstadt2020blimp", "query_title": "{BL}i{MP}: The benchmark of linguistic minimal pairs for {E}nglish", "normalized_title": "blimp the benchmark of linguistic minimal pairs for english", "title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English", "abstract": "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP), 1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairsthat is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.", "source": "crossref", "source_id": "10.1162/tacl_a_00321", "match_status": "exact_title", "missing_reason": null}
{"key": "FLAN", "query_title": "Finetuned language models are zero-shot learners", "normalized_title": "finetuned language models are zero shot learners", "title": "Finetuned Language Models Are Zero-Shot Learners", "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.", "source": "arxiv", "source_id": "2109.01652v5", "match_status": "exact_title", "missing_reason": null}
{"key": "CoT", "query_title": "Chain-of-thought prompting elicits reasoning in large language models", "normalized_title": "chain of thought prompting elicits reasoning in large language models", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "source": "arxiv", "source_id": "2201.11903v6", "match_status": "exact_title", "missing_reason": null}
{"key": "WhisperSpeech", "query_title": "{WhisperSpeech}: An open source text-to-speech system built by inverting {Whisper}, 2024", "normalized_title": "whisperspeech an open source text to speech system built by inverting whisper 2024", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "wav2seq", "query_title": "{Wav2Seq}: Pre-training speech-to-text encoder-decoder models using pseudo languages", "normalized_title": "wav2seq pre training speech to text encoder decoder models using pseudo languages", "title": "Wav2Seq: Pre-training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages", "abstract": "We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task -- transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 20 language pairs for speech-to-text translation, even when competing methods use additional text data for training. Finally, on ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.", "source": "arxiv", "source_id": "2205.01086v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2024towards", "query_title": "Towards audio language modeling-an overview", "normalized_title": "towards audio language modeling an overview", "title": "Towards audio language modeling -- an overview", "abstract": "Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.", "source": "arxiv", "source_id": "2402.13236v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2024codec", "query_title": "{Codec-Superb} @ {SLT} 2024: A lightweight benchmark for neural audio codec models", "normalized_title": "codec superb at slt 2024 a lightweight benchmark for neural audio codec models", "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models", "abstract": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.", "source": "arxiv", "source_id": "2409.14085v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wu-etal-2024-codec", "query_title": "Codec-{SUPERB}: An in-depth analysis of sound codec models", "normalized_title": "codec superb an in depth analysis of sound codec models", "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models", "abstract": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.", "source": "arxiv", "source_id": "2402.13071v3", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2024ts3", "query_title": "{TS3-Codec}: Transformer-based simple streaming single codec", "normalized_title": "ts3 codec transformer based simple streaming single codec", "title": "TS3-Codec: Transformer-Based Simple Streaming Single Codec", "abstract": "Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models. While mainstream NAC models are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored. This paper introduces TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations. Under the streaming setup, the proposed TS3-Codec achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate. Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources.", "source": "arxiv", "source_id": "2411.18803v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2024codecfake", "query_title": "{CodecFake}: Enhancing anti-spoofing models against deepfake audios from codec-based speech synthesis systems", "normalized_title": "codecfake enhancing anti spoofing models against deepfake audios from codec based speech synthesis systems", "title": "CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems", "abstract": "Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker. Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues. Anti-spoofing models have been developed to detect fake speech. However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered. In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech. This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset. Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems. The proposed CodecFake dataset empowers these models to counter this challenge effectively.", "source": "arxiv", "source_id": "2406.07237v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2023decoder", "query_title": "On decoder-only architecture for speech-to-text and large language model integration", "normalized_title": "on decoder only architecture for speech to text and large language model integration", "title": "On decoder-only architecture for speech-to-text and large language model integration", "abstract": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.", "source": "arxiv", "source_id": "2307.03917v3", "match_status": "exact_title", "missing_reason": null}
{"key": "xie2024mini", "query_title": "{Mini-Omni}: Language models can hear, talk while thinking in streaming", "normalized_title": "mini omni language models can hear talk while thinking in streaming", "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming", "abstract": "Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \"Any Model Can Talk\". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.", "source": "arxiv", "source_id": "2408.16725v3", "match_status": "exact_title", "missing_reason": null}
{"key": "xie2024mini2", "query_title": "{Mini-Omni2}: Towards open-source {GPT}-4o model with vision, speech and duplex", "normalized_title": "mini omni2 towards open source gpt 4o model with vision speech and duplex", "title": "Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities", "abstract": "GPT-4o, an all-encompassing model, represents a milestone in the development of large multi-modal language models. It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction. Models from the open-source community often achieve some functionalities of GPT-4o, such as visual understanding and voice chat. Nevertheless, training a unified model that incorporates all modalities is challenging due to the complexities of multi-modal data, intricate model architectures, and training processes. In this paper, we introduce Mini-Omni2, a visual-audio assistant capable of providing real-time, end-to-end voice responses to visoin and audio queries. By integrating pretrained visual and auditory encoders, Mini-Omni2 maintains performance in individual modalities. We propose a three-stage training process to align modalities, allowing the language model to handle multi-modal inputs and outputs after training on a limited dataset. For interaction, we introduce a command-based interruption mechanism, enabling more flexible interaction with users. To the best of our knowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have similar form of functionality, and we hope it can offer valuable insights for subsequent research.", "source": "semantic_scholar", "source_id": "b4c98b8c624f288db2f1dba8a71e4c4962bff192", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "xin2024bigcodec", "query_title": "{BigCodec}: Pushing the limits of low-bitrate neural speech codec", "normalized_title": "bigcodec pushing the limits of low bitrate neural speech codec", "title": "BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec", "abstract": "We present BigCodec, a low-bitrate neural speech codec. While recent neural speech codecs have shown impressive progress, their performance significantly deteriorates at low bitrates (around 1 kbps). Although a low bitrate inherently restricts performance, other factors, such as model capacity, also hinder further improvements. To address this problem, we scale up the model size to 159M parameters that is more than 10 times larger than popular codecs with about 10M parameters. Besides, we integrate sequential models into traditional convolutional architectures to better capture temporal dependency and adopt low-dimensional vector quantization to ensure a high code utilization. Comprehensive objective and subjective evaluations show that BigCodec, with a bitrate of 1.04 kbps, significantly outperforms several existing low-bitrate codecs. Furthermore, BigCodec achieves objective performance comparable to popular codecs operating at 4-6 times higher bitrates, and even delivers better subjective perceptual quality than the ground truth.", "source": "arxiv", "source_id": "2409.05377v1", "match_status": "exact_title", "missing_reason": null}
{"key": "xu2024enablingrealtimeconversationsminimal", "query_title": "Enabling real-time conversations with minimal training costs", "normalized_title": "enabling real time conversations with minimal training costs", "title": "Enabling Real-Time Conversations with Minimal Training Costs", "abstract": "Large language models (LLMs) have demonstrated the ability to improve human efficiency through conversational interactions. Conventional LLM-powered dialogue systems, operating on a turn-based paradigm, preclude real-time interaction during response generation. To address this limitation, researchers have proposed duplex models. These models can dynamically adapt to user input, facilitating real-time interactive feedback. However, these methods typically require substantial computational resources to acquire the ability. To reduce overhead, this paper presents a new duplex decoding approach that enhances LLMs with duplex ability, requiring minimal additional training. Specifically, our method employs parallel decoding of queries and responses in conversations, effectively implementing a channel-division-multiplexing decoding strategy. Experimental results indicate that our proposed method significantly enhances the naturalness and human-likeness of user-AI interactions with minimal training costs.", "source": "arxiv", "source_id": "2409.11727v1", "match_status": "exact_title", "missing_reason": null}
{"key": "xue2024chat", "query_title": "E-chat: Emotion-sensitive spoken dialogue system with large language models", "normalized_title": "e chat emotion sensitive spoken dialogue system with large language models", "title": "E-chat: Emotion-sensitive Spoken Dialogue System with Large Language Models", "abstract": "This study focuses on emotion-sensitive spoken dialogue in human-machine speech interaction. With the advancement of Large Language Models (LLMs), dialogue systems can handle multimodal data, including audio. Recent models have enhanced the understanding of complex audio signals through the integration of various audio events. However, they are unable to generate appropriate responses based on emotional speech. To address this, we introduce the Emotional chat Model (E-chat), a novel spoken dialogue system capable of comprehending and responding to emotions conveyed from speech. This model leverages an emotion embedding extracted by a speech encoder, combined with LLMs, enabling it to respond according to different emotional contexts. Additionally, we introduce the E-chat200 dataset, designed explicitly for emotion-sensitive spoken dialogue. In various evaluation metrics, E-chat consistently outperforms baseline model, demonstrating its potential in emotional comprehension and human-machine interaction.", "source": "arxiv", "source_id": "2401.00475v3", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2023uniaudio", "query_title": "{U}ni{A}udio: Towards universal audio generation with large language models", "normalized_title": "uniaudio towards universal audio generation with large language models", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "airbench", "query_title": "{AIR-Bench}: Benchmarking large audio-language models via generative comprehension", "normalized_title": "air bench benchmarking large audio language models via generative comprehension", "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension", "abstract": "Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\\textbf{A}udio \\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.", "source": "arxiv", "source_id": "2402.07729v2", "match_status": "exact_title", "missing_reason": null}
{"key": "yu2023megabyte", "query_title": "{MEGABYTE}: Predicting million-byte sequences with multiscale transformers", "normalized_title": "megabyte predicting million byte sequences with multiscale transformers", "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers", "abstract": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.", "source": "arxiv", "source_id": "2305.07185v2", "match_status": "exact_title", "missing_reason": null}
{"key": "yu2024connecting", "query_title": "Connecting speech encoder and large language model for {ASR}", "normalized_title": "connecting speech encoder and large language model for asr", "title": "Connecting Speech Encoder and Large Language Model for ASR", "abstract": "The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.", "source": "arxiv", "source_id": "2309.13963v2", "match_status": "exact_title", "missing_reason": null}
{"key": "Flow-Omni", "query_title": "Continuous speech tokens makes llms robust multi-modality learners", "normalized_title": "continuous speech tokens makes llms robust multi modality learners", "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners", "abstract": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.", "source": "arxiv", "source_id": "2412.04917v1", "match_status": "exact_title", "missing_reason": null}
{"key": "soundstream", "query_title": "{SoundStream}: An end-to-end neural audio codec", "normalized_title": "soundstream an end to end neural audio codec", "title": "SoundStream: An End-to-End Neural Audio Codec", "abstract": "We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3kbps to 18kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate, SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.", "source": "arxiv", "source_id": "2107.03312v1", "match_status": "exact_title", "missing_reason": null}
{"key": "zellers2019hellaswag", "query_title": "{H}ella{S}wag: Can a machine really finish your sentence?", "normalized_title": "hellaswag can a machine really finish your sentence", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?", "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.", "source": "arxiv", "source_id": "1905.07830v1", "match_status": "exact_title", "missing_reason": null}
{"key": "zeng2024glm", "query_title": "{GLM-4-Voice}: Towards intelligent and human-like end-to-end spoken chatbot", "normalized_title": "glm 4 voice towards intelligent and human like end to end spoken chatbot", "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot", "abstract": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.", "source": "arxiv", "source_id": "2412.02612v1", "match_status": "exact_title", "missing_reason": null}
{"key": "zeng2024scaling", "query_title": "Scaling speech-text pre-training with synthetic interleaved data", "normalized_title": "scaling speech text pre training with synthetic interleaved data", "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data", "abstract": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.", "source": "arxiv", "source_id": "2411.17607v2", "match_status": "exact_title", "missing_reason": null}
{"key": "anygpt", "query_title": "{A}ny{GPT}: Unified multimodal {LLM} with discrete sequence modeling", "normalized_title": "anygpt unified multimodal llm with discrete sequence modeling", "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/", "source": "arxiv", "source_id": "2402.12226v5", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2023speechgpt", "query_title": "{SpeechGPT}: Empowering large language models with intrinsic cross-modal conversational abilities", "normalized_title": "speechgpt empowering large language models with intrinsic cross modal conversational abilities", "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities", "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.", "source": "arxiv", "source_id": "2305.11000v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2024mm", "query_title": "{MM}-{LLM}s: Recent advances in multimodal large language models", "normalized_title": "mm llms recent advances in multimodal large language models", "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models", "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.", "source": "semantic_scholar", "source_id": "a050c9b0c321839e4427ab9defa3463be7825ac4", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2025omniflattenendtoendgptmodel", "query_title": "Omni{F}latten: An end-to-end {GPT} model for seamless voice conversation", "normalized_title": "omniflatten an end to end gpt model for seamless voice conversation", "title": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation", "abstract": "Full-duplex spoken dialogue systems significantly surpass traditional turn-based dialogue systems, as they allow simultaneous bidirectional communication, closely mirroring human-human interactions. However, achieving low latency and natural interactions in full-duplex dialogue systems remains a significant challenge, especially considering human conversation dynamics such as interruptions, backchannels, and overlapping speech. In this paper, we introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex conversation, capable of effectively modeling the complex behaviors inherent to natural conversations with low latency. To achieve full-duplex conversation capabilities, we propose a multi-stage post-training scheme that progressively adapts a text large language model (LLM) backbone into a speech-text dialogue LLM, capable of generating text and speech in real time, without modifying the architecture of the backbone LLM. The training process comprises three stages: modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning. In all training stages, we standardize the data using a flattening operation, which enables unifying the training methods and the GPT backbone across different modalities and tasks. Our approach offers a simple modeling technique and a promising research direction for developing efficient and natural end-to-end full-duplex spoken dialogue systems. Audio samples of dialogues generated by OmniFlatten can be found at this web site (https://omniflatten.github.io/).", "source": "arxiv", "source_id": "2410.17799v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2022opt", "query_title": "{OPT}: Open pre-trained transformer language models", "normalized_title": "opt open pre trained transformer language models", "title": "OPT: Open Pre-trained Transformer Language Models", "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.", "source": "arxiv", "source_id": "2205.01068v4", "match_status": "exact_title", "missing_reason": null}
{"key": "speechtokenizer", "query_title": "{SpeechTokenizer}: Unified speech tokenizer for speech language models", "normalized_title": "speechtokenizer unified speech tokenizer for speech language models", "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models", "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.", "source": "semantic_scholar", "source_id": "5fc1a1f79ea4d1a1c6e8da1a40ae08022a6d7308", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "zhang2024turnbasedgameenablingrealtime", "query_title": "Beyond the turn-based game: Enabling real-time conversations with duplex models", "normalized_title": "beyond the turn based game enabling real time conversations with duplex models", "title": "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models", "abstract": "As large language models (LLMs) increasingly permeate daily lives, there is a growing demand for real-time interactions that mirror human conversations. Traditional turn-based chat systems driven by LLMs prevent users from verbally interacting with the system while it is generating responses. To overcome these limitations, we adapt existing LLMs to \\textit{duplex models} so that these LLMs can listen for users while generating output and dynamically adjust themselves to provide users with instant feedback. % such as in response to interruptions. Specifically, we divide the queries and responses of conversations into several time slices and then adopt a time-division-multiplexing (TDM) encoding-decoding strategy to pseudo-simultaneously process these slices. Furthermore, to make LLMs proficient enough to handle real-time conversations, we build a fine-tuning dataset consisting of alternating time slices of queries and responses as well as covering typical feedback types in instantaneous interactions. Our experiments show that although the queries and responses of conversations are segmented into incomplete slices for processing, LLMs can preserve their original performance on standard benchmarks with a few fine-tuning steps on our dataset. Automatic and human evaluation indicate that duplex models make user-AI interactions more natural and human-like, and greatly improve user satisfaction compared to vanilla LLMs. Our duplex model and dataset will be released.", "source": "arxiv", "source_id": "2406.15718v2", "match_status": "exact_title", "missing_reason": null}
{"key": "usm", "query_title": "Google {USM}: Scaling automatic speech recognition beyond 100 languages", "normalized_title": "google usm scaling automatic speech recognition beyond 100 languages", "title": "Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages", "abstract": "We introduce the Universal Speech Model (USM), a single large model that performs automatic speech recognition (ASR) across 100+ languages. This is achieved by pre-training the encoder of the model on a large unlabeled multilingual dataset of 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller labeled dataset. We use multilingual pre-training with random-projection quantization and speech-text modality matching to achieve state-of-the-art performance on downstream multilingual ASR and speech-to-text translation tasks. We also demonstrate that despite using a labeled training set 1/7-th the size of that used for the Whisper model, our model exhibits comparable or better performance on both in-domain and out-of-domain speech recognition tasks across many languages.", "source": "arxiv", "source_id": "2303.01037v3", "match_status": "exact_title", "missing_reason": null}
{"key": "zheng2023judging", "query_title": "Judging {LLM}-as-a-judge with {MT}-bench and {Chatbot Arena}", "normalized_title": "judging llm as a judge with mt bench and chatbot arena", "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.", "source": "arxiv", "source_id": "2306.05685v4", "match_status": "exact_title", "missing_reason": null}
