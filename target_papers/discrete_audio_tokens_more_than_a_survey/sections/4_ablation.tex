\section{Ablation Studies}\label{sec:ablation}

While the above discussion has extensively evaluated publicly available discrete audio tokens across various applications, conducting fair comparisons between these codecs remains challenging. The development of audio tokenizers inherently involves numerous hyperparameters, including codebook setups, quantization algorithms, and training data composition, all of which significantly influence performance. This variability in model design and implementation creates substantial obstacles for researchers attempting to make meaningful comparisons, ultimately hindering both a comprehensive understanding of existing approaches and a systematic exploration of new audio tokenizer designs.

In this section, we aim to mitigate this issue by conducting experiments in a carefully controlled setup. Specifically, we use ESPnet-Codec~\citep{shi2024espnet} as the base training framework to evaluate the effects of training data, codebook setups, quantization methods, and pre-trained model distillation.


\subsection{Experimental Setups}
To ensure reproducibility and isolate key variables in our experimental investigation, we establish a methodical framework that controls for potential confounding factors. First, we present our training data in various domains and sampling rates. Next, we establish uniform implementation protocols for model architecture and hyperparameter configuration, enabling direct performance comparisons across model variants. Building on this controlled foundation, we present a comprehensive set of experimental models, each systematically varying only the target parameters under investigation. Finally, we detail our evaluation methodology, including metrics and testing environments, to provide a consistent basis for assessing model performance and drawing meaningful conclusions about codec effectiveness.

\noindent \textbf{Training Dataset}. We prepare the dataset in three major domains, including speech, general audio, and music. All data in the three domains is sourced from the AMUSE dataset discussed in ESPnet-Codec~\citep{shi2024espnet}. The AMUSE dataset is a combination of high-quality datasets for codec training purposes. For speech, it contains DAPS, DNS Challenge 4, Commonvoice, VCTK, AISHELL3, Googlei18n-TTS corpora, and Mexican endangered languages~\citep{dubey2024icassp, kuhn2014daps, yamagishi2019cstr, shi2021highland, shi2021leveraging, amith_audio_corpus_sierra, amith_totonac, amith_yoloxochitl_mixtec, ardila2019common, shi2021aishell}. For general audio, it contains all data from the AudioSet unbalanced training set~\citep{gemmeke2017audio}. For music, it contains MusDB, Jamendo, OpenSinger, StyleSing111, M4Singer, Kiritan-singing, Oniku Kurumi Utagoe database, Natsume Singing database, Opencpop, ACE-KiSing (excluding original voices), PJS, and JSUT singing~\citep{musdb18, bogdanov2019mtg, huang2021multi, dai2023singstyle111, zhang2022m4singer, ogawa2021tohoku, wang2022opencpop, shi2024singing, koguchi2020pjs, takamichi2020jsut}. To ensure equal consideration of the three domains and reduce the effect of unbalanced data distributions, we randomly sample 1k hours of data from each domain over the AMUSE dataset. The following experiments are conducted in either one of the domain-specific datasets or a combination of all three subsets. For each set of training data, we provide both 16~kHz version and 44.1~kHz version to consider the effect of different sampling rates.

\noindent \textbf{Model Implementation}. 
For our experiments, we utilize the Descript Audio Codec (DAC) framework implemented in ESPnet-Codec~\citep{shi2024espnet, kumar2023high} as the foundation for neural-codec training. To evaluate discrete SSL approaches, we employ the discrete unit-based HiFiGAN vocoder as implemented by~\citet{yan2023espnet}.

Our ablation studies focus on two key aspects: quantization techniques and semantic distillation. For quantization, we compare three distinct approaches: RVQ, single-layer VQ, and FSQ. Additionally, following recent research demonstrating the efficacy of self-supervised learning representations as distillation targets for quantizer training (see Section~\ref{sec:literature} for more discussion), we incorporate semantic distillation variants for both our RVQ and VQ-based models to systematically evaluate its impact.

\input{tables/ablation/ablation_models}

\noindent \textbf{Summary of Models}. Candidate models are summarized in Table~\ref{tab:ablation_models}.  For each model listed in the table, we conduct the training at 16~kHz and 44.1~kHz with the corresponding dataset. Here, we mostly follow the previous literature discussed in Section~\ref{sec:literature}, where we ignore model setups with no or limited related work, such as the scenarios of using SSL-based distillation in audio or music domains or the use of Uni-HifiGAN for higher sampling rates. While we standardize core training parameters across all experimental conditions to ensure fair comparisons, we implement targeted customization for specific model variants to integrate different ablation factors. Complete documentation of both the standardized parameters and model-specific adjustments is available in our released model checkpoints\footnote{\url{https://huggingface.co/collections/espnet/codec-survey-pre-trained-models-67ce8e09568b741d1c4483c8}}.

\noindent \textbf{Evaluation Setup}. We follow the reconstruction evaluation protocol outlined in Section~\ref{ssec: reconstruction evaluation}, adapting our methodology to accommodate the specific requirements of different data domains. For evaluation data, we utilize the LibriSpeech test-clean set (speech), the AudioSet test set (general audio), and the MUSDB test set~(music).

Sampling rate considerations necessitated domain-specific evaluation approaches. For speech reconstruction, all evaluations are conducted at 16 kHz, even when testing 44.1~kHz neural codecs, to maintain consistency with the source LibriSpeech dataset. For music evaluation, we use different sampling rates based on model capabilities: 16~kHz for models designed at that native rate, and 24~kHz for 44.1~kHz neural codecs, reflecting the upper-frequency limitations in the MUSDB test set. For general audio evaluation, we align the evaluation with the codec models. These adjustments ensure fair comparisons while respecting both technical constraints and the inherent characteristics of each dataset.


\input{tables/ablation/ablation_results}

\subsection{Results and Discussion}

The results of our ablation study are shown in Table~\ref{tab:recon_speech_results_ablation} and Table~\ref{tab:recon_audio_results_ablation}. We summarize our findings as follows:

\noindent \textbf{Data Domains}. Domain alignment between training and testing data has emerged as a critical determinant of performance in discrete audio representation modeling. Our experiments confirm that reconstruction quality consistently peaks when models are evaluated on domains matching their training data. More significantly, we observe that even with carefully balanced multi-domain training datasets, models still exhibit notable performance degradation when assessed on individual domains compared to domain-specific training. These challenges have become increasingly relevant in light of recent audio foundation models, which aim for broad generalization across diverse audio types. Our findings highlight the need for two crucial research directions: \textit{developing more effective methodologies for balancing domain-specific optimization}, and \textit{addressing the fundamental challenges of cross-domain generalization in discrete audio representation learning}.

\noindent \textbf{Sampling Rate}. 
While prior research has rarely examined sampling rate effects on discrete audio representation, this gap is largely due to methodological challenges in creating controlled comparisons across different rate conditions. Our systematic ablation study addresses this limitation and reveals sampling rate as a significant factor in model performance. As shown in Table~\ref{tab:recon_speech_results_ablation} and Table~\ref{tab:recon_audio_results_ablation}, RVQ-based models consistently demonstrate performance improvements across multiple evaluation metrics when trained at 44.1 kHz, even when the reconstructed audio is downsampled to 16 kHz for evaluation. These benefits, however, are not universal across all quantization approaches. Models utilizing Finite Scalar Quantization (FSQ) actually exhibited performance degradation on several metrics when trained at higher sampling rates. This contrasting behavior indicates that the relationship between sampling rate and model effectiveness is contingent on the specific quantization methodology employed. Based on these findings, we recommend that future research on discrete audio representation should incorporate sampling rate as a critical design parameter, with careful optimization based on the selected quantization approach and target application domain.

\noindent \textbf{Distillation Effect}. Prior studies involving distillation from pre-trained models have frequently demonstrated that such distillation supports comparable or improved signal reconstruction, while providing substantial performance benefits for downstream tasks~\citep{du2023funcodec, defossez2024moshi, zhang2023speechtokenizer}. In our controlled ablation analyses, we similarly observed that incorporating distillation from pre-trained speech representations can enhance model performance on certain metrics for signal reconstruction, as demonstrated by our comparison between models trained with and without distillation (e.g., A-S vs. A-S+). However, it should be noted that the domain-specific nature of the pre-trained model may limit generalization, especially when applied to broader domains such as general audio or music, as evidenced in Table~\ref{tab:recon_audio_results_ablation}. This highlights a potential trade-off between achieving high performance in specialized tasks and maintaining broader generalization capabilities.

\noindent \textbf{Quantization Methods}. Our experiments demonstrate that different quantization methods significantly impact codec performance. The RVQ modeling consistently outperforms other quantization approaches across most reconstruction metrics. Conversely, SVQ models typically yield the poorest results. An interesting exception emerges with FSQ at 16 kHz, which surpasses RVQ in speech quality metrics measured by UTMOS and DNSMOS. Generally, RVQ demonstrates a higher potential for audio quality due to its high-fidelity reconstruction capabilities. However, we caution readers that the performance alignment between audio reconstruction and downstream applications is not guaranteed. The metrics observed in this study may not directly translate to broader application performance, and further research is needed to establish definitive correlations.
