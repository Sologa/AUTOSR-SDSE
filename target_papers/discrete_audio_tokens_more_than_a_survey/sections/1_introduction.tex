\section{Introduction}
Audio compression has been a well-established research topic since the foundations of digital communication~\citep{shannon1948mathematical, nyquist1928certain}. Traditional audio codecs, such as linear predictive coding (LPC)~\citep{itakura1968analysis, atal1970speech}, modified discrete cosine transform (MDCT)~\citep{wang2003modified}, and Code Excited Linear Prediction (CELP)~\citep{schroeder1985code, jage2016celp}, were designed to reduce redundancy and remove perceptually irrelevant information. These models have been effective in compressing raw audio signals into compact bitstreams (encoding) and then restoring them to the original signal domain (decoding). Codecs like USAC~\citep{6530580}, Opus~\citep{valin2012definition}, and EVS~\citep{dietz2015overview} combine these techniques to support a range of content types, bitrates, and sampling rates while ensuring low latency for real-time communication. These approaches rely heavily on domain knowledge, combining signal processing pipelines with hand-crafted components to achieve efficient but lossy compression.

Traditional codecs are efficient and optimized for perceptual quality, but their design requires substantial manual effort, including parameter tuning and subjective listening tests~\citep{valin2012definition, dietz2015overview}. This has motivated a shift toward data-driven approaches with deep learning, known as neural codecs. Neural codecs consist of an encoder, decoder, and a quantization module, closely resembling standard autoencoders. The key difference is that neural codecs produce discrete representations (audio tokens) instead of continuous ones.
The discretization is performed by a differentiable quantizer, such as residual vector quantization (RVQ)~\citep{zeghidour2021soundstream}, which enables end-to-end training by allowing gradients to propagate through the quantization step. Neural codecs are often trained using a combination of losses. For example, reconstruction losses in the time and frequency domains~\citep{kankanahalli2018end}, optionally combined with psychoacoustic calibration~\citep{zhen2020psychoacoustic}, direct guide signal reconstruction. Adversarial losses~\citep{zeghidour2021soundstream} and generative models~\citep{Kleijn2018wavenet, valin2019real, garbacea2019low} indirectly improve the perceptual quality of the reconstructed signal. Finally, auxiliary losses are often introduced to improve the learning process and often act as regularizers or encode inductive bias~\citep{zhang2023speechtokenizer,defossez2024moshi, har2025past}. 

Discrete tokens have several useful properties. As they are normally compact, audio tokens enable more efficient storage and transmission than continuous embeddings. They also simplify audio generation by converting tasks that involve modeling continuous distributions, such as regression, into discrete classification problems~\citep{wu2024toksing, mousavi2024dasb}.
More importantly, they help bridge the gap between text and audio processing, making them a natural choice for multimodal models and a core component of many recent multimodal LLMs~\citep{peng2024survey, cui2024recent, ji2024wavchat, latif2023sparks, liu2023audioldm, wu2024towards, tian2025espnet}.
Driven by these advantages, discrete audio tokens have already been adopted as an alternative to continuous features in a wide range of downstream tasks: automatic speech recognition~\citep{chang2023exploration, du2023funcodec}, speech-to-speech translation~\citep{popuri2022enhanced, inaguma2022unity, wu2023speechgen, chang2024speechprompt}, voice conversion~\citep{maimon2023speaking, wang2024streamvoice}, text-to-speech synthesis~\citep{ju2024naturalspeech, wang2023neural, hayashi2020discretalk}, speech enhancement~\citep{wang2023selm, yang24h_interspeech, xue24lowlatency}, and source separation~\citep{shi2021discretization, erdogan2023tokensplit, mousavi2024, bie2024learning, yip2024towards}. Discrete tokens are also used in music and general audio tasks, including music generation~\citep{copet2024musicgen, chen2024musicldm}, environmental sound synthesis~\citep{yang2023diffsound, kreuk2022audiogen}, and multimodal generation~\citep{borsos2023audiolm, liu2023audioldm, ziv2024masked,rubenstein2023audiopalm,wang2024viola}.

Recent studies have introduced a variety of tokenization methods, often grouped into two main categories: \textit{acoustic} and \textit{semantic}\footnote{It is important to clarify that the term ``semantic'' in the speech context does not align with its conventional linguistic meaning. In the speech context, these discrete tokens are more accurately described as phonetic units~\citep{sicherman2023analysing,choi2024selfsupervisedspeechrepresentationsphonetic} and typically do not carry semantic content~\citep{arora2025landscape}. In this paper, to maintain consistency across different domains (speech, audio, music) and with established terminology such as ``semantic distillation,'' we consistently use the term ``semantic.''}
~\citep{borsos2023audiolm,zhang2023speechtokenizer,har2025past,guo2025recent}. Acoustic tokens are typically learned through encoder-decoder architectures optimized for waveform reconstruction~\citep{zeghidour2021soundstream, defossez2022high, kumar2023high, yang2023hifi}. Semantic tokens are derived from pretrained self-supervised learning (SSL) models~\citep{lakhotia2021generative, mousavi2024}, \rebuttal{which are trained on raw audio without labels by solving proxy tasks (e.g., masked prediction) to learn transferable representations to downstream tasks}, or encoders trained in a supervised manner~\citep{du2024cosyvoice} designed to capture phonetic or linguistic content for discriminative tasks such as speech recognition and translation. Some recent approaches aim to combine both types, introducing hybrid tokenizers~\citep{zhang2023speechtokenizer, defossez2024moshi} that balance acoustic and phonetic properties.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.00\linewidth]{figures/benchmark.pdf}
\caption{Overview of our empirical study, covering three domains: speech, music, and general audio, with four evaluation components: Downstream Evaluation (Section \ref{sec:dasb}) using the DASB benchmark, Reconstructed Audio Evaluation (Section \ref{sec:reconstruct}) using Codec-SUPERB and Versa, Acoustic LLM Evaluation (Section \ref{sec:acousticLLM}) using SALMon and the Zero-Resource benchmark, Tokenizer Training Ablation Study (Section \ref{sec:ablation}) using ESPnet-Codec.}
  % \vspace{-.77cm}
  \label{fig:survey_pipeline}
\end{figure}

We argue the common division of discrete tokens into acoustic and semantic categories has notable limitations. Acoustic tokenizers can capture semantic information~\citep{defossez2024moshi,du2023funcodec,zhang2023speechtokenizer,bai2024dmel}, while semantic tokenizers have been effectively used in generative tasks~\citep{polyak2021speech,wang2023selm,nguyen2024spiritlminterleavedspokenwritten,lakhotia2021generative,maimon2025slamming,hassid2023textually,mousavi2024,wu2025esi}. This overlap blurs the boundary between the two categories and suggests that the acoustic-semantic distinction alone is insufficient. Moreover, as tokenization methods continue to evolve, traditional classifications fail to capture key architectural differences and practical trade-offs. To address this limitation, we introduce a refined taxonomy that captures key design choices, including encoder-decoder, quantization techniques, training paradigms, streamability, and application domains.


Another notable gap in the literature is that existing surveys and benchmark papers have primarily focused on speech applications~\citep{cui2024recent,kim2024neural,ji2024wavchat,anees2024speech,guo2025recent,arora2025landscape,vashishth2024stab}, often overlooking tokenization methods for music and general audio. As a result, the current literature lacks a unified study that covers multiple domains and diverse evaluation criteria. Moreover, rather than providing a holistic comparison, most existing works focus on a single aspect, such as reconstruction quality in Codec-SUPERB~\citep{wu2024codec, wu2024codecslt}, downstream task performance in DASB~\citep{mousavi2024dasb}, controlled evaluation settings in ESPnet-Codec~\citep{shi2024espnet}, or audio language modeling in SALMon~\citep{maimon2024suite}.
These limitations persist even in the latest surveys. For example,~\citet{guo2025recent} focuses on reconstruction and voice conversion, while~\citet{cui2024recent,peng2024survey} explores integration with LLMs. To help bridge this gap, we present a comprehensive benchmark of discrete audio tokenizers.
Our benchmark covers three audio domains: speech, music, and general audio. It considers multiple evaluation criteria, including signal reconstruction, downstream task performance, and acoustic language modeling. These aspects are analyzed jointly to provide a more robust and comprehensive assessment.
An additional issue in current benchmarks is that tokenizers are often trained under inconsistent conditions, such as different datasets, domains, or sampling rates. These inconsistencies make direct and fair comparisons difficult. To ensure fair comparisons, we support our analysis with ablation studies that examine different quantization methods under controlled experimental settings.

Our contribution is organized into three core studies, as illustrated in Figures~\ref{fig:survey_pipeline} and~\ref{fig:taxanomy}:
\begin{itemize}[leftmargin=0.5cm, itemsep=10pt, parsep=0pt, topsep=5pt]
\item \textbf{Study 1: Audio Tokenizer Taxonomy (Section \ref{sec:literature}).} We propose a comprehensive taxonomy of discrete audio tokenization methods based on key architectural and functional criteria.
\item \textbf{Study 2: Benchmark Evaluation (Section \ref{sec:benchmark}).} We evaluate existing tokenizers using multiple benchmarks. Codec-SUPERB\footnote{\url{https://codecsuperb.github.io/}} and VERSA\footnote{\url{https://github.com/wavlab-speech/versa/tree/main/egs/survey}.}~\citep{shi2024versa}  are used for reconstruction. DASB\footnote{\url{https://poonehmousavi.github.io/DASB-website/}} is used for downstream tasks. SALMon\footnote{\url{https://pages.cs.huji.ac.il/adiyoss-lab/salmon/}} and the Zero-resource speech benchmark\footnote{\url{https://github.com/zerospeech/zerospeech2021_baseline}}~\citep{nguyen2020zeroresourcespeechbenchmark}  are used for acoustic language modeling. All evaluations are conducted under consistent conditions.

\item \textbf{Study 3: Ablation Studies (Section \ref{sec:ablation}).} We perform controlled experiments to isolate the effects of specific design choices for training audio tokenizers, including sampling rate and single-domain versus multi-domain training using ESPnet-Code\footnote{\url{https://github.com/espnet/espnet}}~\citep{shi2024espnet}.
\end{itemize}

This survey provides a unified and practical perspective on discrete audio tokenization and its role in speech, music, and general audio processing. We aim to clarify key design trade-offs, highlight current limitations, and offer guidance for future research in this evolving field.

