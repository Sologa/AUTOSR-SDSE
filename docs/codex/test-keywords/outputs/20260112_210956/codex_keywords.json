{
  "run_id": "20260112_210956",
  "mode": "single",
  "provider": "codex",
  "models": {
    "Keywords": "gpt-5.1-codex-mini"
  },
  "topic": "discrete audio tokens more than a survey",
  "prompt_path": "/Users/xjp/Desktop/Survey-with-LLMs/Survey-for-survey-review-with-LLMs/AUTOSR-SDSE/resources/LLM/prompts/keyword_extractor/generate_search_terms.md",
  "input_pdf_dir": "/Users/xjp/Desktop/Survey-with-LLMs/Survey-for-survey-review-with-LLMs/AUTOSR-SDSE/workspaces/discrete_audio_tokens_more_than_a_survey/seed/downloads/arxiv",
  "input_pdfs": [
    "/Users/xjp/Desktop/Survey-with-LLMs/Survey-for-survey-review-with-LLMs/AUTOSR-SDSE/workspaces/discrete_audio_tokens_more_than_a_survey/seed/downloads/arxiv/2502.06490.pdf"
  ],
  "pdf_text_inputs": [
    "/Users/xjp/Desktop/Survey-with-LLMs/Survey-for-survey-review-with-LLMs/AUTOSR-SDSE/docs/codex/test-keywords/outputs/20260112_210956/2502.06490.txt"
  ],
  "pdf_text_generated": true,
  "payload": {
    "topic": "discrete audio tokens more than a survey",
    "anchor_terms": [],
    "search_terms": {},
    "papers": []
  },
  "raw_output": "",
  "errors": [
    "2026-01-12T13:10:04.103812Z ERROR codex_core::codex: failed to load skill /Users/xjp/.codex/skills/gget/SKILL.md: invalid YAML: mapping values are not allowed in this context at line 2 column 92\nOpenAI Codex v0.79.0 (research preview)\n--------\nworkdir: /Users/xjp/Desktop/Survey-with-LLMs/Survey-for-survey-review-with-LLMs/AUTOSR-SDSE\nmodel: gpt-5.1-codex-mini\nprovider: openai\napproval: never\nsandbox: workspace-write [workdir, /tmp, $TMPDIR]\nreasoning effort: medium\nreasoning summaries: auto\nsession id: 019bb253-f04a-7800-8736-ca7104ddd356\n--------\nuser\n- Role: Academic Search Strategy Designer and Systematic Review Analyst\n- Background: The user uploads one or more survey papers (PDFs). Your goal is to extract high-quality search terms and prepare a metadata-aligned JSON summary for downstream systematic-review tooling.\n- Profile: You design evidence-grounded, reproducible search strategies for literature reviews. You prioritize deduplication, clarity, and coverage.\n- Skills: Systematic review methodology, taxonomy-driven term extraction, boolean query synthesis, deduplication and synonym consolidation, concise rationale writing.\n- Goals: Produce a JSON-only output containing anchor terms, categorized search terms, and per-paper metadata entries (including detected keywords with evidence). Ground every field in the PDFs and the provided metadata.\n- Constraints:\n  - Use only information present in the uploaded PDFs or in the metadata block appended below.\n  - Copy each paper title and abstract exactly as provided; do not paraphrase or truncate them.\n  - Prefer multi-paper-supported terms; mark single-paper terms with lower confidence.\n  - Keep each rationale under 20 words; cite page numbers if available; otherwise use \"page\": \"n/a\".\n  - Keep total recommended search terms <= 50 (default 50).\n  - Keep each search term as a concise noun phrase (ideally 1‚Äì2 words, maximum 3); never output full sentences or tokens with underscores.\n  - Output strictly valid JSON, no extra text.\n- Downstream usage (important):\n  - The output anchor_terms and search_terms will be used to construct boolean search queries for arXiv-style engines.\n  - anchor_terms are treated as stable topic anchors; search_terms are category-specific query terms.\n  - Queries are built by combining anchor_terms with search_terms (e.g., (anchor OR anchor OR ...) AND (term OR term OR ...)).\n  - Matching is mostly literal, so anchor_terms and search_terms must be searchable, generalizable phrases (not full titles, not overly specific wording).\n  - Avoid punctuation-heavy strings, quotes, dataset IDs, or long phrases; prefer 1‚Äì3 word noun phrases likely to appear in titles/abstracts.\n  - Do not add meta terms like \"survey/review\" unless the topic itself explicitly centers on surveys/reviews.\n- Topic interpretation (important):\n  - The provided topic_hint may be either (1) a broad research area or (2) a specific paper title (possibly exact).\n  - Silently decide which type topic_hint is; do not output the decision.\n  - Never output topic_hint itself as anchor_terms.\n  - If topic_hint is type (2), you must NOT copy the title, subtitle, punctuation, or create abbreviations from it. Derive anchors only from the PDFs/metadata.\n  - If topic_hint is type (1), you may use common field abbreviations only when they appear in the PDFs/metadata and are widely used in the field.\n  - Anchor_terms must appear verbatim (case-insensitive) in the titles/abstracts; use 1‚Äì3 word noun phrases without punctuation.\n  - Abbreviations are allowed only if they appear in the PDFs/metadata and are widely used in the field; never invent acronyms.\n  - When an abbreviation is used, include its long-form anchor term alongside it (do not output acronym-only anchors).\n  - Do not inject external domain assumptions; derive anchor_terms from the PDFs/metadata only.\n- Workflow:\n  1) Review the provided paper metadata (see block below) to capture canonical identifiers, titles, abstracts, publication years, and URLs.\n  2) Read the PDFs and identify the central task/topic; propose 2‚Äì4 anchor_terms aligned with the metadata guidance.\n  3) For each paper, extract candidate terms grouped by categories: Ëá™Ë°åÊ≠∏Á¥ç 4‚Äì6 ÂÄãÂÖ∑ÊèèËø∞ÊÄßÁöÑ snake_case ÂàÜÈ°ûÔºà‰æãÂ¶Ç benchmarks, training_methods, datasets, evaluation_metricsÔºâ.\n     Êé®Â∞éÂ§öÂÄã‰∏ªÈ°åÂàÜÈ°ûÔºå‰∏¶ËÆìÊØèÂÄãÂàÜÈ°ûËá≥Â∞ëÂåÖÂê´ 8 ÂÄãË°ìË™ûÔºà‰∏çË∂≥ 3 ÂÄãË°ìË™ûÊôÇ‰∏çË¶ÅÁç®Á´ãÊàêÂàÜÈ°ûÔºâÔºåÁõÆÊ®ôÁ∏ΩÊï∏Â§ßÁ¥Ñ 48 ÂÄãÊàñ‰æùÊìöË≠âÊìöË™øÊï¥\n  - Â¶ÇÊûúÊó¢ÊúâÂàÜÈ°û‰∏çË∂≥‰ª•ÊèèËø∞ÈáçË¶ÅË≠∞È°åÔºåÂèØ‰ª•Êñ∞Â¢ûÊúÄÂ§öÂÖ©ÂÄãÊñ∞ÁöÑ snake_case ÂàÜÈ°ûÔºå‰∏¶Êèê‰æõ‰ª£Ë°®ÊÄßË°ìË™û„ÄÇ\n\n  4) Normalize and merge across papers: lemmatize, deduplicate, and consolidate related phrasing for each category.\n  5) Identify supporting evidence for detected keywords (quotes + page numbers when available) so each term can be traced back to the PDFs.\n  6) Keep each `papers[*]` entry aligned with the metadata block; copy titles/abstracts verbatim and supply detected keywords with evidence.\n- Runtime overrides (current request):\n  - topic_hint: discrete audio tokens more than a survey\n  - language: en\n  - include_ethics: false\n  - max_queries: 50\n  - seed_anchors: not provided\n  - custom_categories: not provided (use defaults)\n  - exclude_terms: not provided\n  - anchor_terms: infer 2‚Äì4 anchors grounded in the PDFs\n- Provided paper metadata (copy titles/abstracts exactly; keep ordering):\n--- Paper 1 ---\nsource_id: arXiv:2502.06490\ntitle: Recent Advances in Discrete Speech Tokens: A Review\nabstract: The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.\nyear: 2025\nurl: https://arxiv.org/abs/2502.06490v4\npdf_path: /Users/xjp/Desktop/Survey-with-LLMs/Survey-for-survey-review-with-LLMs/AUTOSR-SDSE/workspaces/discrete_audio_tokens_more_than_a_survey/seed/downloads/arxiv/2502.06490.pdf\n- OutputFormat (strict JSON):\n{\n  \"topic\": \"discrete audio tokens more than a survey\",\n  \"anchor_terms\": [\"‚Ä¶\", \"‚Ä¶\"],\n  \"search_terms\": {\n    \"<category>\": [\"‚Ä¶\"],\n    \"<category>\": [\"‚Ä¶\"]\n  },\n  \"papers\": [\n    {\n      \"id\": \"<stable short id>\",\n      \"source_id\": \"arXiv:<identifier>\",\n      \"title\": \"<copy title exactly>\",\n      \"abstract\": \"<copy abstract exactly>\",\n      \"year\": \"<YYYY or unknown>\",\n      \"source_url\": \"<https://arxiv.org/abs/...>\",\n      \"detected_keywords\": [\n        {\n          \"term\": \"‚Ä¶\",\n          \"category\": \"<category label>\",\n          \"evidence\": {\"quote\": \"‚Ä¶\", \"page\": \"n/a|<number>\"},\n          \"confidence\": 0.0\n        }\n      ]\n    }\n  ]\n}\n- Example (illustrative only; copy structure, not content):\n```json\n{\n  \"topic\": \"Challenges of Abstractive Dialogue Summarization\",\n  \"anchor_terms\": [\n    \"dialogue summarization\",\n    \"dialog summarization\",\n    \"conversation summarization\"\n  ],\n  \"search_terms\": {\n    \"core_concepts\": [\"technique\", \"dataset\", \"evaluation\"],\n    \"technical_terms\": [\"language model\", \"semantic representation\", \"information extraction\"],\n    \"advanced_concepts\": [\"topic segmentation\", \"personalization\"],\n    \"implementation\": [\"automatic\", \"training\"],\n    \"subdomains\": [\"meeting summarization\", \"customer service summarization\"],\n    \"ethical\": [\"privacy\", \"cost\"]\n  },\n  \"papers\": [\n    {\n      \"id\": \"cads_taxonomy_2025\",\n      \"source_id\": \"arXiv:2501.01234\",\n      \"title\": \"CADS: A Systematic Review of Abstractive Dialogue Summarization\",\n      \"abstract\": \"We categorize 133 dialogue summarization papers published between 2019‚Äì2024 across six challenge areas (language, structure, comprehension, speaker, salience, factuality) and map them to techniques, datasets, and metrics.\",\n      \"year\": \"2025\",\n      \"source_url\": \"https://arxiv.org/abs/2501.01234\",\n      \"detected_keywords\": [\n        {\n          \"term\": \"language challenge\",\n          \"category\": \"core_concepts\",\n          \"evidence\": {\"quote\": \"We outline the language challenge covering informal dialogue and colloquialisms.\", \"page\": \"n/a\"},\n          \"confidence\": 0.6\n        },\n        {\n          \"term\": \"meeting summarization\",\n          \"category\": \"subdomains\",\n          \"evidence\": {\"quote\": \"The taxonomy highlights datasets for meeting summarization such as AMI and ICSI.\", \"page\": \"n/a\"},\n          \"confidence\": 0.5\n        }\n      ]\n    }\n  ]\n}\n```\n- Notes:\n  - Keep \"papers\" in the same order as the metadata block.\n  - Do not emit additional top-level keys beyond the schema above.\n\n\nExtracted PDF text (pre-processed):\n\n--- 2502.06490.txt ---\n1\nRecent Advances in Discrete Speech Tokens: A\nReview\nYiwei Guo, Student Member, IEEE, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang,\nChenpeng Du, Xie Chen, Shujie Liu, Kai Yu, Fellow, IEEE\nAbstract‚ÄîThe rapid advancement of speech generation tech-\nnologies in the era of large language models (LLMs) has\nDiscrete\nestablished discrete speech tokens as a foundational paradigm Speech Tokenizer Codec decoder\nfor speech representation. These tokens, characterized by their Tokenization (Acoustic/Semantic) or (optional) vocoder\ndiscrete,compact,andconcisenature,arenotonlyadvantageous\nforefficienttransmissionandstorage,butalsoinherentlycompat- Discrete speech tokens\nible with the language modeling framework, enabling seamless\nintegration of speech into text-dominated LLM architectures. The quick fox ‚Ä¶\nCurrent research categorizes discrete speech tokens into two Speech\nToken-based Spoken Speech Spoken ‚Ä¶‚Ä¶\nprincipal classes: acoustic tokens and semantic tokens, each of Downstream Language Synthesis Language (Dialogue)\nwhich has evolved into a rich research domain characterized Applications Understanding Modeling\nby unique design philosophies and methodological approaches. The quick fox ‚Ä¶\nThissurveysystematicallysynthesizestheexistingtaxonomyand\nFig. 1: Diagram of discrete speech tokenization process and\nrecent innovations in discrete speech tokenization, conducts a\ncritical examination of the strengths and limitations of each speech token-based downstream applications.\nparadigm, and presents systematic experimental comparisons\nacrosstokentypes.Furthermore,weidentifypersistentchallenges To transform long speech waveforms into compact sequences\nin the field and propose potential research directions, aiming to of discrete tokens compatible with textual representations,\noffer actionable insights to inspire future advancements in the particularly for language modeling tasks involving speech.\ndevelopment and application of discrete speech tokens.\nAs a result, significant efforts have been directed towards\nIndex Terms‚ÄîDiscrete speech tokens, neural audio codec,\nspeech tokenizer, speech LLMs, spoken language modeling, developing efficient and powerful speech tokenization meth-\nspeech generation, acoustic tokens, semantic tokens ods.Generally,thesemethodsarebasedontwodistinctprinci-\nples,givingrisetotwotypesofspeechtokens:acoustictokens\nand semantic tokens. Acoustic tokens are derived from neural\nI. INTRODUCTION codecs designed to encode speech at a low bitrate while pre-\nserving as much information as possible [8]‚Äì[11]. In contrast,\nTHErapidadvancementoflargelanguagemodels(LLMs)\nsemantictokensoriginatefromspeechself-supervisedlearning\nin natural language processing has revolutionized speech (SSL) [12], which aims to learn a more phonetic or semantic\ngeneration tasks [1], [2], with speech being tokenized and representation space, making it easier for speech recognition.\nmodeled using decoder-only Transformers [3]. Efforts starting These two nearly independent lines of research magically\nfrom GSLM [4] and AudioLM [5] aim to develop text-free intersectinthecontextoflanguagemodelingforspeech.Now,\nspoken LLMs, akin to how current LLM-powered chatbots there are also efforts that try to design a speech tokenizer\nenable text-based interactions. Other works, including VALL- thataccomplishesthetwoobjectivessimultaneously[13],[14].\nE [6] and VioLA [7], extend this approach to conditional Consequently,speechtokenizationhasbecomeacoreproblem\nspeech generation tasks, such as zero-shot text-to-speech and of speech processing under the new paradigm, with versatile\nspeech translation. However, this paradigm requires data to downstream applications, as shown in Fig.1.\nbe tokenized, as LLMs typically process discrete data only. Prior to discrete tokens, continuous speech representations\nTextual tokens naturally meet this requirement because they from autoencoders and self-supervised learning have been\nare designed as discrete units separated by clear boundaries, extensively explored [12]. Comparatively, continuous repre-\nwhereasrawspeechsignalsarecontinuousandboundary-less. sentations generally provide higher fidelity in reconstruction\nTherefore, a necessary step before applying speech data to or stronger performance on understanding tasks, but lack the\nLLM is the tokenization of speech, whose goal is: compactness, symbolic abstraction, and compatibility with\nlanguage model-based generation that discrete tokens easily\nCorrespondingAuthor:KaiYu.Email:kai.yu@sjtu.edu.cn afford. In other words, they highlight different tradeoffs in\nYiweiGuo,ZhihanLi,BohanLi,ChongtianShao,HangleiZhang,Hankun\nrepresentation learning.\nWang, Chenpeng Du, Xie Chen and Kai Yu are with the MoE Key Lab of\nArtificialIntelligence,JiangsuKeyLabofLanguageComputing;X-LANCE Despite the rapid development and numerous recent works,\nLab,DepartmentofComputerScienceandEngineering,ShanghaiJiaoTong a comprehensive taxonomy of methodologies in discrete\nUniversity,Shanghai,China.Email:yiwei.guo@sjtu.edu.cn\nspeech tokens has not been clearly constructed. Existing\nShujie Liu is with Microsoft Research Asia (MSRA), Beijing 100080,\nChina. reviews [1], [2], [8]‚Äì[10], [12] in this field overlook the\n5202\nceD\n21\n]SA.ssee[\n4v09460.2052:viXra\n\n2\ndiverse categories and methodologies in both acoustic and assign each sample x to a group such that some cost is\ni\nsemantic tokens. For example, [1], [2] focus primarily on minimized. The most frequently used clustering method for\nmethods in spoken language modeling, providing only brief discretespeechtokensisk-meansclustering[40].Forexample,\ndescriptions of some speech tokens used in existing models. k-meansisappliedonHuBERT[34]featuresinGSLM[4].K-\nThetaxonomyofneuralaudiocodecshasbeensummarizedin means is a clustering algorithm based on Euclidean distances.\n[8], [11], but the realm of semantic tokens is still overlooked. Its training process iteratively assigns each data sample to the\nIn this review, we provide a comprehensive overview of the nearest centroid, and moves cluster centroids till convergence,\nconcepts, methods, and characteristics of various types of with a pre-defined number of clusters. After training, the\ndiscrete speech tokens, with their applications in spoken lan- centroids form the codebook, and new data can be quantized\nguage understanding, speech generation, and spoken dialogue totheindexofthenearestcentroidinthisVoronoipartition.In\nmodels. We hope that through this review, the community can practice, centroids are usually initialized with the k-means++\nhave a clear understanding of the current development and algorithm [41] for better convergence.\nkey technologies of discrete speech tokens, so as to promote Hierarchical agglomerative clustering has also been used in\nfurther research in the future. discrete speech tokens, which iteratively merges the closest\nOur contributions are summarized as follows: clusters. It is usually applied after k-means to reduce the\n‚Ä¢ This review is the first to focus specifically on discrete number of clusters [42], [43]. Other clustering algorithms are\nspeech tokens with sufficient depth in the LLM era. less explored in the context of discrete speech tokens.\n‚Ä¢ We construct a comprehensive taxonomy of current re-\nsearchondiscretespeechtokensandmeticulouslyreview\nB. Vector Quantization\nthemotivation,representativeapproaches,andchallenges\nin each sub-category. Clustering is often an isolate process, thus cannot be opti-\n‚Ä¢ We provide a unified comparison of different types of mized together with other neural network modules. Instead,\ndiscrete speech tokens in terms of reconstruction and vector quantization (VQ) [44] enables a learnable network\nvoiceconversionperformance,coveringbothacousticand module that allows gradients to pass through when producing\nsemantic tokens. discrete representations. Autoencoders with a VQ module is\n‚Ä¢ Wesummarizethecurrentchallengesandpotentialfuture termed VQ-VAE [45]. There are multiple VQ methods:\ndirectionsfordiscretespeechtokens,includingdecoupled 1) K-means VQ: Like k-means clustering, k-means VQ\nand variable frame rate tokens. method finds the code-vector closest to the input, i.e.\nThestructureofthisreviewisshowninFig.2.Following[5],\n[15],[16],weclassifydiscretespeechtokensintoacousticand q(x)= argmin ‚à•x‚àíc i ‚à•2. (1)\ni‚àà{1,2,...,V}\nsemantictokensbasedontheirprinciples.Wewillcharacterize\nthe two types of tokens both by conceptual descriptions and\nThen, code-vector c ‚âú c is fed to subsequent networks.\nunified experimental comparisons. k q(x)\nAs the min operation is not differentiable, straight-through\nestimators (STEs) [46] are usually applied to graft gradients,\nII. PRE-REQUISITES:DISCRETEREPRESENTATION\ni.e. STE(c ,x)=x+sg(c ‚àíx) where sg(¬∑) stops tracking\nLEARNING k k\ngradients. In this way, the input value to subsequent networks\nDiscrete speech tokens are obtained through the quantiza-\nis still c , but gradients are grafted to x in back propagation.\nk\ntion of continuous representations, which is usually achieved\nAuxiliary loss functions are often used together with k-\nby offline clustering or online vector quantization algorithms.\nmeans VQ [45]: commitment loss L =‚à•sg(c )‚àíx‚à•2 and\nThissectionprovidesaconciseoverviewoftheexistingquan- cmt k\ncodebook loss L =‚à•sg(x)‚àíc ‚à•2. The commitment loss\ntization methods commonly used in discrete speech tokens. code k\npushes the continuous input x towards the closest codebook\nDenotex‚ààRd asavectorinthed-dimensionalcontinuous\nentry, while the codebook loss does the opposite and updates\nspace. A quantization process q transforms x into a discrete\nthe code-vector c . The two loss terms are weighted by\ntoken in a finite set, i.e. q(x):Rd ‚Üí{1,2,...,V} where V is k\ndifferent factors to put different optimization strengths on x\nthevocabularysize.Theoutputtokensaresometimesreferred\nand c , as pushing c towards x is an easier task. It is also\nto as indexes in the finite V-cardinal set. The function q is k k\ncommon to replace L with exponential moving average\nusuallyassociatedwithacodebookC ={c ,c ,...,c }where code\n1 2 V (EMA) to update the codebook instead [47], which does not\nevery code-vector c ‚àà Rd corresponds to the i-th token.\ni rely on explicit loss functions.\nThe code-vectors are representations of tokens in the original\nVQ in high-dimensional spaces is known to suffer from\nd-dimensional space. As V elements can be encoded using\ncodebook collapse, where the codebook usage is highly im-\n‚åàlog V‚åâ rawbits1,quantization oftencompresses thecost for\n2 balanced [48], [49]. To improve codebook utilization, random\ndata storage and transmission to a great extent.\nreplacement (as known as codebook expiration) can be ap-\nA. Offline Clustering\nplied [49] on code-vectors that have remained inactive for a\nClustering is a simple approach for quantization. Given a\nlongtime.Otherpopularsolutionsincludeadditionalauxiliary\ndataset X = {x ,x ,...x }, a clustering algorithm aims to\n1 2 N constraintssuchasentropypenalty[50],[51],factorizedcode-\nbook lookup in low-dimensional space [52], and reparameter-\n1We denote by ‚åàz‚åâ and ‚åäz‚åã the ceiling and floor of scalar z, i.e., ‚åàz‚åâ=\nmin{n‚ààZ:n‚â•z}and‚åäz‚åã=max{n‚ààZ:n‚â§z}. izing code-vectors through a learnable linear projection [53].\n\n3\nPre-requisites:DiscreteRepresentationLearning OfflineClustering:k-meansclustering,agglomerativeclustering,etc.\n(SectionII) VectorQuantization:k-meansVQ,GumbelVQ,FSQ,GVQ,RVQ,etc.\nModelarchitectures VQ-GAN:CNN-based,Transformer-based,U-Net-based,etc.\n(SectionIII-A) Diffusion:LaDiffCodec[17],SemantiCodec[18],etc.\nAdvancedVQmethodsandmodelarchitectures:DAC[19],TS3-Codec[20],etc.\nGeneral-Purpose\nTemporalredundancyreduction:Disen-TF-Codec[21],TiCodec[22],etc.\n(SectionIII-B)\nMulti-resolutionorvariable-bitrate:SNAC[23],VRVQ[24],etc.\nAcousticTokens\n(SectionIII,TableII) Semanticfeatureguidance:SpeechTokenizer[13],Mimi[14],etc.\nSemanticDistillation\nFixedsemanticcodebook:LLM-Codec[25],etc.\n(SectionIII-C)\nSemanticfeaturesasinputsoroutputs:X-Codec[26],SemantiCodec[18],etc.\nDiscrete Gradientreversallayer:SSVC[27],FACodec[28],etc.\nDisentanglement\nSpeech Perturbation:LSCodec[29],etc.\n(SectionIII-D)\nTokens Sourceseparation:SD-Codec[30],DeCodec[31],etc.\nContrastivemodels:vq-wav2vec[32],wav2vec2.0[33],etc.\nSSLSemanticTokens\nPredictivemodels:HuBERT[34],WavLM[35],etc.\n(SectionIV-A)\nPerturbation-invariantSSLmodels:ContentVec[36],NAST[37],etc.\nSemanticTokens\nSupervisedSemanticTokens(SectionIV-B):Whisper[38],S3Tokenizer[39],etc.\n(SectionIV,TableIII)\nSpeechTokenVocoders(SectionIV-C)\nLengthReductionbyDeduplicationandAcousticBPE(SectionV-A)\nLengthReductionandVariable-RateTokenization\n(SectionV)\nVariableFrameRateTokensandUnitDiscovery(SectionV-B)\nAnalysis(SectionVI):Metrics,benchmarks,experimentalcomparisons(reconstruction,voiceconversion,downstreamsemanticmodeling)\nApplications(SectionVII),ChallengesandFutureDirections(SectionVIII)\nFig. 2: Structure of this review. After a brief introduction to the preliminary knowledge, we will taxonomize acoustic tokens\nand semantic tokens, followed by cross-cutting methods such as length reduction and variable-rate tokenization. Later sections\ncover experimental analysis, applications, and future directions. Each branch corresponds to a subsection in the paper.\n2) Gumbel VQ: Instead of quantizing by Euclidean dis- 3) FiniteScalarQuantization(FSQ): Asmentionedbefore,\ntance, another choice is by probability. Gumbel VQ [54] VQ methods based on code-vector assignment usually suffer\nuses Gumbel-Softmax as a proxy distribution for traditional from codebook collapse. Despite many efforts, this remains\nSoftmax to allow differentiable sampling. Given input x and a crucial challenge. FSQ [56] is an alternative that performs\na codebook of size V, a transform h(¬∑) is applied on x into quantization in scalar domain. FSQ quantizes each dimension\nV logits: l = h(x) ‚àà RV. In inference, quantization is of a vector x into L levels. For the i-th dimension x(i), FSQ\nperformed by choosing the index with the largest logit, i.e. transforms the values into a limited range and then rounds to\nq(x)=argmax\n(cid:8) l(i)(cid:9)\n. In training, samples are drawn from integers, i.e.\ni\nthe categorical distribution implied by l for the subsequent (cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)\nneural networks. To achieve efficient sampling and let gradi- q x(i) =round ‚åäL/2‚åãtanh x(i) . (4)\nents pass through, Gumbel trick is used:\nThe quantized values for each dimension are thus integers\nu‚ààRV ‚àºUniform(0,1),v =‚àílog(‚àílog(u)) (2) ranging from ‚àí‚åäL/2‚åã to ‚åäL/2‚åã2. For a d-dimensional vector\ns=Softmax((l+v)/œÑ) (3) x, there are V = Ld possible quantization outcomes. STE\nis also applied to pass gradients. As quantization is simply\nwhere Eq.(2) samples Gumbel noise v element-wise, and done via rounding to integers, there are no explicit codebooks\nEq.(3) calculates Gumbel-Softmax distribution s with a tem- associated with the FSQ process.\nperature œÑ. The forward pass simply uses j =argmax i {s(i)} 4) Other VQ Tricks: In many cases, a single VQ module\nasthesampledindex,butthetruegradientofGumbel-Softmax suffersfromahighly-limitedrepresentationspace,thusresults\nis used in backward pass. In other words, the gradient on the in poor performance compared to continuous counterparts.\none-hot distribution corresponding to j is grafted to s as an Therearesomewidely-usedVQtricksthatintroducemultiple\napproximate. The temperature œÑ balances the approximation quantizers to refine the quantized space, as shown in Fig.3:\naccuracyandgradientvariances.Thetransformh(¬∑)isusually\n1) Grouped VQ (GVQ), also known as product quantiza-\nparameterized as neural networks, or negatively proportional\ntion[57].Itpartitionstheinputvectorxbydimensionsand\nto Euclidean distances [55].\napplies VQ on different groups independently. Different\nAfter quantization, code-vector c with k = q(x) is fed\nk groupscanhavedifferentorsharedcodebooks.Eachgroup\nto subsequent networks. Gumbel VQ does not require addi-\ntional losses, since code-vectors can be directly learned with\n2Following [56], this is the symmetric case for L being odd. When L is\ngradients and do not need to be pushed towards x. even,thereisanoffsetbeforeroundingtoobtainasymmetricquantizedvalues.\n\n4\n...... ...... Discriminator\nùíô !\"# ‚àà‚Ñù$!\"# ùíÑ !!\"# ùíô! ùíÑ !!\"#\nùíô‚àà‚Ñù$ ùíô ! ‚àà‚Ñù$! ùëò!=ùëû!(ùíô!) ùíÑ !! ùíÑ‚àà‚Ñù$ ùíô‚àà‚Ñù$ ùíô! ùíô \" ùëò ! # ! \" = # = ùíô ùëû ! !( ‚àí ùíô! ùíÑ ) $! ùíÑ !! + ùíÑ‚àà‚Ñù$ VQ-GAN Encoder Glo Q b u a a l ntizer spe D e i c s h c r t e o t k e e n D s ecoder\n...... ...... Encoder\nùíô !%# ‚àà‚Ñù$!$# ùíÑ !!$# ùíÑ !!$#\nEEnnccooddeerr Quantizer\nFig. 3: Diagram of GVQ (left) and RVQ (right). GVQ quan- Discrete\nspeech tokens\ntizes different partitions of the input vector independently, Diffusion\nwhile RVQ sequentially quantizes the residuals. Diffusion /\nFlow Matching\nGaussian Fixed\nproduces a code-vector of the same dimensionality as distribution continuous space\nits partition. These code-vectors are concatenated across Fig.4:Neuralarchitecturesofacoustictokens.Notethatinputs\ngroups to form a final output whose dimensionality equals and outputs can be waveforms, frequency-domain features or\nthat of x. even SSL features depending on purpose and design.\n2) Residual VQ (RVQ), also known as multi-stage quanti-\nzation [58]. It adopts a serial approach that iteratively preventsefficientparallelizationincomputation,butmightalso\nquantizestheresidualofthelastquantizer.SimilartoGVQ, complicate optimization due to the nested STE operations.\nRVQ also has multiple quantizers. For the i-th quantizer\nq with input x and output code-vector c , the residual III. SPEECHTOKENIZATIONMETHODS:ACOUSTIC\ni i k\nis defined as x = x ‚àí c . The outputs from all q TOKENS\ni+1 i k i\nare finally summed as the quantized result of x. In this Acoustic speech tokens are discrete representations derived\nway, information in the codebooks is supposed to follow fromcodecmodels,primarilydesignedforspeechcompression\na coarse-to-fine order, and more details in the original x and reconstruction. The audio codec technology emerged\ncan be preserved than a plain quantizer. long ago. Traditional codecs, including MP3 [63], Opus [64]\nGVQ and RVQ can also be flexibly combined to form and EVS [65], typically take advantage of signal processing\nGRVQ [59] that applies RVQ on each GVQ branch for better algorithms to improve quality and lower the bitrate.\ncodebook utilization. RVQ can also be applied to FSQ [60]. In the deep learning era, numerous codec models based on\nNote that RVQ naturally produces an order of importance neural networks have been developed. These models typically\nin residual layers, while all quantizers in GVQ are equally consist of an encoder that compresses speech signals and a\nimportant. Such order of importance can also be enforced in decoder that reconstructs the speech signals, with a quantizer\nGVQ by a ‚Äúnested dropout‚Äù trick [61]. situated between the two. The quantizer is also parameterized\n5) Comparisons: Compared to k-means VQ, both Gumbel and jointly trained with the whole network in an end-to-end\nVQandFSQavoidadditionallosstermsduringtraining.How- manner. The codebook indices produced by the quantizer are\never, Gumbel VQ is sensitive to the temperature parameter referred to as acoustic tokens. To improve the representation\nœÑ in practice [62]. FSQ, by contrast, has a simpler design abilityofdiscreteVQspacesandthusobtainbettercodecper-\nand optimization procedure, and has been reported to achieve formance, RVQ, GVQ, GRVQ and FSQ tricks are commonly\nbetter codebook utilization3 under large vocabulary sizes than applied in the quantization module.\nk-means VQ [56]. WelisttheVQmethod,numberofquantizersQ,framerate\nNevertheless, FSQ also has certain limitations. In FSQ, as F, vocabulary size V for each quantizer, and the resulting\nthe vocabulary size V follows V =Ld, d is usually chosen to bitrate of existing neural acoustic speech tokens in Table II in\nbe small (like d=6 in StableCodec [60]). For VQ, V is not the appendix.\nrelated to the code-vector dimension d, which can therefore\nbe set to a wider range of values. The bottleneck of such low A. Model Architectures\ndimensionalitymightalsocauseFSQtounderperforminsmall\nAlthough acoustic codec models differ from one to one\nvocabulary sizes compared to a fully-utilized VQ [56]. Also,\nregardingtheirpurposes,mostofthemshareasimilarencoder-\nthe quantization space of FSQ is strictly fixed, whereas VQ\nquantizer-decoderframework.Withaudioclipxthatcaneither\nmethods maintain a learnable codebook. This rigidity forces\nbe time-domain sampling points, frequency-domain features\nthe surrounding network modules, particularly the encoder, to\nor even other machine learning features, an encoder f (¬∑)\nŒ∏\nmapthedatadistributionintosuchlow-dimensionalstructured\ntransformsittof (x)inacontinuouslatentvectorspace.The\nŒ∏\ncodes, thereby placing greater demands on model capacity.\nencoderf (¬∑)willusuallyperformdownsamplingtoreducethe\nŒ∏\nRegarding GVQ and RVQ, the inherent ordering in RVQ\ntemporal length of the input signals, especially for waveform\nprovides greater flexibility than GVQ in trading off bitrate\ninputs. A VQ module q (¬∑) discretizes f (x) into tokens and\nœï Œ∏\nand performance. However, RVQ‚Äôs sequential nature not only\ncorresponding codebook vectors c. A decoder g (¬∑) then uses\nœà\nc to reconstruct xÀÜ, and a certain distance metric of d(x,xÀÜ)\n3AlthoughFSQdoesnotmaintainanexplicitcodebook,utilizationcanstill\nbemeasuredovertheV =Ld possibleoutcomes. is usually optimized. There are two major paradigms for\n\n5\nQ. Block\nPatchify\nConv.\nConv.\nor\nBlock Conv. Block Conv. Block Conv.\nQ.\nTrans.\nor\nCNN\nTrans.\nBlock\nUnpatchify\nConv.\nor\nQ.\nQ. Q.\nTrans.\nor\nCNN\nTrans.\nTrans.\nor\nor\nCNN\nCNN\nTrans.\nTrans.\nor\nor\nCNN\nCNN\nCNN-based\nTransformer-\nbased\nU-Net-based\nTrans.\nBlock\nTrans.\nBlock\nC B o lo n c v k .\nTrans.\nBlock\nBlock\nTrans.\nBlock\nConv. Block\nTrans.\nBlock\nConv. Block Conv.\ninputs and outputs of the codec model can also be frequency-\ndomainfeatureslikemagnitudeandphasespectraforreducing\ncomputation burden [71]. There, the convolution kernels are typically 2D instead of 1D in the time-domain codecs.\nLater, Transformers [3] have been adopted, e.g. in Single-\nCodec [72] and Mimi [14]. They can be directly applied\nto frequency-domain inputs and outputs. When operating on\nwaveform-domain inputs or outputs, a CNN [14] or patchi-\nfying [20], [60] operation is usually added before or after\ntheTransformerblocks.InMimi,ashallowTransformerlayer\nis added after the CNN-based encoder, and vice versa in its\ndecoder. Recently, some propose to use purely Transformer-\nbased backbone and discard the CNN blocks, e.g. TS3-\nCodec [20]. As Transformers demonstrate superior modeling\nability and scaling property, these works prove to outperform\nCNN-basedcodecseitherwithlesscomputation[20]orlarger\nFig. 5: Major generator architectures of VQ-GAN-based scale [60]. However, to ensure stream-ability, an attention\nacoustic tokens. ‚ÄúConv.‚Äù, ‚ÄúQ.‚Äù and ‚ÄúTrans.‚Äù are short for mask should be employed [14]. The encoder and decoder\nconvolution, quantizer and Transformer, respectively. can also be designed to be different. For example, Single-\nCodec [72] uses Conformer [73] encoder and CNN decoder,\ndesigning the encoder, decoder, and quantizers, which can be\nwhile LSCodec [29] uses the reverse configuration.\nsummarized as diagrams in Fig.4.\nInaddition,U-Net-basedcodecsemploymultiplequantizers\n1) VQ-GAN: VQ-GAN [66] is a very commonly adopted\nat different layers of the network, rather than relying on\nframework of codec models that trains a VQ-VAE with GAN\na single GVQ or RVQ module for the entire codec. Typi-\nobjectives. Besides the original reconstruction and VQ objec-\ncal examples in this category include CoFi-Codec [74] and\ntives in a VQ-VAE, VQ-GAN uses discriminators d (x,xÀÜ)\nŒæ ESC [75]. In such designs, each sub-encoder or decoder in\nto distinguish real and reconstructed data that adversarially\nthe U-Net can be implemented with a CNN or Transformer,\ntrain the generator network composed of f ,q , and g . In\nŒ∏ œï œà offering more flexible control over the resolution of each VQ\nacousticcodecs,thereareusuallymultiplediscriminators,e.g.\nstream (Section III-B2c).\nmulti-resolutionandmulti-scaleSTFTdiscriminatorsfromthe\nIt is also noteworthy that training a separate vocoder on\nneural vocoder researches [67], [68]. The generator architec-\ntop of existing acoustic tokens may result in improved audio\nture of VQ-GAN-based codec models has multiple choices,\nquality than the original decoded outputs, since reconstruct-\nwith the three most representative ones visualized in Fig.5:\ning waveform alone may be simpler than optimizing VQ\nCNN-based, Transformer-based, and U-Net-based.\nrepresentation and reconstruction at the same time. This is\nThe CNN-based generator is the most widely used ar-\nexemplarily verified in AudioDec [76], MBD [77] and Vo-\nchitecture so far in codec models. SoundStream [69] and\ncos [78]. Therefore, some codec models directly simplify the\nEnCodec [70] are two famous early neural codec models that\nVQ-GAN training objective back to the original VQ-VAE,\noperateinanend-to-endVQ-GANmanner.Theyreceivetime-\nwhere the discrete acoustic tokens are obtained first by a\ndomain waveforms as inputs and directly reconstruct wave-\nsimple reconstruction loss, and a vocoder is trained as an\nforms.Theirencoderanddecoderhaveamirroredarchitecture\nadditionalstage,likeAudioDec[76]andLSCodec[29].These\nto perform down and up-samplings. In SoundStream, the\nworks are denoted as ‚ÄúVQ-VAE+GAN‚Äù in Table S-I.\nencoder and decoder are purely constructed by convolutional\nneural networks (CNNs) while EnCodec augments them with 2) Diffusion: Different from VQ-GAN which uses GAN\nan LSTM. The CNN encoder down-samples the waveform to to generate waveforms or frequency features, some codecs\na high-dimensional embedding sequence, whose frame rate also use denoising diffusion [79], [80] or flow matching\nis determined by the sampling rate, CNN kernel sizes and models [81] as an alternative. Since diffusion and flow\nstridesatafixedratio.Thecontinuousembeddingsarepassed matching belongs to the same family of generative models,\nthrough an RVQ quantizer. The resulting quantized vectors we collectively refer to them as ‚Äúdiffusion‚Äù throughout this\nare summed and then passed to a CNN decoder to reconstruct paper. These diffusion-based codecs use discretized tokens to\nthewaveform.Thetrainingcriteriaincludereconstructionloss conditionthetransformationofstandardGaussiandistributions\n(in the time and frequency domain), adversarial loss, feature to the distribution of some continuous acoustic features, e.g.\nmatching loss, and quantization losses for RVQ layers. To spectrogramfeatures,orthelatentspaceofapretrainedspeech\nallow for a flexible choice of bitrates, structured dropout is autoencoder. The diffusion loss can be propagated back to the\nadopted where the number of codebooks in the RVQ module encoder and quantizer in this design, like [18]. The encoding\ncan be randomly chosen [69], such that only a portion of process in such codecs is identical to that of VQ-GAN. In\nquantizers in front are activated during training. The resulting the decoding process, the decoder runs an iterative sampling\nacoustic tokens can consequently reside in variable bitrates process to generate the target acoustic features, which are\ndepending on the chosen number of RVQ quantizers. The converted to waveforms by a separate pretrained model.\n\n6\n3) Comparisons: Compared to diffusion, VQ-GAN-based exhibiting strong scalability to large model sizes up to 950M\ncodecs are more intuitive in design, and have been a well- parametercount.Italsoexploresaflexiblepost-trainingquan-\nestablished method. Within this category, different architec- tization level adjustment technique and residual FSQ strategy.\nturesofferdistinctadvantages:CNN-basedmodelsareusually Note that most acoustic tokens require multiple quantizers,\nlightweight and context-invariant because of a limited recep- but single-codebook codecs have also been explored. Single-\ntive field. Transformer-based models are easier to scale and Codec [72] designs an encoder consisting of Conformer and\nbelieved to have better compression capacity. U-Net-based bidirectional LSTM to better compress mel spectrogram in-\nmodels offer greater flexibility in quantizer resolutions, but puts. WavTokenizer [90] and BigCodec [91] further explores\nthe correlation of tokens from different quantizers may be single-codebook codec modeling with better network designs\nmorecomplexfordownstreammodelingcomparedtoadjacent or larger parameter count. TS3-Codec [20] adopts a fully\nquantizers in a single RVQ module. Transformer design that leads to a better single-codebook\nHowever, VQ-GAN-based codecs rely on sophisticated dis- codec with fewer computation overhead. LSCodec [29] also\ncriminators, which is crucial to the performance. In contrast, achieves single-codebook coding with speaker disentangle-\ndiffusion-based codecs do not need adversarial training, and ment (Section III-D). These single-codebook codecs with\nthus have a simpler training objective. Inference latency is remarkably low bitrates offer great benefit to downstream\na major concern of diffusion-based codec models, unless speech generation models on simplicity and efficiency.\nspecifically optimized for limited sampling steps. b) Temporal redundancy reduction: Instead of capturing\nalltheinformationthroughVQlayerslikethepreviouslymen-\nB. General-Purpose Acoustic Tokens\ntioned codecs, some researchers have attempted to reduce the\n1) Motivation: In this section, we describe the most com- redundant bitrate of time-varying VQ codes. One reasonable\nmontypeofneuralacoustictokens(speechcodecs)thatarede- method is to encode the global information in speech, e.g.\nsignedonlywiththeobjectiveofspeechsignalreconstruction. speakertimbreandchanneleffects,byaglobalencoderinstead\nThoseacoustictokensaredesignedtoachievebetterobjective of the time-varying codes. Disen-TF-Codec [21] is the first\nor perceptual quality at the lowest possible bitrates. to explore VQ-GAN codec models with an additional global\n2) Approaches: encoder that aids the codec decoder. In Disen-TF-Codec, the\na) Advanced VQ methods and model architectures: global features are designed to be sequential to adapt to\nBased on SoundStream and EnCodec, more codecs with speaker changes during transmission. In TiCodec [22], the\nadvanced VQ methods, network structure, or optimization global tokens are time-invariant and vector-quantized instead.\nstrategies have been researched with depth. As an exam- They are extracted from different segments of an utterance in\nple, DAC [19] achieves remarkable reconstruction quality by conjunctionwithtime-varyingtokens.Similarglobalencoders\nadding periodic inductive bias, better discriminators, modi- are also seen in [72], [74], [92], [93]. FreeCodec [94] further\nfied loss functions, and a better VQ mechanism from ViT- incorporates a prosody encoder [95] that compresses the low-\nVQGAN [52] to improve codebook usage. Specifically, it frequency range of mel spectrograms into a low frame rate\nperforms L2-normed code lookup in a low-dimensional space VQ sequence to assist in reconstruction.\n(e.g. 8 or 32) instead of a high-dimensional space like 1024. Another typical example of temporal redundancy reduc-\nOther architectural improvements include using frequency- tion is predictive coding, as seen in TF-Codec [55]. This\ndomain inputs [72], [82], [83], variance-constrained residual approach captures temporal-varying information in the latent\nblocks [84], multi-filter bank discriminator [84], selective spacebyautoregressiveprediction,whichsignificantlyreduces\ndown-sampling back-projection [85], etc. redundancy and entropy in the residual part for quantization.\nSeveral training tricks are explored, such as not applying LMCodec [96] employs autoregressive prediction from coarse\nVQ with a certain probability and pure adversarial training codes (first RVQ levels) to fine codes (last RVQ levels) [5],\nproposed in Moshi [14]. Also, the training of neural speech enabling the transmission of fewer codes.\ncodecs does not need to be end-to-end, i.e. the learning of c) Multi-resolution and variable-bitrate coding: Rather\nVQrepresentationsandsignalreconstructioncanbeseparated. than relying solely on uni-resolution tokens, where all quan-\n[76], [86] adopt a two-stage training process that introduces tizers share the same temporal frequency, it is reasonable to\nadversariallossesandanadditionalvocoderaftertrainingonly design multi-resolution codecs, because speech contains both\nwith metric losses, to achieve improved quality. Additional fast and slow information streams. For instance, many vowels\ntrainingcriteriaontheVQmoduleareproposedforbetterVQ exhibit slowly changing characteristics, while events such\nutilization. For example, ERVQ [87] introduces a fine-grained as explosive consonants and background noises require fine-\ncode-vector replacement strategy, a codebook balancing loss, grained modeling. Therefore, incorporating multiple temporal\nand a similarity loss between consecutive RVQ layers. resolutions in codecs is likely to reduce the necessary bitrate.\nOther VQ methods besides GVQ or RVQ also exist in SNAC [23] is a notable multi-resolution neural speech\nspeech codecs. NDVQ [88] improves the capacity of RVQ codec. It follows the DAC [19] architecture, but in each RVQ\nspace by changing codebook vectors to parameterized Gaus- layer,residualsaredownsampledbeforecodebooklook-upand\nsian distributions. FSQ has also been introduced to several upsampled afterward. This enables SNAC to have three RVQ\nspeech codecs, like SQ-Codec [89] where scalar rounding is streamsataframerateof12,23,47Hzrespectively.Similarly,\napplied to each of its 32-dimensional latent space. Stable- CoFi-Codec [74] achieves multi-resolution tokenization by\nCodec [60] adopts FSQ in a Transformer-based architecture, GVQ quantizers within its U-Net-based architecture. LLM-\n\n7\nDiscriminator\nEncoder Quantizer Decoder\nSemantic\npredictor\nDiscriminator\nQuantizer\nEncoder Decoder\n(frozen)\nSemantic\ncodebook\nDiscriminator\nEncoder Quantizer Decoder\nerutaef\ncitnameS\ncitnames\ndexiF\nserutaef\ncitnameS\necnadiug\nkoobedoc\nstuptuo\nro\nstupni\nsa\nSSL models aim to capture high-level phonetic or semantic\ninformationwithoutexternalsupervision[12],integratingSSL\nfeatures does not impose additional data requirements for\ninjectingsemanticinformationintothetrainingprocess.Codec\nmodels with criteria beyond reconstruction are sometimes\nSemantic referred to as having a ‚Äúmixed objective‚Äù [1]. Given that\nfeatures\nthe primary purpose of these models remains acoustic recon-\nstruction in these models, we continue to refer to them as\nacoustic tokens. The process of introducing semantic infor-\nmation into acoustic tokens is termed semantic distillation,\nwith approaches summarized in Fig. 6.\n2) Approaches:\na) Semantic feature guidance: The earliest effort in\nsemantic distillation is to guide some RVQ layers in codec\nmodels towards semantic features, which are typically SSL\nfeatures.SinceinformationinRVQnaturallyfollowsacoarse-\nto-fine order, guiding early RVQ layers towards semantic-\noriented features helps establish and reinforce a semantic-\nSemantic Semantic to-acoustic information hierarchy. For example, SpeechTok-\nfeatures features enizer [13] uses a HuBERT [34] SSL model to guide the\nfirst RVQ layer in EnCodec. This ensures that the first RVQ\nFig. 6: Different semantic distillation methods applied to\nlayer contains more semantic information, thereby pushing\nacoustic tokens (illustrated with the VQ-GAN architecture).\nacousticdetailstothesubsequentRVQlayers.Thisdistillation\nCodec [25] also adopts this idea to achieve very low frame is implemented either by regressing the first RVQ output to\nrates with semantic distillation (Section III-C). continuous HuBERT embeddings or by classifying it into\nIn addition to multiple temporal resolutions, it is also discrete HuBERT tokens. LLM-Codec [25] alternatively uses\nfeasible to consider the varying information intensities across Whisper[38]andT5[97]assemanticteachers.Mimi[14]uses\ndifferentspeechframes.Thisobservationmotivatesthedesign aWavLM[35]teacherandappliesdistillationtoaspecialized\nofcodecstoallocatedifferentnumbersofquantizersfordiffer- VQmoduleratherthanthefirstRVQlayer.Pairedspeech-text\nent speech frames. As an example, VRVQ [24] automatically data can also be utilized, like in SecoustiCodec [98] where\nselectsthenumberofRVQquantizersperframebyapredictor aligned phoneme sequences serve as the semantic teacher.\nthat is jointly trained with the whole network. Since SSL feature guidance occurs only during the training\n3) Challenges: Despite the emergence of single-codebook stage, it does not incur additional inference costs. It has been\nand low-bitrate codecs [29], [72], [90], [91], achieving ideal reported that language modeling-based TTS trained with such\nreconstructionqualitywithahighlylimitedVQspaceremains acoustic tokens can exhibit better robustness than those with\na challenging problem. Additionally, as acoustic tokens aim unguided tokens [13].\nto encode all necessary information for signal recovery, they b) Fixed semantic codebook: A more direct approach to\nmay become redundant and overly complex for downstream achieve semantic distillation is to integrate semantic knowl-\nmodeling. While scaling up the model size or switching edge into the codebook of quantizers. This forces the quanti-\nto non-causal networks has been shown to improve perfor- zationspaceitselftobemoresemantic-related.Thismethodis\nmance[60],[72],[91],theseapproachesmayalsocompromise proposedinLLM-Codec[25]whereallthreeRVQcodebooks\nstreamability or efficiency. Furthermore, simply introducing are initiated from the token embedding module of LLaMa-\nglobal encoders like[21], [22], [74] does notguarantee disen- 2 [99] and remain frozen during training. According to [25],\ntanglement (Section III-D) and may still result in redundancy this approach reduces the bitrate and improves the semantic\nwithin the time-varying codes. representation ability of LLM-Codec.\nc) Semantic features as inputs or outputs: Semantic\nC. Acoustic Tokens with Semantic Distillation\nfeatures can also be compressed together with the acoustic\n1) Motivation: Acoustictokensareaconvenientchoicefor features. This requires the encoder and quantizer to construct\nspoken language models, as they can be directly converted a shared acoustic and semantic space that balances the two\nback to waveforms without the need for extra vocoders. informationsources.Thefirstattemptinthisdirectionismade\nHowever,ifreconstructionisthesoleobjectiveofthesetokens, in [100] where Conformer representations from a pretrained\ntheir representation space may become overly complex and wav2vec2.0[33]arecombinedwithCNNencoderoutputsfor\noverly focused on acoustic details, in contrast to natural quantization. SemantiCodec [18] quantizes AudioMAE [101]\nlanguage tokens that primarily carry semantic information. A SSLfeatureswithoutrelyingonacousticinputs.Thequantized\nnaturalimprovementistoincorporatespeechsemanticfeatures SSLfeaturesthenserveasaconditionforacousticreconstruc-\neither from speech self-supervised learning (SSL) models, tion using latent diffusion, which resembles a vocoder that\nsupervised models, or even text transcriptions. Since speech transforms semantic inputs into acoustic outputs. Providing\n\n8\naligned phoneme sequences instead of SSL features to the viaRVQ,andthespeakerbranchistrainedusingacontrastive\nquantizer has also shown benefits on reducing bitrates [71]. loss to produce speaker embeddings. Disentanglement is en-\nMoreover, semantic features can also serve as outputs, forced by a GRL between the speaker embeddings produced\nthereby reinforcing the constraint that semantic information from the speaker branch and the content representations.\nbe compressed into the discrete latent space. For instance, Similarly, PromptCodec minimizes an SSIM loss [106] be-\n[26],[92]combinehiddenHuBERTembeddingswithacoustic tween content and speaker representations, with the help of a\nfeaturesbeforeRVQandjointlyoptimizesacousticandseman- pretrained speaker verification model.\ntic reconstruction objectives. X-Codec 2.0 [102] improves it SuchGRLtechniqueisnotlimitedtodisentanglingspeaker\nby using w2v-BERT 2.0 [103] and FSQ. XY-Tokenizer [104] timbre alone. FACodec [28] employs supervised decoupling\nfurther replaces the semantic reconstruction objective by an to factorize speech into speaker timbre, content, prosody, and\nLLM-basedASRtask,aimingatstrongeralignmentwithtexts. acoustic detail information. The timbre extractor in FACodec\n3) Challenges: Guiding part of the RVQ layers towards is optimized via a speaker classification loss. For the other\nsemanticfeaturesdoesnotguaranteethatacousticinformation components ‚Äì prosody, content, and acoustic detail ‚Äì separate\nis only encoded in the remaining layers, as shown by the RVQ modules are applied prior to the supervised decoupling\ndegraded VC performance in SpeechTokenizer [13]. It may process. For each component, a supervision signal specific\nimpose a greater challenge for the VQ layer to encode both to the desired information is applied, and GRL is employed\nacoustic and semantic information if semantic features serve to other non-related information components. These three\nas inputs as well. Additionally, fixing a semantic codebook quantized features are then combined before applying GRL\ncould negatively impact acoustic reconstruction ability, as the with the speaker information. Finally, the decoder integrates\nVQ representation space will become overly restricted. all four information branches to reconstruct the speech signal.\nD. Acoustic Tokens with Disentanglement b) Perturbation: For speaker disentanglement, a more\nstraightforward approach is to apply speaker timbre perturba-\n1) Motivation: Another line of mixed-objective acoustic\ntions to speech signals and leverage the strong information\ntokens is disentanglement. A prominent research direction is\nbottleneck created by the discrete VQ module. When the\nthe disentanglement of speaker timbre information, as this\nencoder is unable to learn sufficient timbre information, and\nis a global trait among all the speech information aspects.\nthe decoder is provided with prominent timbre, the bottleneck\nEncoding speaker information into every token timestep is\nin the middle will naturally prevent timbre from being en-\nredundant; thus, removing the global speaker timbre can\ncoded [107]. This idea is adopted in LSCodec [29], which\nmake the information in acoustic tokens more compact and\nappliesatimestretching-basedspeakerperturbationalgorithm\nreducethenecessarybitrate.Speaker-decoupledspeechtokens\nto the input waveform. LSCodec then leverages continuous\ncan alleviate the modeling burden for downstream tasks.\nWavLM features to represent speaker timbre, and feeds them\nFor example, a TTS model using these tokens can achieve\nto a Conformer-based decoder by position-agnostic cross at-\nindependent control over prosody and speaker identity. The\ntention [108], [109]. Through this approach, LSCodec reports\ndisentanglement of speaker timbre also enables an acoustic\nhigh-qualityspeechreconstructionandvoiceconversionusing\ntoken to perform voice conversion (VC), as timbre from\nonly a single codebook with highly limited bitrates.\nthe target speaker can be easily combined with the speaker-\nc) Sourceseparation: Apartfromthedisentanglementof\nagnostic content tokens from the source speech.\nspeakertimbre,sourceseparationhasalsobeenexploredinthe\nNote that in Section III, it is mentioned that some codecs\ncontext of acoustic tokens. SD-Codec [30] and DeCodec [31]\nintroduce a global encoder to reduce the necessary bitrate of\nproposetodecoupledifferentaudiosourcesintheneuralcodec\ntime-variant tokens [21], [22], [72], [94]. They have already\nbyencodingtheminseparateRVQmodules.DeCodecalsoim-\ndemonstrated some ability to decouple global speaker timbre\nproves separability by enforcing an orthogonality loss. These\nand local contents, albeit in an implicit manner through the\napproaches allow for more efficient and targeted processing\nnatural information bottleneck from VQ. In this section, we\nof each audio component, and are also related to the broader\nelaborateonexplicitmethods,whichinvolvespecializedtrain-\nscope of universal audio coding.\ning techniques and criteria to achieve disentanglement. Also,\n3) Challenges: The GRL technique for disentanglement\ndisentanglementisnotexclusivewithsemanticdistillation,and\ninherentlycarriestheriskofamorecomplexoptimizationtra-\nmany codecs incorporate techniques for both objectives.\n2) Approaches: jectory. Additionally, some disentanglement methods require\na) Gradient reversal layer (GRL): The GRL tech- supervised data [28], which imposes a significant constraint.\nnique [105] is commonly used for disentanglement. Suppose Due to the intricate nature of speech informatics, current\nspeaker information needs to be disentangled, and a classifier efforts are still suboptimal compared to semantic tokens,\n(or speaker verifier, etc.) s (¬∑) receives some latent feature particularly in terms of VC performance [29].\n¬µ\nh from the codec model to perform speaker discriminative\ntasks. GRL operates by negating the gradient sign before\nIV. SPEECHTOKENIZATIONMETHODS:SEMANTIC\ns (¬∑), thereby forcing h to fool the speaker classifier while\nTOKENS\n¬µ\nthe classifier itself improves, similar to adversarial training. Semantic tokens refer to discrete speech representations\nSSVC[27]isoneofthepioneeringeffortsinthisdirection. from discriminative or self-supervised learning (SSL) models.\nSSVC attempts to decouple content and speaker representa- While we use the term semantic tokens to maintain consis-\ntions from WavLM features. The content branch is quantized tency with prior works, some researchers recently argue that\n\n9\nCNN\nTransformer\nClustering\nor\nVQ-VAE\nSSL\nContinuous Discrete Semantic features speech tokens\nTokens Transformer\nDiscrete\nspeech tokens\nCNN\nCNN\nQuantizer\nQuantizer\nTransformer\nDecoder\nTask-Specific\nThe\nquick\nfox\n‚Ä¶\nSupervised\nSemantic\nTokens\nDiscrete\nspeech tokens\nQuantizer\nTransformer layer with a portion of training data [4],\n[129]. It is also feasible to perform clustering on multiple\nExternal\nQuantization layers [130], [131], or train a VQ-VAE on the SSL hidden\nembeddings [132], [133].\n‚Ä¢ Internal quantization: when an SSL model contains an\ninner quantizer that is trained together with other network\nmodules, its outputs can also be regarded as semantic\nInternal tokens. Many SSL models involve quantizers to produce\nQuantization\ntargetsfortheirtrainingobjectives[32],[33],[134],[135].\nThis approach provides an efficient and effective way of\nextracting semantic tokens.\nNote that for SSL models with an inner quantizer, it is still\npractical to perform external quantization on its continuous\nembeddings, like wav2vec 2.0 [33]. However, these two\nmethods ‚Äì internal and external quantization ‚Äì may result\nin different patterns of information exhibition, which we will\nFig. 7: Representatives in different kinds of semantic tokens. investigate in Section VI.\nDotted box means the module is optional. Forgeneral-purposeSSLmodels,therearedifferentdesigns\nonthepretexttask[12].TableIIIprovidesasummaryofwell-\nknown semantic tokens.\nSSL features are more accurately described as phonetic than\na) Contrastive models: This type of speech SSL models\nsemantic [110] in nature. Hence to clarify, in this review,\naimstolearnrepresentationsbydistinguishingatargetsample\nsemantic tokens should be more accurately defined as the\n(positives) from distractors (negatives) given an anchor [12].\ncomplementary set of acoustic tokens, such that they are not\nThey minimize the latent space similarity of negative pairs\nprimarily aimed at reconstruction purposes. In practice, the\nand maximize that of the positive pairs. For semantic tokens,\nvast majority of these tokens are designed for discriminative\nvq-wav2vec [32] and wav2vec 2.0 [33] are two representative\ntasks and are believed to have a strong correlation with\ncontrastive SSL models. They involve a quantizer to produce\nphonetic and semantic information [12], [111]‚Äì[113].\nlocalized features that is contrastively compared to contextu-\nalized continuous features. Vq-wav2vec [32] uses pure CNN\nA. Semantic Tokens from SSL Models\nblocks while wav2vec 2.0 [33] adopts a Transformer for\n1) Motivation: When fine-tuned, speech SSL models have\nstronger modeling capacity. Both use GVQ quantizers to\nshown strong performance on various tasks, often surpassing\nexpand the VQ space. Wav2vec 2.0 has also been extended\ntraditional methods [12], [114]. Their potential has been\nto massively multilingual versions [124], [136], [137].\nextensively mined in discriminative tasks such as automatic b) Predictive models: This type of speech SSL models\nspeech recognition (ASR) [32], [34], [115], [116], automatic\nincorporates an external target for prediction, either from sig-\nspeaker verification (ASV) [35], [117], [118], speech emotion\nnal processing features or another teacher network. A popular\nrecognition (SER) [35], [119]‚Äì[121] and speech translation\nline of work is HuBERT [34]. It takes raw waveforms as\n(ST)[122]‚Äì[124].DiscretizedSSLtokensareinitiallyfavored\ninputs, applies random masks on the hidden representations\nfor reducing computation costs and improving robustness\nbefore Transformer contextual blocks, and then predicts k-\nagainst irrelevant information for ASR [125]. Driven by the\nmeans quantized targets from MFCC or another HuBERT\nsuccess of language models, these SSL tokens have been\nteacher.WavLM[35]improvesHuBERTbyadditionalspeaker\nfurther explored in generative tasks such as TTS [15], [126],\nand noise augmentations to achieve superior performance in\n[127] and SLM [4], [5], [128]. This is because they can be\nmoreparalinguistic-relatedtasks.Therearenoinnerquantizers\nconsideredhigh-levelabstractionsofspeechsemanticsthatare\nin both models, so external quantization like k-means cluster-\nlargely independent of acoustic details.\ning is necessary to obtain semantic tokens. BEST-RQ [134]\n2) Approaches: SSLmodelsinitiatethelearningprocessby changes the prediction target to the output of a random\ndefiningapretexttaskwhichenablesthemodeltolearnmean- projection quantizer. The next-token prediction criterion from\ningful representations directly from the data itself. Typical language models (LMs) have also been adopted into speech\nspeech SSL models employ CNNs and Transformer encoders SSL[138],[139],eitherwithorwithoutapretrainedtextLM.\nto extract deep contextual embeddings. When it comes to Thismethodemphasizestheautoregressivepredictionproperty\nsemantic tokens, there are mainly two ways to extract those oflearnedtokensthatmaybebettersuitedfortheLMusecase.\ndiscrete tokens from an SSL model (see upper part of Fig.7): c) Perturbation-invariantmodels: AsSSLtokensfeature\n‚Ä¢ External quantization, like clustering or training a VQ- semantic or phonetic information, a major concern is to\nVAE.Thisreferstoextractingcontinuousembeddingsfrom improvetheresistanceagainstperturbationsintheinputsignal.\nacertainlayerormultiplelayersinapretrainedSSLmodel, This invariance includes noise and speaker aspects that don‚Äôt\nand performing quantization manually. For example, a affect the contents of speech. Specifically, speaker-invariant\ncommon semantic token is the HuBERT+kmeans units, SSL tokens aims to remove speaker information, similar to\nwhere k-means clustering is performed on a HuBERT speaker-disentangled acoustic tokens. In the training process\n\n10\nof perturbation-invariant SSL models, noise [37], [140]‚Äì[142] its supervised tokenizer trained on Chinese and English can\norspeaker[36],[143]‚Äì[145]augmentationsareoftenexplicitly also work in Japanese and Korean, it remains unclear how\nintroduced. Special training losses are then incorporated, like well these supervised tokenizers generalize to more unseen\ncontrastive losses [36], [141], [142], [145] that distinguish languages.\nthe same spoken content among perturbed distractors, and\ndistribution-basedmetrics[37],[143],[144]thatminimizethe C. Speech Token Vocoders\ndistance of latent features caused by perturbations. Acoustic tokens are naturally coupled with a decoder that\n3) Challenges: Firstly, SSL models typically require large outputswaveformsorspectrogramsgiventokens,butsemantic\namount of data to train, as indicated in Table S-II. For SSL tokens do not inherently require a vocoder, particularly for\nmodels without a built-in quantizer during pretraining, k- speech understanding tasks. However, when semantic tokens\nmeans clustering is a prevalent approach to obtain discrete are used for speech generation, a speech token vocoder\nunits. However, given that most SSL models operate in high- (also known as speech resynthesis model) becomes necessary.\ndimensional spaces (e.g., with 768 or 1024 dimensions), the Unlike traditional spectrogram-based vocoders [153], speech\nspace and time complexity of k-means clustering are substan- tokenvocodersusuallyneedtocompensatethelossofacoustic\ntial. The clustering results can sometimes be unreliable due details in semantic tokens by introducing additional inputs.\nto the curse of dimensionality in Euclidean space. Moreover, Polyak et al. [146] first explores speech resynthesis from\nit is often reported, and will also be shown by experiments discrete speech units by a HifiGAN [153] augmented with\nin Section VI, that discretized SSL units lose much acoustic discretized pitch units and speaker embedding inputs. The\ndetailsafterquantization[112],[146],[147].Differentcluster- vec2wav vocoder in VQTTS [126] improves this vocoder by\ningsettings,suchasthechosenlayerandvocabularysize,can aConformer[73]frontendmodulebeforeHifiGANgenerator.\nleadtodifferentoutcomeswithinasinglemodel.Finally,since Later,CTX-vec2wav[108]proposesaposition-agnosticcross-\nmost SSL models utilize Transformer blocks, their causality attentionmechanismthateffectivelyintegratestimbreinforma-\nand streaming ability are compromised. tion from surrounding acoustic contexts without the need for\nWhile perturbation-invariant SSL models have emerged as pretrained speaker embeddings. This makes it more timbre-\npromising approaches for semantic tokens, they currently rely controllable and suitable for zero-shot TTS and VC [109].\non content-preserving augmentations that are typically hand- Upon it, vec2wav 2.0 [154] introduces SSL timbre features\ncrafted. Most methods in this type have only been evaluated and adaptive activations to improve timbre controllability, and\non small-scale data and models. It also remains unclear how reports competitive VC performance.\nthese methods will generally benefit generative tasks such as It is also feasible to apply diffusion or flow matching\nspeech generation and spoken language modeling. algorithms in token vocoders [39], [155], [156]. There, the\ndiscrete tokens are treated as a condition for diffusion or flow\nmatching to generate mel-spectrograms, and further converted\nB. Semantic Tokens from Supervised Models\nto waveform by a pretrained mel vocoder. Compared to train-\nAs representing semantic or phonetic information is the ing a token-to-wav vocoder in an end-to-end fashion, training\nmajor purpose of semantic tokens, a more direct way to a token-to-mel model is more convenient and does not need\nachieve this is through supervised learning. Supervised se- adversarial training. To better control timbre, a mask strategy\nmantic tokenizers are typically trained on the ASR task. A is introduced into the training process where the model only\nfamous example shown at the bottom of Fig.7 is the S3 computes loss on the un-masked part of spectrograms [39].\nTokenizer from CosyVoice [39]. It places a single-codebook Duringinference,spectrogramfromspeakerpromptconditions\nVQ layer between two Transformer encoder modules and the generative process, which can be regarded as a form\noptimizesthenetworkusingacross-entropyASRloss,similar of ‚Äúin-context learning‚Äù. However, this requires tokens to\nto Whisper [38]. The same method is adopted in [148], [149] be extracted from reference prompts before synthesis. Also,\nwhere the frame rate is further reduced to 12.5Hz. CosyVoice inferenceefficiencymaybecompromisedforbettergeneration\n2 [150] improves S3 Tokenizer by replacing plain VQ with quality with multiple inference steps, and this method is only\nFSQ for better codebook utilization. Note that in this kind validated on massive amount of data currently.\nof supervised semantic tokens, it is the output of the VQ\nlayer that serves as tokens. This allows for more preservation V. LENGTHREDUCTIONANDVARIABLE-RATE\nof paralinguistic information than directly transcribing speech TOKENIZATION\ninto text. CosyVoice 3 [151] extends supervised tokens to In most cases, the frame rate of discrete speech tokens\nmore tasks involving language, emotion, speaker and audio ranges from 25 to 100Hz. This leads to a huge discrepancy\nanalysis. These supervised tokenizers are trained on massive in lengths between speech representations and the underlying\npaired speech-text data, and have demonstrated rich speech text modality. This discrepancy has been a critical issue\ncontent understanding capabilities [39], [152]. in building decoder-only TTS and other LM-based speech\nHowever, training these models is highly costly due to generationtasks[157],sincelongersequencesresultinharder\nthe heavy data demands. Training with only the ASR task training and more unstable inference. Therefore, researchers\nmay still result in the loss of some prosody information, and have proposed different ways to mitigate this issue, either\ntraining with multiple tasks poses a higher demand for data by post-processing on tokens, or learning-based variable-rate\nand task balancing. Although [150] has demonstrated that tokenization.\n\n11\nA. Length Reduction by Deduplication and Acoustic BPE\n85 Post-training length reduction methods for speech tokens\n80\nare inspired by language processing techniques. A common 75\napproach to reduce token sequence lengths is deduplica-\n70 tion[125],[158],i.e.removingtherepeatedconsecutivetokens 65\ninasequence.Sincetheencodedcontinuousfeaturesareoften 60\ncloseinconsecutiveframeswherethespeechdynamicsdonot 55\nchangerapidly,theyarelikelytobequantizedtothesameunit. 50 Therefore, removing these redundant tokens can yield a more 45\nphonetic representation. When the deduplicated tokens are 40\nused for generative modeling, a unit-to-speech model (similar 35\nto TTS) should be employed to upsample the tokens and 30\nconvert them back to acoustic signals [4]. 25\nAnother popular approach is acoustic byte-pair encoding 20\n0 2000 4000 6000 8000 10000 12000\n(BPE)4 or so-called subword modeling [125], [159]‚Äì[162]. BPE size\nSimilar to text BPE [163], acoustic BPE iteratively merges\nthetwomostfrequentconsecutivetokensandaddsthemerged\ntoken to the vocabulary. After training on a corpus, a deter-\nministic BPE mapping is established between original token\ncombinations and the new vocabulary. This mapping enables\na lossless compression algorithm, allowing tokens to be per-\nfectly reconstructed after BPE decoding. This operation can\nidentifycertainmorphologicalpatternsintokensequences,and\noffersapowerfulwaytoremoveredundanttokens.Inpractice,\nacousticBPEsonHuBERTsemantictokenshasdemonstrated\nsignificant speed and performance gains in ASR [125], [158],\nspokenlanguagemodeling[161],[162]andTTS[127],[164].\nAlthough deduplication is a simple and training-free\nmethod,acousticBPEoffersseveraluniqueadvantagesoverit.\nFirst,acousticBPEcanidentifyredundantpatternsthatarenot\nsimply repetitions, whereas deduplication only removes exact\nduplicates. Second, deduplication discards the duration infor-\nmation of every token in the resulting sequence. This could\nbe problematic for downstream tasks, as important rhythmic\ninformationmayresideintherepetitionsoftokens.Incontrast,\nacoustic BPE preserves duration information by encoding\nrepetitions of varying lengths into distinct new tokens. Third,\nacoustic BPE is more flexible than deduplication in terms of\ntarget vocabulary size, which can be adjusted based on the\ndesired length reduction ratio and downstream performance.\nWevisualizethelengthreductioneffectofBPEondifferent\nspeechtokensinFig.8,includingacousticandsemantictokens\nall with a single codebook. From Fig.8, it is evident that\ndifferenttypesoftokensexhibitverydistinctpatterns.Seman-\ntic tokens generally show significant length reduction when\napplying BPE. For single-codebook acoustic tokens, speaker-\ndecoupledLSCodectokensshowmorereductionthangeneral-\npurpose WavTokenizer and BigCodec. This suggests that the\neffect of BPE is negatively correlated with the information\ndensityinthespeechtokens:thelessinformationtheycontain,\nthe more length reduction is achieved by BPE.\nB. Variable Frame Rate Tokens and Unit Discovery\nInformationinspeechisnotuniformlydistributedalongthe\ntime axis [165]. In segments such as silence or long vowels,\n4The term ‚Äúacoustic‚Äù here is used to distinguish it from traditional BPE\nappliedtotexttokens,ratherthanreferringto‚Äúacoustictokens‚Äù.\n)zH(\netar\nemarf\n.gvA\n0.976x0.965x 50 0.994x 48\n46 0.991x 0.983x 0.979x\n44\nLSCodec (50Hz) 42 LSCodec (25Hz) 40\nWavTokenizer (small-75Hz)\nWavTokenizer (small-40Hz) 38\nBigCodec 36 SecoustiCodec (21.5Hz) SecoustiCodec (86Hz) 34\n0.928x 32 0.871x 30\n0.814x 0.995x 0.986x 0.983x 28\n0.752x 26\n0.686x0.667x 24\n22\n0.960x0.929x 0.891x 0.879x 20\n0.987x0.979x 18\n2000 4000 6000 8000 10000 12000\nBPE size\nSingle-codebook Acoustic Tokens\n)zH(\netar\nemarf\n.gvA\nS3 Tokenizer (25Hz) S3 Tokenizer\n(50Hz)\nContentVec-500 (L12+km2048)\nHuBERT-Large\n0.833x (L24+km2048) WavLM-Large (L24+km2048)\n0.732x\n0.704x 0.681x 0.666x\n0.662x 0.629x 0.613x\n0.603x 0.535x\n0.516x0.506x 0.487x\n0.500x\n0.866x 0.469x\n0.766x 0.739x\nSingle-codebook Semantic Tokens\nFig.8:BPEeffectcomparisonofmultipletokens.Thestarting\npoint of each line represents the original vocabulary size.\ninformationdensityislow,whereasinsegmentswithexplosive\nconsonants, speech events occur much more frequently. This\ninherentnon-uniformitysuggeststhatitmightbemorenatural\ntoallocatemoretokenizedbitstoregionswithdenseinforma-\ntion and higher variance, and fewer bits to regions with less\nuncertainty. This kind of discrete speech tokens is referred to\nas variable frame rate (VFR) tokens in this review. Note that\nwhile multi-resolution and variable-bitrate tokens have been\nintroduced previously, the concept of VFR is still distinct. In\nmulti-resolution tokens [23], [74], each quantizer operates at\na fixed frame rate. In variable-bitrate tokens [24], the frame\nrate remains fixed, while the variability lies in the number\nof quantizers per frame. Instead, VFR tokens should directly\nallocate different granularities on the temporal axis.\nVFRtokensarecloselyrelatedtoacousticunitdiscovery.As\nspeech lacks a natural boundary of phonetic units [12], there\nare much research efforts to find and locate the underlying\nacoustic units behind speech utterances in an unsupervised\nmanner [166]‚Äì[169]. This is particularly of interest for low-\nresourcelanguages.Thediscoveredunitscanguidethebound-\nary segmentation of VFR tokens. To this end, VFR tokens are\ninteresting not only because they might reduce the necessary\nbitrate, but also because they can introduce a strong inductive\nbias that linguistic knowledge is encoded [165].\nA recent direction of VFR tokens is to discover acoustic\nunits from an SSL model. Sylber [170] and SyllableLM [43]\ntake similar approaches that first heuristically locate acoustic\nboundaries from existing HuBERT models, and then train\nanother HuBERT student with segment-level pooled targets\nbetween boundaries. The final HuBERT embeddings undergo\nthe same segment-level pooling and kmeans clustering proce-\nduretoproducetokensataverylowframerate(‚âà5Hz).Such\ntokens are reported to align well with syllables [43], [170].\nBoundary prediction can be involved to achieve frame rate\nvariability in the training process, where a specific model\npredicts frame-level boundaries and is trained together with\nother network modules. The training techniques of such mod-\nels include reinforcement learning [171], soft pooling [145],\n\n12\nand slowness constraint [165]. tokens. ESPnet-Codec [180] integrates multiple codecs into\nFor VFR acoustic tokens, heuristic downsampling methods a unified training and evaluation framework VERSA [181]\nhave been explored recently. TFC [172] incorporates a frame- and extends evaluation to some generative tasks such as TTS\nrate selection strategy based on entropy values, assigning and SVS. DASB [147] performs more downstream probing\ndifferentdownsamplingratestosegmentswithvaryingentropy tasks,andincludesgenerativetasksaswellassemantictokens.\nlevels under a shared quantizer. CodecSlime [173] adopts a STAB [182] takes a different perspective that measures the\ndynamic programming-based downsampling strategy driven invariance,robustness,compressibility,andvocabularyutiliza-\nby pairwise latent embedding distances, and further intro- tion of speech tokens. This emphasizes the application in\nduces a specialized training method to adapt fixed frame-rate spoken language models instead of reconstruction.\ncodecmodelstodynamicframerates.Bothapproachesenable\nflexible frame-rate allocation within a single model, and are\northogonal to the codec architecture. However, the study of B. Existing Analyses\nVFR acoustic tokens remains generally insufficient.\nSimilar to supervised semantic tokens, explicit linguistic There are several theoretical or experimental analyses of\nboundaries can also be incorporated into VFR design. Kara- the advantages of discrete speech tokens. Nguyen et al. [183]\npiperisetal.[174]andTASTE[175]areaninitialexplorations demonstrates by an encoder-only language model that seman-\nin this direction, where the tokens are aligned with the frame tic SSL tokens are favorable for spoken language modeling,\nrate of phonemes or text BPE units, resulting in variable due to their removal of linguistically irrelevant information.\ntemporal spans. Karapiperis et al. [174] is an acoustic token Sicherman et al. [112] supports this claim by showing that\nthat directly performs phoneme-level downsampling before semantic units have a strong correlation with phonemes, but\nquantization. In contrast, TASTE aggregates information from a weaker correlation with speakers. Abdullah et al. [184]\na whole spoken utterance onto each text BPE unit by cross- refines the correlation between semantic SSL tokens and\nattention. The BPE-level aggregated speech embeddings are linguistic content to the ‚Äúsub-phonemic‚Äù level instead of\nthen quantized and optimized using a TTS-style objective, high-level phonemes due to contextual variability. Chang et\nThese works redefine the notion of tokens: rather than en- al. [125] explores the use of WavLM tokens for end-to-end\ncoding spoken information by themselves, they serve as an ASR together with deduplication and BPE. Although these\nadditional acoustic layer conditioned on explicit text content. tokens underperform continuous SSL features, they still show\ncompetitive performance. Similar findings are reported on\nVI. ANALYSISOFDISCRETESPEECHTOKENS\ncontextual ASR [185], multilingual ASR [186], end-to-end\nA. Metrics and Existing Benchmarks speechtranslation,understanding[158],andmoreLLM-based\nDiscrete speech tokens can be evaluated from various as- semantic-related tasks with discrete units as inputs [187].\npects besides bitrate and codebook utilization: Expresso [188] evaluates the resynthesis quality of HuBERT\n‚Ä¢ Signal-level reconstruction metrics: For reconstruc- and EnCodec tokens on an expressive dataset, finding that\ntion evaluations, signal-level metrics like PESQ [176], HuBERT struggles to preserve source speech expressivity\nSTOI[177],meldistance,GPE[178],etc.areoftenused. while EnCodec performs better.\n‚Ä¢ Perceptual reconstruction metrics: Apart from signal- The downside of speech tokens is also studied. Yeh et\nlevel metrics, there can also be perceptual evaluations of al. [113] suggests that VQ on HuBERT embeddings can-\nreconstruction performance. This includes intelligibility not achieve perfect disentanglement of speaker and phonetic\n(oftenmeasuredbyword,character,orphoneerrorrates), content. EMO-Codec [189] shows that codec reconstruction\nspeaker similarity, subjective listening tests, etc. still sometimes degrades the emotion information. O‚ÄôReilly et\n‚Ä¢ Performance on downstream tasks: Probing tasks can al. [190] shows that neural audio codecs often lack stability\nbe used to measure the preservation or prominence of after repeated encoding and decoding, i.e. not idempotent.\ncertain information in tokens, like ASR, ASV, emotion ERSB[191]revealsspeechcodecsstillstruggleundercomplex\nrecognition, and spoken language modeling [169]. Note acoustic environments, in terms of both reconstruction and\nthat this is different from perceptual reconstruction met- downstream task consistency.\nrics since it operates directly on tokens. Performance in Therefore, the reconstruction quality of acoustic tokens\ngenerative tasks like TTS and VC can also be evaluated. and the performance on discriminative downstream tasks of\n‚Ä¢ Semantic/phonetic relevance: If the tokens are ex- both acoustic and semantic tokens have been benchmarked.\npected to align with texts (e.g. for semantic tokens and However, the reconstruction performance of semantic tokens\nsemantic-distilled acoustic tokens), metrics like phone still requires a more thorough comparison. Hence, we adopt\ndiscrminability [169], phone purity [34], and phone- a reconstruction approach to compare different types of to-\nnormalized mutual information [34] can be computed. kens. Specifically, we use a timbre-controllable speech token\n‚Ä¢ Invariance and robustness: If the tokens are expected vocoder to resynthesize semantic tokens into waveforms and\nto be invariant to perturbations, unit edit distance [140] measurethepreservationofcontent,prosody,speakeridentity,\ncan be considered as a measurement. andacousticdetails,respectively.WealsofollowDASB[147]\nThere are several existing benchmarks on discrete speech toconductsemanticmodelingprobingtasksonvarioustokens.\ntokens. Codec-SUPERB [179] evaluates both signal-level re- The detailed setups of these experiments can be found in the\nconstructionmetricsanddownstreamperformancesofacoustic appendix.\n\n13\nTABLEI:Reconstruction,voiceconversion,anddownstreamsemanticmodelingcomparisonsoftokensindifferentcategories.\nFor reconstruction and voice conversion, we train a specific CTX-vec2wavŒ± vocoder [29] on LibriTTS [192] for all semantic\ntokens. Settings in parentheses denote model versions. Semantic modeling tasks, ASR and IC (intent classification), follow the\nDASB[147]setupwhichusestokenindicestotrainasimpleLSTMnetwork.‚ÄúL‚ÄùmeansacertainlayerintheSSLTransformer\nblock,‚Äúkm‚Äùmeansmanualk-meansclustering,and‚ÄúNC‚Äùisshortfor‚Äúnotconverged‚Äù.Pleaserefertotheappendixforevaluation\ndetails.\nToken Bitrate‚Üì Reconstruction VoiceConversion SemanticModeling\nTokenType\nModel(version) (kbps)\nWER‚Üì GPE‚Üì PESQ‚Üë STOI‚Üë WER‚Üì SECS‚Üë P.Corr‚Üë ASRWER‚Üì ICACC‚Üë\nGroundTruthRecording - 1.16 0.00 4.50 1.000 - - - - -\nContinuousBaselines\nMel+BigVGAN(100band) - 1.18 0.88 4.30 0.995 - - - 17.4 50.1\nEnCodec(Q=8) 6.00 1.53 1.33 2.83 0.946 - - - 19.4 34.8\nDAC(24kHz,Q=8) 6.00 1.34 0.93 3.52 0.958 - - - 26.1 18.3\nAcoustic TiCodec(Q=2) 1.50 3.31 1.51 2.00 0.898 2.62 0.642 0.886 26.4 41.7\n(General-Purpose) SNAC(24kHz) 0.98 2.25 1.48 2.23 0.914 - - - 35.4 16.7\nWavTokenizer(Small,F=75Hz) 0.90 2.45 1.63 2.47 0.925 - - - 37.2 15.5\nStable-Codec(2residualFSQ) 0.70 4.94 1.73 2.16 0.917 - - - NC 14.0\nSpeechTokenizer 4.00 1.47 1.20 2.60 0.930 - - - 19.3 57.3\nX-Codec(HuBERTLibriSpeech) 4.00 1.27 1.49 2.82 0.905 - - - 9.8 69.6\nAcoustic\nMimi 1.10 2.44 1.68 2.27 0.917 - - - 26.8 50.9\n(SemanticDistillation)\nLLM-Codec 0.85 6.25 1.86 1.82 0.879 - - - NC 16.2\nSemantiCodec(F=25Hz,V=213+215) 0.70 3.44 2.28 1.75 0.866 - - - NC 26.8\nAcoustic FACodec(withdetailcodes) 4.80 1.37 1.02 2.91 0.954 1.57 0.773 0.583 14.6 51.1\n(Disentanglement) LSCodec(F=50Hz) 0.45 3.33 2.42 1.77 0.688 4.04 0.852 0.697 25.3 49.8\nvq-wav2vec(k-means) 1.80 2.81 2.73 1.49 0.795 3.27 0.857 0.718 16.9 58.7\nwav2vec2.0(Large,innerquantizer) 0.90 3.24 2.92 1.52 0.680 4.40 0.814 0.759 22.0 51.9\nwav2vec2.0(Large,L14+km2048) 0.55 2.51 9.57 1.20 0.630 2.81 0.880 0.492 5.8 69.5\nSemantic(SSL)\nHuBERT(Large,L24+km2048) 0.55 1.86 15.65 1.17 0.625 1.97 0.876 0.375 6.1 67.2\nWavLM(Large,L24+km2048) 0.55 1.67 17.94 1.16 0.621 1.92 0.872 0.374 6.1 74.2\nContentVec(L12+km2048) 0.55 2.09 18.88 1.15 0.613 2.21 0.869 0.348 5.5 72.0\nSemantic(Supervised) S3Tokenizer(F=50Hz) 0.60 2.12 4.25 1.37 0.673 2.52 0.868 0.687 17.5 67.2\nC. Reconstruction Analysis Wetake representativeworksineach tokencategory.When\nthere are multiple feasible configurations for a model, we\nTo enable a fair comparison between acoustic and semantic\nchoose one typical configuration that balances bitrate and\ntokens from a reconstruction perspective, we train a CTX-\nperformance. Note that different variants (especially on frame\nvec2wavŒ± vocoder [29] for different semantic tokens on\nrateandnumberofquantizers)withinthesamemodelcanlead\nLibriTTS [192]. This vocoder supplements the insufficient\nto significant differences in reported metrics. For SSL models\nspeaker timbre information in semantic tokens using continu-\nlike wav2vec 2.0, HuBERT and WavLM, we take the official\nous WavLM features extracted from the reference prompts.\n‚ÄúLarge‚Äù model variant. For wav2vec 2.0, we experiment with\nThis approach enables semantic tokens to perform voice\nboth its inner quantizer before the Transformer blocks and k-\nconversion(VC)byswitchingreferencepromptsconveniently.\nmeans clustering results on a specific Transformer layer.\nThe training details follow [108]. We compute several metrics\nThe results are shown in Table I. It is evident that acous-\nfor reconstruction ability:\ntic tokens designed only for reconstruction can achieve de-\n‚Ä¢ WER (word error rate, in percentage) measures the con- cent speech quality, but still far from the state-of-the-art\ntentintelligibilityofreconstructedspeech.Itiscomputed spectrogram-based vocoders because of higher compression\nbetween ground truth texts and ASR-decoded transcrip- rates. Retaining good speech intelligibility (i.e. low WER)\ntions. We use NeMo-ASR5 here. becomes particularly challenging when the frame rate is low.\n‚Ä¢ GPE (gross pitch error, in percentage) measures the Acoustic tokens with semantic distillation can also achieve\nrelative error percentage of pitch contours of the recon- strong reconstruction quality. Explicitly disentangled acoustic\nstructed speech compared to ground truth. tokensmaysacrificesomereconstructionperformancemetrics\n‚Ä¢ PESQ (perceptual evaluation of speech quality) and when the bitrate is extremely low. Semantic tokens generally\nSTOI (short-time objective intelligibility) measure the struggle to achieve the same level of acoustic reconstruction\nspeech quality from a signal perspective. as acoustic tokens, as evidenced by lower GPE, PESQ, and\nWeuseLibriTTStestset-B [108]asthetestsetforevaluations. STOI scores. Notably, most semantic tokens included exhibit\nIt contains 500 utterances from unseen speakers that sum up significant information loss in prosody as reflected by their\nto about 1 hour. We use the original utterance to provide GPE scores. However, their WER scores remain comparable\ntimbreinformationwhennecessary,i.e.forTiCodec,FACodec, to acoustic tokens, despite having much lower bitrates. This\nLSCodec and all semantic tokens. All evaluation metrics highlights the property that semantic tokens primarily retain\nare computed on 16kHz waveform for a fair comparison, content-related information rather than acoustic details.\nand reconstructed waveforms with higher sampling rates are\ndownsampled before evaluation. D. Voice Conversion Analysis\nDespite the loss of acoustic information, a prominent ad-\n5https://huggingface.co/nvidia/stt en fastconformer transducer large vantage of semantic tokens over most acoustic tokens is their\n\n14\ninherenttimbrecontrollability.Someacoustictokensalsohave 18 target classes and accuracy (ACC) as the metric. For ASR,\nthis ability, such as those with a global encoder like TiCodec we use LibriSpeech [195] 960h to train the probing network\nand disentangled acoustic tokens, also possess this ability To and report WER on test-clean split.\ncompare this ability across different tokens, we conduct voice TheresultscanbefoundatthelasttwocolumnsofTableI.\nconversion(VC)experimentsusingthesetokensasthecontent General-purposeacoustictokensshowrelativelyworseseman-\nfrom the source speech. We use the same source utterances in ticmodelingabilitiesthanothersunderasmallprobingmodel,\nSection VI-C, but assign a different target speaker for each although some acoustic tokens have excellent reconstruction\nsource utterance as the prompt. quality. Semantic distillation can enhance such downstream\nThen, we perform VC experiments on the 500 VC pairs. In performance, especially X-Codec which uses SSL features\nadditiontoWER,wealsomeasureSECS(speakerembedding both as input and outputs. Meanwhile, semantic tokens gen-\ncosine similarity) [193] as the metric for speaker similarity, erally achieve significantly better results in such tasks, with\nand P.Corr [154] as an objective metric for prosody preser- predictive SSL tokens being the best. For semantic tokens,\nvation. SECS requires a speaker verification model6 to output downstream ASR performance appears to be negatively cor-\nspeaker embeddings. P.Corr calculates the Pearson correlation related with prosody preservation: tokens that better preserve\ncoefficient between the pitch contours of the converted and prosodyoftenyieldworseASRresults.Thistrendisobserved\nsourceutterances.NotethatP.Corrwillbemeaninglesslyhigh in vq-wav2vec, wav2vec 2.0, and even the supervised S3\nif the VC similarity is low, i.e., when the source timbre is Tokenizer, all of which employ an internal quantizer. In\nbarely changed. As the source utterances are the same as summary, these experiments reveal a trade-off between how\nSection VI-C, the WER numbers are directly comparable to strongly semantic information is emphasized in the tokens,\nthose in the reconstruction experiments. and how comprehensively it is preserved.\nThe results presented in Table I indicate that semantic\ntokens often achieve much higher VC similarity compared\nVII. DISCRETESPEECHTOKEN-BASEDAPPLICATION\nto acoustic tokens. However, due to the substantial loss of\nPARADIGMS\nprosody information, semantic tokens tend to have lower\nA. Spoken Language Understanding\nP.Corr scores than acoustic tokens. Among the acoustic to-\nkens capable of performing VC, explicit disentanglement 1) Motivation: Spoken language understanding (SLU)\nmethods, such as FACodec and LSCodec, outperform the tasks, including automatic speech recognition (ASR), speech\nimplicit criterion employed in TiCodec. It is also noteworthy translation, intent classification and others, aim to extract\nthat different tokenization settings in wav2vec 2.0 lead to meaningful domain-specific information from speech. Most\ndrasticallydifferentoutcomes.Tokensgeneratedfromitsinner SLU tasks follow a speech-in text-out pipeline, except S2ST\nquantizer preserve prosody well but also retain much speaker which also involves speech generation. The adoption of dis-\ninformation, whereas clusters derived from its Transformer cretetokensinSLUofferssomebenefits.Discretetokensmay\nhidden embeddings exhibit the opposite characteristics. naturally exhibit some invariance against noise and speaker\nSupervised semantic tokens from S3 Tokenizer also exhibit information, particularly semantic tokens, which can make\ngood intelligibility and VC ability. Unlike HuBERT-style SSL downstream models to focus more effectively on content-\nmodels, this supervised tokenizer demonstrates better preser- related information in some tasks. On a broader scale, dis-\nvation of prosody both in reconstruction and VC settings. crete tokens provide a promising approach to unifying speech\nGiven that prosody and intonation are a crucial factors for understanding and generation in spoken language models.\nASR, it is reasonable to assume that the tokenizer‚Äôs VQ AsanalternativeinputtoanSLUmodelinsteadofcontinu-\nmodule encodes some prosody information. In contrast, while ous features, discrete speech tokens are typically deduplicated\nHuBERT-style SSL models do contain rich prosody informa- or BPE-encoded before subsequent modules. Semantic tokens\ntion in their continuous features (e.g., as evidenced by good have been better explored than acoustic ones in this context.\nemotion recognition results [114]), phonetic information is 2) Speech Translation: Among the various SLU tasks, dis-\nlikely the primary component. Therefore, offline clustering is crete speech tokens are mostly adopted in speech translation,\nprone to discard these prosody characteristics. including speech-to-text translation (S2TT) and speech-to-\nspeech translation (S2ST). Since semantic tokens correlate\nE. Downstream Semantic Modeling Analysis well with phonetics, they can serve as universal pseudo-labels\nfor untranscribed languages, useful for S2TT in low-resource\nBesides reconstruction and voice conversion which mainly\nsettings [196]. Direct S2ST using discrete tokens has gar-\ncoveracousticaspects,wealsoexplorethesemanticmodeling\nnered more attention on the generation side (Section VII-B).\nabilitiesinthetokens.Tothisend,wefollowtheDASB[147]\nEarly approaches primarily rely on extracting discrete tokens\nsetup and considers two representative downstream semantic\nusing VQ-VAEs, particularly for unwritten languages [197],\nmodeling tasks: ASR and intent classification (IC). In both\n[198]. Recent researches in this area include employing se-\ntasks, we train a small LSTM-based probing network to\nmantic tokens [199]‚Äì[201], acoustic tokens [202]‚Äì[204], two-\nprocess the discrete speech tokens and produce outputs. IC\npass architectures [205], [206], and non-autoregressive frame-\nis a classification task to determine the spoken intents from\nworks[207].Theseeffortscollectivelycontributetoadvancing\nspeech directly, where we use the SLURP [194] dataset with\nthe performance and applicability of discrete token-based\n6https://github.com/resemble-ai/Resemblyzer speech translation systems.\n\n15\n3) Unified Speech Understanding: Discrete tokens provide 2) Autoregressive TTS: Autoregressively predicting the\nopportunity to construct unified and adaptable spoken lan- next VQ index of discrete speech tokens is first proposed in\nguagemodelswithvariousSLUfunctionalities.Effortsinclude VQTTS [126], which uses an LSTM conditioned on Trans-\ntask identifies [7], prompt tuning [201], [208]‚Äì[210], shared former representations to generate vq-wav2vec [32] seman-\naudio and text vocabulary [211], and combining continuous tic tokens. A discrete token vocoder converts the tokens to\nand discrete representations [212]. These efforts highlight the waveformswiththeassistanceofhandcraftedprosodyfeatures.\npotential of discrete tokens in enhancing the performance and VQTTSachievesstate-of-the-artTTSqualityatthattime,and\nversatility of universal SLU models. showspromisingperformanceinspeaker-adaptiveTTS[224]‚Äì\n4) Limitations: Despite the advantages and growing pop- [226] and expressive TTS [227].\nularity in S2ST tasks, discrete tokens still underperform in Subsequently, decoder-only TTS models using neural au-\nmany SLU tasks. Lots of SLU studies [158], [185], [186], dio codecs have made tremendous success in zero-shot TTS\n[213], [214] only verify that discrete tokens can surpass startingfromVALL-E[6].VALL-Econtainsanautoregressive\ntraditional frequency-domain features in certain tasks such (AR) model and non-autoregressive (NAR) model, both of\nas ASR. Continuous SSL features continue to have superior which generate EnCodec [70] RVQ tokens. The AR model\nperformance [187]. The majority of current LLM-based SLU performs next-token prediction on the first RVQ layer condi-\nmodels rely predominantly on continuous inputs, such as tioned on text. The NAR model predicts the n+1-th RVQ\nWhisper features [215]‚Äì[220]. Moreover, the performance of tokens given the text, all EnCodec tokens from the speaker\ndiscrete tokens in speaker-related tasks is generally much in- reference, and the previous n RVQ layers. VALL-E employs\nferiortothatofcontinuousfeatures[147],[213].Asignificant aconcisedesigninwhichtextandspeakerreferencesserveas\nlimitation of discrete tokens for SLU is the inevitable infor- ‚Äúprompts‚Äù for a language model. It achieves remarkable zero-\nmation loss during the quantization process. Mitigating such shot TTS performance when trained on 60k hours of speech.\nloss with more VQ codebooks may hinder the accessibility of Later, methods have been proposed to improve generation\nsemantic information crucial for SLU as well. Therefore, the robustness [228]‚Äì[233], efficiency [234], style control [235]‚Äì\nfull potential of leveraging discrete tokens for SLU remains [237], and to incorporate LLMs [238], [239].\nlargely untapped and warrants further exploration. Besides using an NAR model to predict the rest RVQ\nlayers,alternatemodelingstrategieshavebeenproposed,such\nas hierarchical modeling [240] and token interleaving pat-\nB. Speech Generation terns [241], [242]. Semantic tokens are also introduced to\ncooperatewithacousticcodecs[15],[127],[239],[243],which\n1) Motivation: Discrete tokens have catalyzed a paradigm\nmight decrease the modeling difficulty since they bridge the\nshift in speech generation, with TTS being the most represen-\ngap between texts and acoustics and usually require only\ntative application. In TTS systems, discrete tokens are usually\na single token stream. Numerous industry-level large-scale\nused as intermediate features that bridge the acoustic model\nTTS systems have been produced in this autoregressive TTS\n(text-to-token) and the vocoder or codec decoder (token-to-\nparadigm, such as XTTS [244], BASE-TTS [245], Seed-\nwav). There are two major advantages of applying discrete\nTTS [156], CosyVoice [39], [150], Fish-Speech [246], etc.\ntokens in TTS:\n3) Non-AutoregressiveTTS: Whileautoregressivemodeling\n‚Ä¢ Easier training objectives. Discrete tokens replace the is the current mainstream of TTS with discrete tokens, non-\noriginal spectrogram-based regression task with a clas-\nautoregressive models also exist. These models either treat\nsification task [126], which can be much easier. This\nthe code-vectors as continuous features [247], or directly\nalso offers a better balance between acoustic models and\ngenerate discrete tokens by masked prediction [133] or dis-\nvocoders, since texts are closer to discrete speech tokens\ncrete diffusion models [28], [108]. These non-autoregressive\nthan frequency-domain features.\nmethodsarenaturallymorerobustthanautoregressivemethods\n‚Ä¢ Better use of decoder-only language models. Decoder- in inference, and also supports speech editing.\nonly language models have shown remarkable success in\n4) Unified Speech Generation: The language modeling\nnatural language generation. After discretization, speech\napproachofdiscretetokensallowsaunifiedgenerationframe-\ncan also be autoregressively generated under the same\nwork for multiple tasks. It suffices to use a task identifier\nparadigm. This offers huge potential in leveraging the\nto condition the unified language model. For example, [248],\nin-context learning and scaling capabilities of language\n[249]extendsVALL-Ewithmoretaskslikecross-lingualTTS,\nmodels to achieve zero-shot high-fidelity TTS [6].\nS2ST,speechediting,etc.UniAudio[240]supports11speech\nOther generative tasks, such as singing voice synthesis and and audio generation tasks within a single hierarchical Trans-\nspeech editing, can similarly benefit from the advantages of former model. Prompt tuning upon a spoken language model\ndiscrete tokens observed in TTS. For voice conversion (VC), has also been explored in [201] for efficient, transferable and\nusing discrete tokens as content representations can simplify versatilegeneration.Theseeffortsdemonstratethepotentialof\ntheprocesstoatokenvocoder[154],whentimbreinformation a large-scale foundation model for generation.\nis effectively removed from the tokens. Tasks like speech to 5) Limitations: In contrast to discrete tokens, another\nspeech translation [198], [199], speech enhancement [221], emergingframeworkforspeechgenerationisdiffusionorflow\n[222]andtargetspeakerextraction[223]canalsobeenhanced matching-based models, including non-autoregressive mod-\nthrough language modeling on discrete tokens. els [250]‚Äì[253] or autoregressive models [254]‚Äì[258]. They\n\n16\ngenerate continuous features, and some even eliminate the researches,especiallyfollowingworklikeOpenAI‚ÄôsGPT-4o7,\nneed for forced alignments in non-autoregressive generation. have focused on SLMs that combine three key capabilities:\nOwingtothestrongcapabilityofdiffusionandflowmatching strongunderstandingofspeechsemantics,high-qualityspeech\nalgorithms, they also have remarkable generation fidelity, output, and low latency [14], [149], [152], [266]‚Äì[278]. We\ndiversity and controllability. They can have a higher upper refer to them as text-guided spoken language models (TG-\nbound for speech quality and intelligibility, as they inherently SLMs). Unlike TF-SLMs, while TG-SLMs utilize a unified\navoid quantization errors. Incomparison, discrete token-based LLM for seamless processing of user‚Äôs speech input and\nspeech generation models usually fall short in generation system‚Äôs speech output, they internally decompose the end-\nrobustness. Therefore, there is an ongoing debate between to-end speech dialogue process into two well-established sub-\ndiscrete and continuous representations for speech generation. procedures: SLU powered by LLMs, and real-time TTS. The\ntwo sub-procedures are connected via text as an intermediary\nC. Text-Free Spoken Language Models to stabilize the semantic coherency of the final output. The\nLLMfirstgeneratesatextualresponsetotheaudioinput,then\n1) Motivation: End-to-end spoken language and dialogue\nsynthesizes the speech token sequence in a streaming fashion.\nmodeling is one of the most ultimate goals in speech technol-\nInaTG-SLM,theSLUsub-procedureusuallyusescontinuous\nogy. Discrete tokens are a core component of existing spoken\nspeech features as input since they preserve more acoustic\nlanguage models, as they enable the language modeling tech-\ndetails for understanding, while the TTS sub-procedure typi-\nnique to be applied directly on speech. The models discussed\ncally uses discrete speech tokens as output to better fit LLM\nin this subsection are text-free spoken language models (TF-\nautoregressive generation.\nSLMs). We anticipate that a well-trained TF-SLM will be\n2) Speech Generation in TG-SLMs: To reduce modeling\ncapable of generating semantically coherent speech without\ncomplexity and better align with the autoregressive gener-\nthe need for text transcription guidance.\nation paradigm of LLMs, TG-SLMs favor single-layer dis-\n2) ExistingEfforts: EversinceGSLM[4]andAudioLM[5]\ncrete speech tokens as direct LLM outputs. Existing works\nproposed the vision of TF-SLMs, building such models re-\nmake use of either the first layer of an RVQ codec [266],\nmains a significant challenge even till today. This difficulty\nsingle-codebook codec [276], or single-codebook supervised\nprimarily arises from the lower language modeling efficiency\nsemantic token [149], [277]. Specific designs are introduced\nof speech token sequences compared to text, due to their\ncorresponding to the tokens, such as chain-of-modality [266],\nlower semantic information density, longer sequence lengths,\ntoken interleaving to lower latency [149], two-stage decoding\nand the presence of paralinguistic information [157]. Current\nprocess [276], etc. To better rebuild the speech information\nadvancementsinTF-SLMsmainlyfocusontwostrategies:(1)\nwiththehelpofpretrainedLLMs,severalTG-SLMsusemulti-\nreducing token frame rates, and (2) aligning speech with text.\nlayerspeechtokensasLLMoutput,suchas[14],[268],[269].\nThe first approach aims to shorten speech sequences and\nThey often employ different techniques to generate the text\nenhance semantic density by lowering frame rates [4], [128],\ntokensandmulti-layerspeechtokensinparallelreducelatency.\n[161] to even ‚âà5Hz [43], [170]. While mitigating sequence\nMainstreamTG-SLMswithdiscretetokensasLLMoutputs\nlength issues to different degrees, they still encounter scala-\nneed an additional decoder to synthesize continuous speech\nbilitylimitations[259]andcompromisereconstructionquality.\nsignals, either using the codec decoder or a separately-trained\nThesecondstrategyinvolvesaligningspeechwithtextthrough\nvocoder. There are also efforts to streamingly synthesize\nmethodslikeinitializingpre-trainingwithtextLLMs[128],re-\nspeech signals directly based on the LLM hidden embed-\ninforcementlearningusingASRandtextLLMfeedback[260],\nding [270], [278], eliminating the need for discrete tokens,\ntext-speech token interleaving [261], adopting novel architec-\nadditionaldecoders,orevenexplicittexttokens,hencefurther\nturesappliedintextlanguagemodeling[262],etc[263],[264].\nimproving the real-time ability.\nMeanwhile, full duplex modeling has been proposed [265]\n3) Limitations: Overall, TG-SLMs‚Äô task decomposition is\nto allow users to interrupt and start new dialogues at will.\neffective and flexible. The SLU sub-procedure can handle\nHowever, despite many efforts, these models still struggle to\nboth continuous and discrete representations, and single-layer\ngenerate semantically reliable long speech during inference\ndiscrete tokens simplify the training and inference of the\ndue to the lack of explicit transcription guidance.\nTTS sub-procedure. However, unlike TF-SLMs, TG-SLMs\n3) Limitations: Although these methods show promise,\nrely heavily on text as an intermediary in the TTS sub-\nachieving semantic coherence is still a challenging goal,\nprocedure, which may overlook paralinguistic information\nleavingsignificantprogresstobemadetowardthegoaloftruly\nsuchasemotion,prosody,andenvironmentalcontextfromthe\nend-to-endspokenlanguagemodeling.Improvingthesemantic\npreviousinput,resultinginlesscoherentandnaturalresponse.\ndensity and expressiveness of discrete speech representations,\nAdditionally,thelackofhigh-qualityannotatedconversational\nmaking it easier to align text and speech during TF-SLM\ndataandconcernsoversecurityposesignificantchallengesfor\ntraining, is a promising direction for future exploration.\nthe future development of TG-SLMs.\nVIII. CHALLENGESANDFUTUREDIRECTIONS\nD. Text-Guided Spoken Language Models\nCurrent discrete speech tokens still exhibit certain limita-\n1) Motivation: Since TF-SLM remains an open problem,\ntionsandchallengesthatneedtobeaddressed.Inthissection,\nthe prevalent successful speech dialogue systems settle for an\nalternative choice that uses text as explicit guidance. Recent 7https://openai.com/index/hello-gpt-4o/\n\n17\nwe summarize the existing challenges in this field and outline 5) Combining Acoustic and Semantic Tokens: Given the\nthe corresponding future directions. distinct properties of acoustic and semantic tokens, a natural\nquestionarises:Canarepresentationspacecontainrichspeech\n1) Low-Bitrate Tokens: For bitrates of tokens, factors Q\nunderstanding capabilities while also reconstructing acoustic\n(number of quantizers) and F (frame rate) play a more\ndetails at a decent level? Incorporating semantic information\nimportant role than V (vocabulary size). Using only a sin-\nfromSSLmodelshasproventoenhancethereconstructionand\ngle codebook is very beneficial for language modeling and\ndownstream modeling performance of acoustic tokens [13],\ngeneration since it frees the need for additional designs for\n[18], [26]. Recently, explicit text supervision has also sparked\nmulti-codebook tokens. A critical problem lies in how to\nremarkable progress in acoustic tokens [98], [104], [279]. We\nbetter utilize the highly-compact discrete VQ representation\nanticipate more promising results in this direction.\nspace. For F, the frame rates of most tokens are still much\ngreater than text sequences, which can significantly influence 6) ParalinguisticsinSemanticTokens: Whilespeakerinfor-\nthe syntactic and semantic modeling capability of language mationisgenerallyconsideredirrelevantforsemanticcontent,\nmodels [157]. However, there is usually noticeable perfor- prosody serves as a crucial component of paralinguistic in-\nmance and intelligibility degradation for tokens with single formation. Semantic tokens derived through simple clustering\ncodebook and small F. A lower V is also desirable for methods are likely to discard both speaker information and\nlanguage modeling and length reduction by BPE. prosody, harming downstream models‚Äô ability to handle rich\nItremainsanopenproblemwhatthelowerboundofbitrate emotions, tones, singing voices, and non-verbal vocalizations\nand the frame rate F are, and how to reach them. More that convey semantic meaning. This problem can be partially\npowerful network architectures or advanced VQ strategies mitigated by certain VQ approaches that encode more in-\nshould be helpful, and reducing temporal redundancy by formation from SSL features [130]‚Äì[132], but at a cost of\ndisentangling global information is also a promising solution. more codebooks and higher bitrates. Supervised tokenization\ncould also be considered for directly guiding tokens toward\n2) StreamingAbilityandEfficiency: Real-timeapplications\nparalinguistic information in the future.\nrequire tokens to be stream-able both in encoding and decod-\ning. For most CNN-based acoustic tokens, achieving this is 7) Noise Preservation vs. Noise Robustness: Similar to\neasyduetotheirfixedreceptivefields.Foracoustictokenswith disentanglement in acoustic tokens, the inclusion or exclusion\nTransformer blocks, an attention mask is necessary. However, of background noise and channel effects in the tokens also\nmost SSL models employ a non-causal Transformer architec- depends on the specific application. Most acoustic tokens are\nture,whichmakessemantictokensderivedfromthesemodels designedtocapturenoise,buttheirperformanceacrossvarious\nunsuitable for real-time tokenization. It remains unclear how types of noise and channel effects remains unclear. This issue\nmuchperformancedegradationwouldresultfromtransitioning extends beyond speech and relates to the broader scope of\ntocausalarchitecturesinbothSSLmodelsandtokenvocoders. neuralaudiocodecs.Ontheotherhand,denoising[69]isalso\nStreaming ability also poses a requirement for model effi- an interesting application of acoustic tokens that leverages the\nciency.Currently,largeracoustictokenmodelsarereportedto limitedVQspace.Ifnoiseisconsideredundesirableintokens,\nachieve better performance with lower bitrates [60], [91], but such as semantic tokens, then the robustness against various\nat a cost of efficiency. In addition to reducing the bitrate of types of signal perturbations needs to be investigated.\nthe tokenized codes, the efficiency of tokenizers must also be 8) Timbre Control in Token Vocoders: For semantic tokens\nbalanced for real-time applications. andspeaker-decoupledacoustictokens,tokenvocodersshould\n3) Disentanglement in Acoustic Tokens: Whether disentan- be responsible for controlling speaker timbre. Currently, both\nglement should be incorporated into acoustic tokens depends GAN-based token-to-wav vocoders [154] and flow matching-\non the specific application. If reconstruction is the major basedtoken-to-melmodels[39]havedemonstratedstrongtim-\nobjective, disentanglement may not be necessary. However, bre control capabilities. It remains an open question whether\ndisentanglement can help reduce the bitrate in time-varying the upper bound of the former method can be improved by\ntokens, ensure anonymity during transmission, reduce down- trainingonlarge-scaledatasets,asisdonewiththelatter.Also,\nstream modeling complexity, and achieve independent control thetimbrecontrollabilityofin-the-wildreferencepromptswith\nofdifferencevoiceproperties.Therearecurrentlyonlylimited various acoustic conditions should be further investigated.\neffortsondecoupledacoustictokens,andthedecouplingeffect\nIX. CONCLUSION\nis still suboptimal or causing a negative impact on recon-\nstruction quality. More advanced techniques for information Recently, discrete speech tokens have emerged as a rapidly\ndecoupling should be considered in the future. evolvingfieldandacoreresearchdirectioninthespeechLLM\nera.Thesetokensencodeacousticorsemanticinformationinto\n4) Variable Frame Rate Tokens: As mentioned in Section\na compact discrete representation space, catalyzing the fusion\nV-B, the variable-rate nature of linguistic units can offer an\nof LLMs and speech processing. In this review, we provide\nimportant insight for further reducing the bitrate of tokens,\na comprehensive introduction to representative categories of\nand more importantly, closing the gap between speech tokens\ndiscrete speech tokens, summarizing their motivations and\nand natural language units for downstream tasks. More explo-\nlimitations. We conduct a unified analysis of reconstruction,\nrationsneedtobetakenonvariableframerateacoustictokens\nvoice conversion, and downstream semantic modeling across\nand the benefit of these variable frame rate tokens in practice.\ndifferent token types to highlight their unique characteristics.\n\n18\nWe also review popular applications of discrete tokens in [24] Y. Chae, W. Choi, Y. Takida et al., ‚ÄúVariable bitrate residual vector\nspeech processing tasks, including understanding, generation quantizationforaudiocoding,‚ÄùinProc.IEEEICASSP,2025.\n[25] D. Yang, H. Guo, Y. Wang et al., ‚ÄúUniAudio 1.5: Large Language\nand language modeling of speech. Finally, we explore future\nModel-Driven Audio Codec is A Few-Shot Audio Task Learner,‚Äù in\ndirections for discrete speech tokenization methods. We hope Proc.NeurIPS,2024.\nthisreviewlaysasolidfoundationforfutureresearchinspeech [26] Z.Ye,P.Sun,J.Leietal.,‚ÄúCodecdoesmatter:Exploringthesemantic\nshortcoming of codec for audio language model,‚Äù in Proc. AAAI,\ntechnology.\nvol.39,no.24,2025,pp.25697‚Äì25705.\n[27] A¬¥.Mart¬¥ƒ±n-Cortinas,D.Sa¬¥ez-Trigueros,I.Valle¬¥s-Pe¬¥rezetal.,‚ÄúEnhanc-\nACKNOWLEDGMENTS\ningthestabilityofLLM-basedspeechgenerationsystemsthroughself-\nWe thank Haoran Wang, Jingyu Zhou, and Shuai Wang for supervisedrepresentations,‚ÄùarXivpreprintarXiv:2402.03407,2024.\n[28] Z. Ju, Y. Wang, K. Shen et al., ‚ÄúNaturalSpeech 3: Zero-Shot Speech\ntheir contribution in a tutorial related to this review paper.\nSynthesis with Factorized Codec and Diffusion Models,‚Äù in Proc.\nICML,2024.\nREFERENCES\n[29] Y.Guo,Z.Li,C.Du,H.Wang,X.Chen,andK.Yu,‚ÄúLSCodec:Low-\nBitrateandSpeaker-DecoupledDiscreteSpeechCodec,‚ÄùinProc.ISCA\n[1] W. Cui, D. Yu, X. Jiao et al., ‚ÄúRecent advances in speech language\nInterspeech,2025,pp.5018‚Äì5022.\nmodels:Asurvey,‚ÄùinProc.ACL,Jul.2025,pp.13943‚Äì13970.\n[30] X.Bie,X.Liu,andG.Richard,‚ÄúLearningsourcedisentanglementin\n[2] S.Ji,Y.Chen,M.Fangetal.,‚ÄúWavChat:ASurveyofSpokenDialogue\nneuralaudiocodec,‚ÄùinProc.IEEEICASSP,2025.\nModels,‚ÄùarXivpreprintarXiv:2411.13577,2024.\n[3] A.Vaswani,N.Shazeer,N.Parmaretal.,‚ÄúAttentionIsAllYouNeed,‚Äù [31] X.Luo,J.Huang,R.Yangetal.,‚ÄúDeCodec:Rethinkingaudiocodecs\ninProc.NeurIPS,2017,p.6000‚Äì6010. as universal disentangled representation learners,‚Äù arXiv preprint\n[4] K.Lakhotia,E.Kharitonov,W.-N.Hsuetal.,‚ÄúOnGenerativeSpoken\narXiv:2509.09201,2025.\nLanguageModelingfromRawAudio,‚ÄùTrans.ACL,vol.9,pp.1336‚Äì [32] A.Baevski,S.Schneider,andM.Auli,‚Äúvq-wav2vec:Self-Supervised\n1354,2021. LearningofDiscreteSpeechRepresentations,‚ÄùinProc.ICLR,2020.\n[5] Z. Borsos, R. Marinier, D. Vincent et al., ‚ÄúAudioLM: A Language [33] A. Baevski, Y. Zhou, A. Mohamed et al., ‚Äúwav2vec 2.0: A Frame-\nModeling Approach to Audio Generation,‚Äù IEEE/ACM Trans. ASLP., workforSelf-SupervisedLearningofSpeechRepresentations,‚ÄùProc.\nvol.31,pp.2523‚Äì2533,2023. NeurIPS,vol.33,pp.12449‚Äì12460,2020.\n[6] S. Chen, C. Wang, Y. Wu et al., ‚ÄúNeural codec language models [34] W.-N.Hsu,B.Bolte,Y.-H.H.Tsaietal.,‚ÄúHuBERT:Self-Supervised\nare zero-shot text to speech synthesizers,‚Äù IEEE/ACM Trans. ASLP., Speech Representation Learning by Masked Prediction of Hidden\nvol.33,pp.705‚Äì718,2025. Units,‚ÄùIEEE/ACMTrans.ASLP.,vol.29,pp.3451‚Äì3460,2021.\n[7] T. Wang, L. Zhou, Z. Zhang et al., ‚ÄúVioLA: Conditional Lan- [35] S. Chen, C. Wang, Z. Chen et al., ‚ÄúWavLM: Large-Scale Self-\nguage Models for Speech Recognition, Synthesis, and Translation,‚Äù Supervised Pre-Training for Full Stack Speech Processing,‚Äù IEEE\nIEEE/ACMTrans.ASLP.,2024. JSTSP,vol.16,no.6,pp.1505‚Äì1518,2022.\n[8] H.Wu,X.Chen,Y.-C.Linetal.,‚ÄúTowardsAudioLanguageModelling: [36] K. Qian, Y. Zhang, H. Gao et al., ‚ÄúContentVec: An Improved Self-\nAnOverview,‚ÄùarXivpreprintarXiv:2402.13236,2024. SupervisedSpeechRepresentationbyDisentanglingSpeakers,‚ÄùinProc.\n[9] M.KimandJ.Skoglund,‚ÄúNeuralSpeechandAudioCoding:Modern ICML. PMLR,2022,pp.18003‚Äì18017.\nAI Technology Meets Traditional Codecs,‚Äù IEEE Signal Processing [37] S. Messica and Y. Adi, ‚ÄúNAST: Noise Aware Speech Tokenization\nMagazine,vol.41,no.6,pp.85‚Äì93,2024. for Speech Language Models,‚Äù in Proc. ISCA Interspeech, 2024, pp.\n[10] M.Anees,‚ÄúSpeechCodingTechniquesandChallenges:AComprehen- 4169‚Äì4173.\nsive Literature Survey,‚Äù Multimedia Tools and Applications, vol. 83, [38] A.Radford,J.W.Kim,T.Xuetal.,‚ÄúRobustSpeechRecognitionvia\nno.10,pp.29859‚Äì29879,2024. Large-Scale Weak Supervision,‚Äù in Proc. ICML. PMLR, 2023, pp.\n[11] J.Du,X.Chen,H.Wuetal.,‚ÄúCodecFake-Omni:ALarge-ScaleCodec- 28492‚Äì28518.\nbased Deepfake Speech Dataset,‚Äù arXiv preprint arXiv:2501.08238, [39] Z.Du,Q.Chen,S.Zhangetal.,‚ÄúCosyVoice:AScalableMultilingual\n2025. Zero-Shot Text-to-Speech Synthesizer Based on Supervised Semantic\n[12] A. Mohamed, H.-y. Lee, L. Borgholt et al., ‚ÄúSelf-Supervised Speech Tokens,‚ÄùarXivpreprintarXiv:2407.05407,2024.\nRepresentationLearning:AReview,‚ÄùIEEEJSTSP,vol.16,no.6,pp. [40] A. M. Ikotun, A. E. Ezugwu, L. Abualigah et al., ‚ÄúK-Means Clus-\n1179‚Äì1210,2022. tering Algorithms: A Comprehensive Review, Variants Analysis, and\n[13] X. Zhang, D. Zhang, S. Li et al., ‚ÄúSpeechTokenizer: Unified Speech AdvancesintheEraofBigData,‚ÄùInformationSciences,vol.622,pp.\nTokenizerforSpeechLanguageModels,‚ÄùinProc.ICLR,2024. 178‚Äì210,2023.\n[14] A. De¬¥fossez, L. Mazare¬¥, M. Orsini et al., ‚ÄúMoshi: A Speech- [41] D. Arthur and S. Vassilvitskii, ‚ÄúK-Means++: The Advantages of\nText Foundation Model for Real-Time Dialogue,‚Äù arXiv preprint CarefulSeeding,‚ÄùinProc.SODA. SIAM,2007,pp.1027‚Äì1035.\narXiv:2410.00037,2024.\n[42] C.J.Cho,A.Mohamed,S.-W.Lietal.,‚ÄúSD-HuBERT:Sentence-Level\n[15] E.Kharitonov,D.Vincent,Z.Borsosetal.,‚ÄúSpeak,ReadandPrompt: Self-DistillationInducesSyllabicOrganizationinHuBERT,‚ÄùinProc.\nHigh-FidelityText-to-SpeechwithMinimalSupervision,‚ÄùTrans.ACL,\nIEEEICASSP,2024,pp.12076‚Äì12080.\nvol.11,pp.1703‚Äì1718,2023.\n[43] A. Baade, P. Peng, and D. Harwath, ‚ÄúSyllableLM: Learning Coarse\n[16] Y. Yang, F. Shen, C. Du et al., ‚ÄúTowards Universal Speech Discrete\nSemanticUnitsforSpeechLanguageModels,‚ÄùinProc.ICLR,2025.\nTokens: A Case Study for ASR and TTS,‚Äù in Proc. IEEE ICASSP,\n[44] R. Gray, ‚ÄúVector Quantization,‚Äù IEEE Assp Magazine, vol. 1, no. 2,\n2024,pp.10401‚Äì10405.\npp.4‚Äì29,1984.\n[17] H.Yang,I.Jang,andM.Kim,‚ÄúGenerativeDe-QuantizationforNeural\n[45] A.VanDenOord,O.Vinyalsetal.,‚ÄúNeuralDiscreteRepresentation\nSpeechCodecViaLatentDiffusion,‚ÄùinProc.IEEEICASSP,2024,pp.\nLearning,‚ÄùProc.NeurIPS,vol.30,2017.\n1251‚Äì1255.\n[46] Y.Bengio,N.Le¬¥onard,andA.Courville,‚ÄúEstimatingorPropagating\n[18] H.Liu,X.Xu,Y.Yuanetal.,‚ÄúSemantiCodec:AnUltraLowBitrate\nGradientsThroughStochasticNeuronsforConditionalComputation,‚Äù\nSemantic Audio Codec for General Sound,‚Äù IEEE JSTSP, pp. 1‚Äì14,\narXivpreprintarXiv:1308.3432,2013.\n2024.\n[19] R. Kumar, P. Seetharaman, A. Luebs et al., ‚ÄúHigh-Fidelity Audio [47] A. Razavi, A. Van den Oord, and O. Vinyals, ‚ÄúGenerating Diverse\nCompressionwithImprovedRVQGAN,‚ÄùProc.NeurIPS,vol.36,2024. High-FidelityImageswithVQ-VAE-2,‚ÄùProc.NeurIPS,vol.32,2019.\n[20] H. Wu, N. Kanda, S. E. Eskimez et al., ‚ÄúTS3-Codec: [48] A. ≈Åan¬¥cucki, J. Chorowski, G. Sanchez et al., ‚ÄúRobust Training of\nTransformer-Based Simple Streaming Single Codec,‚Äù arXiv preprint VectorQuantizedBottleneckModels,‚ÄùinProc.IJCNN,2020,pp.1‚Äì7.\narXiv:2411.18803,2024. [49] P. Dhariwal, H. Jun, C. Payne et al., ‚ÄúJukebox: A Generative Model\n[21] X.Jiang,X.Peng,Y.Zhangetal.,‚ÄúDisentangledFeatureLearningfor forMusic,‚ÄùarXivpreprintarXiv:2005.00341,2020.\nReal-TimeNeuralSpeechCoding,‚ÄùinProc.IEEEICASSP,2023. [50] H. Chang, H. Zhang, L. Jiang et al., ‚ÄúMaskGIT: Masked Generative\n[22] Y. Ren, T. Wang, J. Yi et al., ‚ÄúFewer-Token Neural Speech Codec ImageTransformer,‚ÄùinProc.CVPR,2022,pp.11315‚Äì11325.\nwithTime-InvariantCodes,‚ÄùinProc.IEEEICASSP,2024,pp.12737‚Äì [51] L.Yu,J.Lezama,N.B.Gundavarapuetal.,‚ÄúLanguageModelBeats\n12741. Diffusion:TokenizerIsKeytoVisualGeneration,‚ÄùinProc.ICLR,2024.\n[23] H.Siuzdak,F.Gro¬®tschla,andL.A.Lanzendo¬®rfer,‚ÄúSNAC:Multi-scale [52] J.Yu,X.Li,J.Y.Kohetal.,‚ÄúVector-QuantizedImageModelingwith\nneuralaudiocodec,‚ÄùinNeurIPS2024Workshop,2024. ImprovedVQGAN,‚ÄùinProc.ICLR,2022.\n\n19\n[53] Y. Zhu, B. Li, Y. Xin et al., ‚ÄúAddressing Representation Collapse in [81] Y.Lipman,R.T.Chen,H.Ben-Hamu,M.Nickel,andM.Le,‚ÄúFlow\nVectorQuantizedModelswithOneLinearLayer,‚ÄùinProc.IEEE/CVF matchingforgenerativemodeling,‚ÄùinProc.ICLR,2023.\nICCV,2025. [82] Y.Ai,X.Jiang,Y.Lu,H.Du,andZ.Ling,‚ÄúAPCodec:ANeuralAudio\n[54] E. Jang, S. Gu, and B. Poole, ‚ÄúCategorical Reparameterization with Codec With Parallel Amplitude and Phase Spectrum Encoding and\nGumbel-Softmax,‚ÄùinProc.ICLR,2017. Decoding,‚ÄùIEEE/ACMTrans.ASLP.,vol.32,pp.3256‚Äì3269,2024.\n[55] X. Jiang, X. Peng, H. Xue et al., ‚ÄúLatent-Domain Predictive Neural [83] Y.Ai,Y.-X.Lu,X.-H.Jiangetal.,‚ÄúAlow-bitrateneuralaudiocodec\nSpeech Coding,‚Äù IEEE/ACM Trans. ASLP., vol. 31, pp. 2111‚Äì2123, frameworkwithbandwidthreductionandrecoveryforhigh-sampling-\n2023. ratewaveforms,‚ÄùinProc.ISCAInterspeech,2024,pp.1765‚Äì1769.\n[56] F.Mentzer,D.Minnen,E.Agustssonetal.,‚ÄúFiniteScalarQuantization: [84] S. Ahn, B. J. Woo, M. H. Han et al., ‚ÄúHILCodec: High-Fidelity and\nVQ-VAEMadeSimple,‚ÄùinProc.ICLR,2024. LightweightNeuralAudioCodec,‚ÄùIEEEJSTSP,pp.1‚Äì14,2024.\n[57] H.Je¬¥gou,M.Douze,andC.Schmid,‚ÄúProductQuantizationforNearest [85] Y.Zheng,W.Tu,L.Xiaoetal.,‚ÄúSuperCodec:ANeuralSpeechCodec\nNeighborSearch,‚ÄùIEEETPAMI,vol.33,no.1,pp.117‚Äì128,2011. withSelectiveBack-ProjectionNetwork,‚ÄùinProc.IEEEICASSP,2024,\n[58] B.-H. Juang and A. Gray, ‚ÄúMultiple Stage Vector Quantization for pp.566‚Äì570.\nSpeechCoding,‚ÄùinProc.IEEEICASSP,vol.7,1982,pp.597‚Äì600. [86] H.-P.Du,Y.Ai,R.-C.Zhengetal.,‚ÄúAPCodec+:ASpectrum-Coding-\n[59] D. Yang, S. Liu, R. Huang et al., ‚ÄúHiFi-Codec: Group-Residual BasedHigh-FidelityandHigh-Compression-RateNeuralAudioCodec\nVector Quantization for High Fidelity Audio Codec,‚Äù arXiv preprint withStagedTrainingParadigm,‚ÄùinProc.ISCSLP,2024.\narXiv:2305.02765,2023. [87] R.-C.Zheng,H.-P.Du,X.-H.Jiangetal.,‚ÄúERVQ:Enhancedresidual\n[60] J. D. Parker, A. Smirnov, J. Pons et al., ‚ÄúScaling Transformers for vectorquantizationwithintra-and-inter-codebookoptimizationforneu-\nLow-BitrateHigh-QualitySpeechCoding,‚ÄùinProc.ICLR,2025. ral audio codecs,‚Äù IEEE/ACM Trans. ASLP., vol. 33, pp. 2539‚Äì2550,\n[61] O. Rippel, M. Gelbart, and R. Adams, ‚ÄúLearning Ordered Represen- 2025.\ntations with Nested Dropout,‚Äù in Proc. ICML. PMLR, 2014, pp. [88] Z.Niu,S.Chen,L.Zhouetal.,‚ÄúNDVQ:RobustNeuralAudioCodec\n1746‚Äì1754. WithNormalDistribution-BasedVectorQuantization,‚ÄùinProc.IEEE\n[62] R. Shah, M. Yan, M. C. Mozer, and D. Liu, ‚ÄúImproving discrete\nSLT,2024,pp.705‚Äì710.\noptimisation via decoupled straight-through gumbel-softmax,‚Äù arXiv [89] D. Yang, D. Wang, H. Guo et al., ‚ÄúSimpleSpeech: Towards Simple\npreprintarXiv:2410.13331,2024. andEfficientText-to-SpeechwithScalarLatentTransformerDiffusion\nModels,‚ÄùinProc.ISCAInterspeech,2024,pp.4398‚Äì4402.\n[63] D. R. Finlayson, ‚ÄúA More Loss-Tolerant RTP Payload Format\n[90] S.Ji,Z.Jiang,W.Wangetal.,‚ÄúWavTokenizer:anEfficientAcoustic\nfor MP3 Audio,‚Äù RFC 5219, Feb. 2008. [Online]. Available:\nDiscrete Codec Tokenizer for Audio Language Modeling,‚Äù in Proc.\nhttps://www.rfc-editor.org/info/rfc5219\nICLR,2025.\n[64] J.-M. Valin, K. Vos, and T. B. Terriberry, ‚ÄúDefinition of the Opus\n[91] D.Xin,X.Tan,S.Takamichietal.,‚ÄúBigCodec:PushingtheLimitsof\nAudioCodec,‚ÄùRFC,vol.6716,pp.1‚Äì326,2012.[Online].Available:\nLow-BitrateNeuralSpeechCodec,‚ÄùarXivpreprintarXiv:2409.05377,\nhttps://api.semanticscholar.org/CorpusID:30715761\n2024.\n[65] M. Dietz, M. Multrus, V. Eksler et al., ‚ÄúOverview of the EVS codec\n[92] H.Guo,F.Xie,K.Xieetal.,‚ÄúSoCodec:ASemantic-OrderedMulti-\narchitecture,‚ÄùinProc.IEEEICASSP,2015,pp.5698‚Äì5702.\nStream Speech Codec for Efficient Language Model Based Text-to-\n[66] P.Esser,R.Rombach,andB.Ommer,‚ÄúTamingTransformersforHigh-\nSpeechSynthesis,‚ÄùinProc.IEEESLT,2024.\nResolution Image Synthesis,‚Äù in Proc. IEEE/CVF ICCV, 2021, pp.\n[93] X.Wang,M.Jiang,Z.Maetal.,‚ÄúSpark-TTS:AnefficientLLM-based\n12873‚Äì12883.\ntext-to-speech model with single-stream decoupled speech tokens,‚Äù\n[67] K. Kumar, R. Kumar, T. De Boissiere et al., ‚ÄúMelGAN: Generative\narXivpreprintarXiv:2503.01710,2025.\nAdversarial Networks for Conditional Waveform Synthesis,‚Äù Proc.\n[94] Y. Zheng, W. Tu, Y. Kang et al., ‚ÄúFreeCodec: A disentangled neural\nNeurIPS,vol.32,2019.\nspeechcodecwithfewertokens,‚ÄùinProc.ISCAInterspeech,2025,pp.\n[68] W. Jang, D. Lim, J. Yoon et al., ‚ÄúUnivNet: A Neural Vocoder\n4878‚Äì4882.\nwith Multi-Resolution Spectrogram Discriminators for High-Fidelity\n[95] Y. Ren, M. Lei, Z. Huang et al., ‚ÄúProsoSpeech: Enhancing Prosody\nWaveform Generation,‚Äù in Proc. ISCA Interspeech, 2021, pp. 2207‚Äì\nwithQuantizedVectorPre-TraininginText-to-Speech,‚ÄùinProc.IEEE\n2211.\nICASSP,2022,pp.7577‚Äì7581.\n[69] N. Zeghidour, A. Luebs, A. Omran et al., ‚ÄúSoundStream: An End-\n[96] T. Jenrungrot, M. Chinen, W. B. Kleijn et al., ‚ÄúLMCodec: A Low\nto-End Neural Audio Codec,‚Äù IEEE/ACM Trans. ASLP., vol. 30, pp.\nBitrateSpeechCodecwithCausalTransformerModels,‚ÄùinProc.IEEE\n495‚Äì507,2021.\nICASSP,2023.\n[70] A.De¬¥fossez,J.Copet,G.Synnaeveetal.,‚ÄúHighFidelityNeuralAudio\n[97] C. Raffel, N. Shazeer, A. Roberts et al., ‚ÄúExploring the Limits of\nCompression,‚ÄùTMLR,2023.\nTransfer Learning with a Unified Text-to-Text Transformer,‚Äù JMLR,\n[71] Z. Du, S. Zhang, K. Hu et al., ‚ÄúFunCodec: A Fundamental, Repro- vol.21,no.140,pp.1‚Äì67,2020.\nducibleandIntegrableOpen-SourceToolkitforNeuralSpeechCodec,‚Äù [98] C. Qiang, H. Wang, C. Gong et al., ‚ÄúSecoustiCodec: Cross-modal\ninProc.IEEEICASSP,2024,pp.591‚Äì595. aligned streaming single-codebook speech codec,‚Äù arXiv preprint\n[72] H.Li,L.Xue,H.Guoetal.,‚ÄúSingle-Codec:Single-CodebookSpeech arXiv:2508.02849,2025.\nCodectowardsHigh-PerformanceSpeechGeneration,‚ÄùinProc.ISCA [99] H.Touvron,L.Martin,K.Stoneetal.,‚ÄúLLaMa2:OpenFoundation\nInterspeech,2024,pp.3390‚Äì3394. andFine-TunedChatModels,‚ÄùarXivpreprintarXiv:2307.09288,2023.\n[73] A. Gulati, J. Qin, C.-C. Chiu et al., ‚ÄúConformer: Convolution- [100] A.Siahkoohi,M.Chinen,T.Dentonetal.,‚ÄúUltra-Low-BitrateSpeech\naugmented Transformer for Speech Recognition,‚Äù in Proc. ISCA In- Coding with Pretrained Transformers,‚Äù in Proc. ISCA Interspeech,\nterspeech,2020,pp.5036‚Äì5040. 2022,pp.4421‚Äì4425.\n[74] H. Guo, F. Xie, D. Yang et al., ‚ÄúSpeaking from coarse to fine: [101] P.-Y.Huang,H.Xu,J.Lietal.,‚ÄúMaskedAutoencodersThatListen,‚Äù\nImprovingneuralcodeclanguagemodelviamulti-scalespeechcoding Proc.NeurIPS,vol.35,pp.28708‚Äì28720,2022.\nandgeneration,‚ÄùinProc.IEEEICASSP,2025. [102] Z. Ye, X. Zhu, C.-M. Chan et al., ‚ÄúLlasa: Scaling Train-Time and\n[75] Y.GuandD.Enmao,‚ÄúESC:EfficientSpeechCodingwithCross-Scale Inference-Time Compute for Llama-based Speech Synthesis,‚Äù arXiv\nResidualVectorQuantizedTransformers,‚ÄùinProc.EMNLP,2024. preprintarXiv:2502.04128,2025.\n[76] Y.-C. Wu, I. D. Gebru, D. Markovic¬¥ et al., ‚ÄúAudioDec: An Open- [103] L. Barrault, Y.-A. Chung, M. C. Meglioli et al., ‚ÄúSeamless: Multi-\nSourceStreamingHigh-FidelityNeuralAudioCodec,‚ÄùinProc.IEEE lingualExpressiveandStreamingSpeechTranslation,‚ÄùarXivpreprint\nICASSP,2023. arXiv:2312.05187,2023.\n[77] R. San Roman, Y. Adi, A. Deleforge et al., ‚ÄúFrom Discrete Tokens [104] Y. Gong, L. Jin, R. Deng et al., ‚ÄúXY-Tokenizer: Mitigating the\nto High-Fidelity Audio Using Multi-Band Diffusion,‚Äù Proc. NeurIPS, semantic-acousticconflictinlow-bitratespeechcodecs,‚ÄùarXivpreprint\nvol.36,pp.1526‚Äì1538,2023. arXiv:2506.23325,2025.\n[78] H. Siuzdak, ‚ÄúVocos: Closing the Gap Between Time-Domain and [105] Y. Ganin and V. Lempitsky, ‚ÄúUnsupervised Domain Adaptation by\nFourier-BasedNeuralVocodersforHigh-QualityAudioSynthesis,‚Äùin Backpropagation,‚ÄùinProc.ICML. PMLR,2015,pp.1180‚Äì1189.\nProc.ICLR,2024. [106] Z.Wang,A.C.Bovik,H.R.Sheikhetal.,‚ÄúImageQualityAssessment:\n[79] J. Ho, A. Jain, and P. Abbeel, ‚ÄúDenoising Diffusion Probabilistic FromErrorVisibilitytoStructuralSimilarity,‚ÄùIEEETrans.onImage\nModels,‚ÄùProc.NeurIPS,vol.33,pp.6840‚Äì6851,2020. Processing,vol.13,no.4,pp.600‚Äì612,2004.\n[80] Y. Song, J. Sohl-Dickstein, D. P. Kingma et al., ‚ÄúScore-Based Gen- [107] K.Qian,Y.Zhang,S.Changetal.,‚ÄúAutoVC:Zero-ShotVoiceStyle\nerative Modeling through Stochastic Differential Equations,‚Äù in Proc. TransferwithOnlyAutoencoderLoss,‚ÄùinProc.ICML. PMLR,2019,\nICLR,2021. pp.5210‚Äì5219.\n\n20\n[108] C. Du, Y. Guo, F. Shen et al., ‚ÄúUniCATS: A Unified Context- [134] C.-C. Chiu, J. Qin, Y. Zhang et al., ‚ÄúSelf-Supervised Learning with\nAware Text-to-Speech Framework with Contextual VQ-Diffusion and Random-ProjectionQuantizerforSpeechRecognition,‚ÄùinProc.ICML.\nVocoding,‚ÄùinProc.AAAI,vol.38,no.16,2024,pp.17924‚Äì17932. PMLR,2022,pp.3915‚Äì3924.\n[109] J.Li,Y.Guo,X.Chenetal.,‚ÄúSEF-VC:SpeakerEmbeddingFreeZero- [135] H.Zhu,Y.Zhou,H.Chenetal.,‚ÄúMuQ:Self-SupervisedMusicRep-\nShotVoiceConversionwithCrossAttention,‚ÄùinProc.IEEEICASSP, resentation Learning with Mel Residual Vector Quantization,‚Äù arXiv\n2024,pp.12296‚Äì12300. preprintarXiv:2501.01108,2025.\n[110] K. Choi, A. Pasad, T. Nakamura et al., ‚ÄúSelf-Supervised Speech [136] A. Conneau, A. Baevski, R. Collobert et al., ‚ÄúUnsupervised Cross-\nRepresentations are More Phonetic than Semantic,‚Äù in Proc. ISCA Lingual Representation Learning for Speech Recognition,‚Äù in Proc.\nInterspeech,2024,pp.4578‚Äì4582. ISCAInterspeech,2021,pp.2426‚Äì2430.\n[111] D. Wells, H. Tang, and K. Richmond, ‚ÄúPhonetic Analysis of Self- [137] V. Pratap, A. Tjandra, B. Shi et al., ‚ÄúScaling Speech Technology to\nsupervised Representations of English Speech,‚Äù in Proc. ISCA Inter- 1,000+Languages,‚ÄùJMLR,vol.25,no.97,pp.1‚Äì52,2024.\nspeech,2022,pp.3583‚Äì3587. [138] A. Turetzky and Y. Adi, ‚ÄúLAST: Language Model Aware Speech\n[112] A.SichermanandY.Adi,‚ÄúAnalysingDiscreteSelf-SupervisedSpeech Tokenization,‚ÄùarXivpreprintarXiv:2409.03701,2024.\nRepresentation for Spoken Language Modelling,‚Äù in Proc. IEEE [139] M. Han, Y. Bai, C. Shen et al., ‚ÄúNEST-RQ: Next Token Pre-\nICASSP,2023. diction for Speech Self-Supervised Pre-Training,‚Äù arXiv preprint\n[113] S.-L. Yeh and H. Tang, ‚ÄúEstimating the Completeness of Discrete arXiv:2409.08680,2024.\nSpeechUnits,‚ÄùinProc.IEEESLT,2024,pp.415‚Äì422. [140] I.Gat,F.Kreuk,T.A.Nguyenetal.,‚ÄúAugmentationInvariantDiscrete\n[114] S. wen Yang, P.-H. Chi, Y.-S. Chuang et al., ‚ÄúSUPERB: Speech Representation for Generative Spoken Language Modeling,‚Äù in Proc.\nProcessingUniversalPERformanceBenchmark,‚ÄùinProc.ISCAInter- IWSLT@ACL,2023,pp.465‚Äì477.\nspeech,2021,pp.1194‚Äì1198. [141] V. S. Lodagala, S. Ghosh, and S. Umesh, ‚ÄúCCC-wav2vec 2.0: Clus-\n[115] S.Schneider,A.Baevski,R.Collobertetal.,‚Äúwav2vec:Unsupervised tering aided Cross Contrastive Self-Supervised Learning of Speech\nPre-TrainingforSpeechRecognition,‚ÄùinProc.ISCAInterspeech,2019, Representations,‚ÄùinProc.IEEESLT,2023,pp.1‚Äì8.\npp.3465‚Äì3469. [142] W. Huang, Z. Zhang, Y. T. Yeung et al., ‚ÄúSPIRAL: Self-\n[116] Y. Zhang, J. Qin, D. S. Park et al., ‚ÄúPushing the Limits of supervised Perturbation-Invariant Representation Learning for Speech\nSemi-SupervisedLearningforAutomaticSpeechRecognition,‚ÄùarXiv Pre-Training,‚ÄùinProc.ICLR,2022.\npreprintarXiv:2010.10504,2020. [143] H.-J.Chang,A.H.Liu,andJ.Glass,‚ÄúSelf-SupervisedFine-Tuningfor\nImprovedContentRepresentationsbySpeaker-InvariantClustering,‚Äùin\n[117] J. weon Jung, W. Zhang, J. Shi et al., ‚ÄúESPnet-SPK: Full\nProc.ISCAInterspeech,2023,pp.2983‚Äì2987.\nPipelineSpeakerEmbeddingToolkitwithReproducibleRecipes,Self-\n[144] H.-J.Chang,H.Gong,C.Wangetal.,‚ÄúDC-Spin:ASpeaker-invariant\nSupervised Front-Ends, and Off-the-Shelf Models,‚Äù in Proc. ISCA\nSpeech Tokenizer for Spoken Language Models,‚Äù arXiv preprint\nInterspeech,2024,pp.4278‚Äì4282.\narXiv:2410.24177,2024.\n[118] V.Miara,T.Lepage,andR.Dehak,‚ÄúTowardsSupervisedPerformance\n[145] I. Hwang and K. Lee, ‚ÄúRemoving Speaker Information from Speech\nonSpeakerVerificationwithSelf-SupervisedLearningbyLeveraging\nRepresentation using Variable-Length Soft Pooling,‚Äù arXiv preprint\nLarge-ScaleASRModels,‚ÄùinProc.ISCAInterspeech,2024,pp.2660‚Äì\narXiv:2404.00856,2024.\n2664.\n[146] A.Polyak,Y.Adi,J.Copetetal.,‚ÄúSpeechResynthesisfromDiscrete\n[119] E. Morais, R. Hoory, W. Zhu et al., ‚ÄúSpeech Emotion Recognition\nDisentangled Self-Supervised Representations,‚Äù in Proc. ISCA Inter-\nUsing Self-Supervised Features,‚Äù in Proc. IEEE ICASSP, 2022, pp.\nspeech,2021,pp.3615‚Äì3619.\n6922‚Äì6926.\n[147] P. Mousavi, L. Della Libera, J. Duret et al., ‚ÄúDASB‚ÄìDiscrete Audio\n[120] S.Madanian,T.Chen,O.Adeleyeetal.,‚ÄúSpeechEmotionRecognition\nandSpeechBenchmark,‚ÄùarXivpreprintarXiv:2406.14294,2024.\nUsingMachineLearning-ASystematicReview,‚ÄùIntelligentSystems\n[148] A.Zeng,Z.Du,M.Liuetal.,‚ÄúScalingspeech-textpre-trainingwith\nwithApplications,vol.20,p.200266,2023.\nsyntheticinterleaveddata,‚ÄùinProc.ICLR,2025.\n[121] Z. Ma, Z. Zheng, J. Ye et al., ‚Äúemotion2vec: Self-Supervised Pre-\n[149] ‚Äî‚Äî,‚ÄúGLM-4-Voice:TowardsIntelligentandHuman-LikeEnd-to-End\nTrainingforSpeechEmotionRepresentation,‚ÄùinFindingsofACL,Aug.\nSpokenChatbot,‚ÄùarXivpreprintarXiv:2412.02612,2024.\n2024,pp.15747‚Äì15760.\n[150] Z. Du, Y. Wang, Q. Chen et al., ‚ÄúCosyVoice 2: Scalable Stream-\n[122] A. Wu, C. Wang, J. Pino et al., ‚ÄúSelf-Supervised Representations\ning Speech Synthesis with Large Language Models,‚Äù arXiv preprint\nImprove End-to-End Speech Translation,‚Äù in Proc. ISCA Interspeech,\narXiv:2412.10117,2024.\n2020,pp.1491‚Äì1495.\n[151] Z. Du, C. Gao, Y. Wang et al., ‚ÄúCosyVoice 3: Towards in-the-wild\n[123] H. Nguyen, F. Bougares, N. Tomashenko et al., ‚ÄúInvestigating Self-\nspeech generation via scaling-up and post-training,‚Äù arXiv preprint\nSupervisedPre-TrainingforEnd-to-EndSpeechTranslation,‚ÄùinProc.\narXiv:2505.17589,2025.\nISCAInterspeech,2020,pp.1466‚Äì1470.\n[152] Q. Fang, S. Guo, Y. Zhou et al., ‚ÄúLLaMA-Omni: Seamless\n[124] A.Babu,C.Wang,A.Tjandraetal.,‚ÄúXLS-R:Self-SupervisedCross-\nSpeech Interaction with Large Language Models,‚Äù arXiv preprint\nLingual Speech Representation Learning at Scale,‚Äù in Proc. ISCA\narXiv:2409.06666,2024.\nInterspeech,2022,pp.2278‚Äì2282.\n[153] J. Kong, J. Kim, and J. Bae, ‚ÄúHifi-GAN: Generative Adversarial\n[125] X.Chang,B.Yan,Y.Fujitaetal.,‚ÄúExplorationofEfficientEnd-to-End Networks for Efficient and High Fidelity Speech Synthesis,‚Äù Proc.\nASRusingDiscretizedInputfromSelf-SupervisedLearning,‚ÄùinProc. NeurIPS,vol.33,pp.17022‚Äì17033,2020.\nISCAInterspeech,2023,pp.1399‚Äì1403. [154] Y.Guo,Z.Li,J.Lietal.,‚Äúvec2wav2.0:AdvancingVoiceConversion\n[126] C.Du,Y.Guo,X.Chenetal.,‚ÄúVQTTS:High-FidelityText-to-Speech viaDiscreteTokenVocoders,‚ÄùarXivpreprintarXiv:2409.01995,2024.\nSynthesis with Self-Supervised VQ Acoustic Feature,‚Äù in Proc. ISCA [155] J. Betker, ‚ÄúBetter Speech Synthesis through Scaling,‚Äù arXiv preprint\nInterspeech,2022,pp.1596‚Äì1600. arXiv:2305.07243,2023.\n[127] X.Zhu,Y.Lv,Y.Leietal.,‚ÄúVec-TokSpeech:Speechvectorizationand [156] P. Anastassiou, J. Chen, J. Chen et al., ‚ÄúSeed-TTS: A Family of\ntokenization for neural speech generation,‚Äù IEEE/ACM Trans. ASLP., High-Quality Versatile Speech Generation Models,‚Äù arXiv preprint\nvol.33,pp.1243‚Äì1254,2025. arXiv:2406.02430,2024.\n[128] M.Hassid,T.Remez,T.A.Nguyenetal.,‚ÄúTextuallyPretrainedSpeech [157] H.Wang,H.Wang,Y.Guoetal.,‚ÄúWhyDoSpeechLanguageModels\nLanguageModels,‚ÄùProc.NeurIPS,vol.36,2024. FailtoGenerateSemanticallyCoherentOutputs?AModalityEvolving\n[129] E. Kharitonov, A. Lee, A. Polyak et al., ‚ÄúText-Free Prosody-Aware Perspective,‚ÄùarXivpreprintarXiv:2412.17048,2024.\nGenerative Spoken Language Modeling,‚Äù in Proc. ACL, May 2022, [158] X. Chang, B. Yan, K. Choi et al., ‚ÄúExploring Speech Recognition,\npp.8666‚Äì8681. Translation, and Understanding with Discrete Speech Units: A Com-\n[130] J.Shi,X.Ma,H.Inagumaetal.,‚ÄúMMM:Multi-LayerMulti-Residual parativeStudy,‚ÄùinProc.IEEEICASSP,2024,pp.11481‚Äì11485.\nMulti-Stream Discrete Speech Representation from Self-supervised [159] T.HayashiandS.Watanabe,‚ÄúDiscreTalk:Text-to-SpeechasaMachine\nLearningModel,‚ÄùinProc.ISCAInterspeech,2024,pp.2569‚Äì2573. TranslationProblem,‚ÄùarXivpreprintarXiv:2005.05525,2020.\n[131] P. Mousavi, J. Duret, S. Zaiem et al., ‚ÄúHow Should We Extract [160] S.Ren,S.Liu,Y.Wuetal.,‚ÄúSpeechPre-trainingwithAcousticPiece,‚Äù\nDiscreteAudioTokensfromSelf-SupervisedModels?‚ÄùinProc.ISCA inProc.ISCAInterspeech,2022,pp.2648‚Äì2652.\nInterspeech,2024,pp.2554‚Äì2558. [161] F. Shen, Y. Guo, C. Du et al., ‚ÄúAcoustic BPE for speech generation\n[132] Z.Huang,C.Meng,andT.Ko,‚ÄúRepCodec:ASpeechRepresentation withdiscretetokens,‚ÄùinProc.IEEEICASSP,2024,pp.11746‚Äì11750.\nCodecforSpeechTokenization,‚ÄùinProc.ACL,2024,pp.5777‚Äì5790. [162] A.DekelandR.Fernandez,‚ÄúExploringtheBenefitsofTokenizationof\n[133] Y.Wang,H.Zhan,L.Liuetal.,‚ÄúMaskGCT:Zero-ShotText-to-Speech DiscreteAcousticUnits,‚ÄùinProc.ISCAInterspeech,2024,pp.2780‚Äì\nwithMaskedGenerativeCodecTransformer,‚ÄùinProc.ICLR,2025. 2784.\n\n21\n[163] P. Gage, ‚ÄúA New Algorithm for Data Compression,‚Äù The C Users [190] P. O‚ÄôReilly, P. Seetharaman, J. Su et al., ‚ÄúCode drift: Towards idem-\nJournal archive, vol. 12, pp. 23‚Äì38, 1994. [Online]. Available: potentneuralaudiocodecs,‚ÄùinProc.IEEEICASSP,2025.\nhttps://api.semanticscholar.org/CorpusID:59804030 [191] H. Wang, G. Chen, B. Li et al., ‚ÄúTowards general discrete speech\n[164] B.Li,F.Shen,Y.Guoetal.,‚ÄúOntheEffectivenessofAcousticBPEin codec for complex acoustic environments: A study of reconstruction\nDecoder-OnlyTTS,‚ÄùinProc.ISCAInterspeech,2024,pp.4134‚Äì4138. anddownstreamtaskconsistency,‚ÄùinProc.IEEEASRU,2025.\n[165] S.Dieleman,C.Nash,J.Engeletal.,‚ÄúVariable-RateDiscreteRepre- [192] H.Zen,V.Dang,R.Clarketal.,‚ÄúLibriTTS:ACorpusDerivedfrom\nsentationLearning,‚ÄùarXivpreprintarXiv:2103.06089,2021. LibriSpeechforText-to-Speech,‚ÄùinProc.ISCAInterspeech,2019,pp.\n[166] R.Eloff,A.Nortje,B.vanNiekerketal.,‚ÄúUnsupervisedAcousticUnit 1526‚Äì1530.\nDiscoveryforSpeechSynthesisUsingDiscreteLatent-VariableNeural [193] E. Casanova, J. Weber, C. D. Shulby et al., ‚ÄúYourTTS: Towards\nNetworks,‚ÄùinProc.ISCAInterspeech,2019,pp.1103‚Äì1107. Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for\n[167] E.Dunbar,J.Karadayi,M.Bernardetal.,‚ÄúTheZeroResourceSpeech Everyone,‚ÄùinProc.ICML. PMLR,2022,pp.2709‚Äì2720.\nChallenge 2020: Discovering Discrete Subword and Word Units,‚Äù in [194] E. Bastianelli, A. Vanzo, P. Swietojanski, and V. Rieser, ‚ÄúSLURP: A\nProc.ISCAInterspeech,2020,pp.4831‚Äì4835. spoken language understanding resource package,‚Äù in Proc. EMNLP.\n[168] B.vanNiekerk,L.Nortje,andH.Kamper,‚ÄúVector-QuantizedNeural AssociationforComputationalLinguistics,Nov.2020,pp.7252‚Äì7262.\nNetworksforAcousticUnitDiscoveryintheZeroSpeech2020Chal- [195] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, ‚ÄúLibriSpeech:\nlenge,‚ÄùinProc.ISCAInterspeech,2020,pp.4836‚Äì4840. an asr corpus based on public domain audio books,‚Äù in Proc. IEEE\n[169] T. A. Nguyen, M. de Seyssel, P. Roze¬¥ et al., ‚ÄúThe Zero Resource ICASSP. IEEE,2015,pp.5206‚Äì5210.\nSpeech Benchmark 2021: Metrics and Baselines for Unsupervised [196] D. Zhang, R. Ye, T. Ko et al., ‚ÄúDUB: Discrete Unit Back-translation\nSpokenLanguageModelling,‚ÄùinNeurIPSWorkshop,2020. forSpeechTranslation,‚ÄùinFindingsofACL,Jul.2023,pp.7147‚Äì7164.\n[170] C. J. Cho, N. Lee, A. Gupta et al., ‚ÄúSylber: Syllabic Embedding [197] A.Tjandra,S.Sakti,andS.Nakamura,‚ÄúSpeech-to-SpeechTranslation\nRepresentationofSpeechfromRawAudio,‚ÄùinProc.ICLR,2025. Between Untranscribed Unknown Languages,‚Äù in Proc. IEEE ASRU,\n[171] S.Cuervo,A.Lancucki,R.Marxeretal.,‚ÄúVariable-RateHierarchical 2019,pp.593‚Äì600.\nCPC Leads to Acoustic Unit Discovery in Speech,‚Äù Proc. NeurIPS, [198] C. Zhang, X. Tan, Y. Ren et al., ‚ÄúUWSpeech: Speech-to-Speech\nvol.35,pp.34995‚Äì35006,2022. TranslationforUnwrittenLanguages,‚ÄùinProc.AAAI,vol.35,no.16,\n[172] H.Zhang,Y.Guo,Z.Lietal.,‚ÄúUnlockingTemporalFlexibility:Neural 2021,pp.14319‚Äì14327.\nSpeechCodecwithVariableFrameRate,‚ÄùinProc.ISCAInterspeech,\n[199] A.Lee,P.-J.Chen,C.Wangetal.,‚ÄúDirectSpeech-to-SpeechTransla-\n2025,pp.5003‚Äì5007. tionWithDiscreteUnits,‚ÄùinProc.ACL,2022,pp.3327‚Äì3339.\n[173] H.Wang,Y.Guo,C.Shaoetal.,‚ÄúCodecSlime:Temporalredundancy\n[200] A. Lee, H. Gong, P.-A. Duquenne et al., ‚ÄúTextless Speech-to-Speech\ncompression of neural speech codec via dynamic frame rate,‚Äù arXiv\nTranslationonRealData,‚ÄùinProc.NAACL,Jul.2022,pp.860‚Äì872.\npreprintarXiv:2506.21074,2025.\n[201] H.Wu,K.-W.Chang,Y.-K.Wu,andH.-y.Lee,‚ÄúSpeechGen:Unlocking\n[174] S.Karapiperis,N.Ellinas,A.Vionietal.,‚ÄúInvestigatingdisentangle-\nthe Generative Power of Speech Language Models with Prompts,‚Äù\nmentinaphoneme-levelspeechcodecforprosodymodeling,‚ÄùinProc.\narXivpreprintarXiv:2306.02207,2023.\nIEEESLT,2024,pp.668‚Äì674.\n[202] Y. Peng, I. Kulikov, Y. Yang et al., ‚ÄúMSLM-S2ST: A Multitask\n[175] L.-H. Tseng, Y.-C. Chen, K.-Y. Lee, D.-S. Shiu, and H.-y. Lee,\nSpeech Language Model for Textless Speech-to-Speech Translation\n‚ÄúTASTE:Text-alignedspeechtokenizationandembeddingforspoken\nwith Speaker Style Preservation,‚Äù arXiv preprint arXiv:2403.12408,\nlanguagemodeling,‚ÄùarXivpreprintarXiv:2504.07053,2025.\n2024.\n[176] A.W.Rix,J.G.Beerends,M.P.Hollier,andA.P.Hekstra,‚ÄúPerceptual\n[203] Y.Wang,B.Jionghao,R.Huangetal.,‚ÄúSpeech-to-SpeechTranslation\nevaluationofspeechquality(PESQ)-anewmethodforspeechquality\nwithDiscrete-Unit-BasedStyleTransfer,‚ÄùinProc.ACL(Vol4:Student\nassessmentoftelephonenetworksandcodecs,‚ÄùinProc.IEEEICASSP,\nResearchWorkshop),Aug.2024,pp.34‚Äì41.\nvol.2,2001,pp.749‚Äì752.\n[204] H. Gong and B. Veluri, ‚ÄúSeamlessExpressiveLM: Speech Language\n[177] C.H.Taal,R.C.Hendriks,R.Heusdens,andJ.Jensen,‚ÄúAnalgorithm\nModel for Expressive Speech-to-Speech Translation with Chain-of-\nforintelligibilitypredictionoftime‚Äìfrequencyweightednoisyspeech,‚Äù\nThought,‚ÄùarXivpreprintarXiv:2405.20410,2024.\nIEEE/ACMTrans.ASLP.,vol.19,no.7,pp.2125‚Äì2136,2011.\n[205] P.-J.Chen,K.Tran,Y.Yangetal.,‚ÄúSpeech-to-SpeechTranslationfor\n[178] P.C.Bagshaw,S.M.Hiller,andM.A.Jack,‚ÄúEnhancedpitchtracking\naReal-WorldUnwrittenLanguage,‚ÄùinFindingsofACL,Jul.2023,pp.\nand the processing of F0 contours for computer aided intonation\n4969‚Äì4983.\nteaching,‚ÄùinEUROSPEECH. ISCA,1993,pp.1003‚Äì1006.\n[206] H. Inaguma, S. Popuri, I. Kulikov et al., ‚ÄúUnitY: Two-pass Direct\n[179] H.Wu,H.-L.Chung,Y.-C.Linetal.,‚ÄúCodec-SUPERB:Anin-depth\nSpeech-to-speechTranslationwithDiscreteUnits,‚ÄùinProc.ACL,Jul.\nanalysisofsoundcodecmodels,‚ÄùinFindingsofACL,Aug.2024,pp.\n2023,pp.15655‚Äì15680.\n10330‚Äì10348.\n[180] J.Shi,J.Tian,Y.Wuetal.,‚ÄúESPnet-Codec:ComprehensiveTraining [207] R. Huang, J. Liu, H. Liu et al., ‚ÄúTranSpeech: Speech-to-Speech\nandEvaluationofNeuralCodecsforAudio,Music,andSpeech,‚ÄùarXiv\nTranslationWithBilateralPerturbation,‚ÄùinProc.ICLR,2023.\npreprintarXiv:2409.15897,2024. [208] K.-W.Chang,W.-C.Tseng,S.-W.Lietal.,‚ÄúAnExplorationofPrompt\n[181] J.Shi,H.-j.Shim,J.Tianetal.,‚ÄúVERSA:Aversatileevaluationtoolkit TuningonGenerativeSpokenLanguageModelforSpeechProcessing\nfor speech, audio, and music,‚Äù in NAACL-HLT 2025, Demo Track. Tasks,‚ÄùinProc.ISCAInterspeech,2022,pp.5005‚Äì5009.\nAssociationforComputationalLinguistics,2025,pp.191‚Äì209. [209] K.-W. Chang, Y.-K. Wang, H. Shen et al., ‚ÄúSpeechPrompt v2:\n[182] S.Vashishth,H.Singh,S.Bharadwajetal.,‚ÄúSTAB:SpeechTokenizer Prompt Tuning for Speech Classification Tasks,‚Äù arXiv preprint\nAssessmentBenchmark,‚ÄùarXivpreprintarXiv:2409.02384,2024. arXiv:2303.00733,2023.\n[183] T.A.Nguyen,B.Sagot,andE.Dupoux,‚ÄúAreDiscreteUnitsNecessary [210] K.-W. Chang, H. Wu, Y.-K. Wang et al., ‚ÄúSpeechPrompt: Prompting\nfor Spoken Language Modelling?‚Äù IEEE JSTSP, vol. 16, no. 6, pp. Speech Language Models for Speech Processing Tasks,‚Äù IEEE/ACM\n1415‚Äì1423,2022. Trans.ASLP.,vol.32,p.3730‚Äì3744,Aug.2024.\n[184] B. M. Abdullah, M. M. Shaik, B. Mo¬®bius et al., ‚ÄúAn Information- [211] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen et al., ‚ÄúAu-\nTheoretic Analysis of Self-supervised Discrete Representations of dioPaLM:ALargeLanguageModelThatCanSpeakandListen,‚ÄùarXiv\nSpeech,‚ÄùinProc.ISCAInterspeech,2023,pp.2883‚Äì2887. preprintarXiv:2306.12925,2023.\n[185] M. Cui, Y. Yang, J. Deng et al., ‚ÄúExploring SSL Discrete [212] Q.Chen,Y.Chu,Z.Gaoetal.,‚ÄúLauraGPT:Listen,Attend,Understand,\nSpeechFeaturesforZipformer-basedContextualASR,‚ÄùarXivpreprint and Regenerate Audio with GPT,‚Äù arXiv preprint arXiv:2310.04673,\narXiv:2409.08797,2024. 2023.\n[186] M. Cui, D. Tan, Y. Yang et al., ‚ÄúExploring SSL Discrete Tokens for [213] K. C. Puvvada, N. R. Koluguri, K. Dhawan et al., ‚ÄúDiscrete Audio\nMultilingualASR,‚ÄùarXivpreprintarXiv:2409.08805,2024. RepresentationasanAlternativetoMel-SpectrogramsforSpeakerand\n[187] D.Wang, M.Cui,D. Yangetal., ‚ÄúAComparativeStudy ofDiscrete SpeechRecognition,‚ÄùinProc.IEEEICASSP,2024,pp.12111‚Äì12115.\nSpeech Tokens for Semantic-Related Tasks with Large Language [214] S.Shon,K.Kim,Y.-T.Hsuetal.,‚ÄúDiscreteSLU:ALargeLanguage\nModels,‚ÄùarXivpreprintarXiv:2411.08742,2024. Model with Self-Supervised Discrete Speech Units for Spoken Lan-\n[188] T.A.Nguyen,W.-N.Hsu,A.D‚ÄôAvirroetal.,‚ÄúExpresso:ABenchmark guageUnderstanding,‚ÄùarXivpreprintarXiv:2406.09345,2024.\nand Analysis of Discrete Expressive Speech Resynthesis,‚Äù in Proc. [215] Y. Gong, A. H. Liu, H. Luo et al., ‚ÄúJoint Audio and Speech Under-\nISCAInterspeech,2023,pp.4823‚Äì4827. standing,‚ÄùinProc.IEEEASRU,2023,pp.1‚Äì8.\n[189] W.Ren,Y.-C.Lin,H.-C.Chouetal.,‚ÄúEMO-Codec:Anin-depthlook [216] Y.Chu,J.Xu,X.Zhouetal.,‚ÄúQwen-Audio:AdvancingUniversalAu-\nat emotion preservation capacity of legacy and neural codec models dioUnderstandingviaUnifiedLarge-ScaleAudio-LanguageModels,‚Äù\nwithsubjectiveandobjectiveevaluations,‚ÄùinAPSIPAASC,2024. arXivpreprintarXiv:2311.07919,2023.\n\n22\n[217] C.Tang,W.Yu,G.Sunetal.,‚ÄúSALMONN:TowardsGenericHearing [244] E. Casanova, K. Davis, E. Go¬®lge et al., ‚ÄúXTTS: A Mas-\nAbilitiesforLargeLanguageModels,‚ÄùinProc.ICLR,2024. sively Multilingual Zero-Shot Text-to-Speech Model,‚Äù arXiv preprint\n[218] S.Hu,L.Zhou,S.Liuetal.,‚ÄúWavLLM:TowardsRobustandAdaptive arXiv:2406.04904,2024.\nSpeech Large Language Model,‚Äù in Findings of EMNLP, 2024, pp. [245] M. ≈Åajszczak, G. Ca¬¥mbara, Y. Li et al., ‚ÄúBASE TTS: Lessons from\n4552‚Äì4572. BuildingaBillion-ParameterText-to-SpeechModelon100KHoursof\n[219] Z.Ma,G.Yang,Y.Yangetal.,‚ÄúAnEmbarrassinglySimpleApproach Data,‚ÄùarXivpreprintarXiv:2402.08093,2024.\nforLLMwithStrongASRCapacity,‚ÄùarXivpreprintarXiv:2402.08846, [246] S. Liao, Y. Wang, T. Li et al., ‚ÄúFish-Speech: Leveraging Large Lan-\n2024. guage Models for Advanced Multilingual Text-to-Speech Synthesis,‚Äù\n[220] Y. Bai, J. Chen, J. Chen et al., ‚ÄúSeed-ASR: Understanding Diverse arXivpreprintarXiv:2411.01156,2024.\nSpeech and Contexts with LLM-Based Speech Recognition,‚Äù arXiv [247] K. Shen, Z. Ju, X. Tan et al., ‚ÄúNaturalSpeech 2: Latent Diffusion\npreprintarXiv:2407.04675,2024. ModelsareNaturalandZero-ShotSpeechandSingingSynthesizers,‚Äù\n[221] Z.Wang,X.Zhu,Z.Zhangetal.,‚ÄúSELM:SpeechEnhancementUsing inProc.ICLR,2024.\nDiscreteTokensandLanguageModels,‚ÄùinProc.IEEEICASSP,2024, [248] Z. Zhang, L. Zhou, C. Wang et al., ‚ÄúSpeak Foreign Languages with\npp.11561‚Äì11565. YourOwnVoice:Cross-LingualNeuralCodecLanguageModelling,‚Äù\n[222] X.Liu,X.Li,J.Serra` etal.,‚ÄúJointSemanticKnowledgeDistillation arXivpreprintarXiv:2303.03926,2023.\nandMaskedAcousticModelingforFull-bandSpeechRestorationwith [249] X. Wang, M. Thakker, Z. Chen et al., ‚ÄúSpeechX: Neural Codec\nImprovedIntelligibility,‚ÄùarXivpreprintarXiv:2409.09357,2024. LanguageModelasaVersatileSpeechTransformer,‚ÄùIEEE/ACMTrans.\n[223] B. Tang, B. Zeng, and M. Li, ‚ÄúTSELM: Target Speaker Extrac- ASLP.,2024.\ntion using Discrete Tokens and Language Models,‚Äù arXiv preprint [250] S.-H. Lee, H.-Y. Choi, S.-B. Kim et al., ‚ÄúHierSpeech++: Bridging\narXiv:2409.07841,2024. the Gap Between Semantic and Acoustic Representation of Speech\n[224] C.Du,Y.Guo,X.Chenetal.,‚ÄúSpeakerAdaptiveText-to-SpeechWith byHierarchicalVariationalInferenceforZero-ShotSpeechSynthesis,‚Äù\nTimbre-Normalized Vector-Quantized Feature,‚Äù IEEE/ACM Trans. arXivpreprintarXiv:2311.12454,2023.\nASLP.,vol.31,pp.3446‚Äì3456,2023. [251] M. Le, A. Vyas, B. Shi et al., ‚ÄúVoicebox: Text-Guided Multilingual\n[225] S.Liu,Y.Guo,C.Duetal.,‚ÄúDSE-TTS:DualSpeakerEmbeddingfor UniversalSpeechGenerationatScale,‚ÄùProc.NeurIPS,vol.36,2024.\nCross-Lingual Text-to-Speech,‚Äù in Proc. ISCA Interspeech, 2023, pp. [252] Z. Liu, S. Wang, P. Zhu et al., ‚ÄúE1 TTS: Simple and fast non-\n616‚Äì620. autoregressiveTTS,‚ÄùinProc.IEEEICASSP,2025.\n[226] C.Du,Y.Guo,F.Shenetal.,‚ÄúMulti-SpeakerMulti-LingualVQTTS [253] Y.Chen,Z.Niu,Z.Maetal.,‚ÄúF5-TTS:Afairytalerthatfakesfluent\nSystemforLIMMITS2023Challenge,‚ÄùinProc.IEEEICASSP,2023. andfaithfulspeechwithflowmatching,‚ÄùinProc.ACL,Jul.2025,pp.\n[227] S.Liu,Y.Guo,X.Chenetal.,‚ÄúStoryTTS:AHighlyExpressiveText- 6255‚Äì6271.\nto-Speech Dataset with Rich Textual Expressiveness Annotations,‚Äù in\n[254] L. Meng, L. Zhou, S. Liu et al., ‚ÄúAutoregressive speech synthesis\nProc.IEEEICASSP,2024,pp.11521‚Äì11525.\nwithoutvectorquantization,‚ÄùinProc.ACL,Jul.2025,pp.1287‚Äì1300.\n[228] Y. Song, Z. Chen, X. Wang et al., ‚ÄúELLA-V: Stable Neural Codec\n[255] Z.Liu,S.Wang,S.Inoueetal.,‚ÄúAutoregressiveDiffusionTransformer\nLanguage Modelling with Alignment-Guided Sequence Reordering,‚Äù\nforText-to-SpeechSynthesis,‚ÄùarXivpreprintarXiv:2406.05551,2024.\nProc.AAAI,2025.\n[256] X. Zhu, W. Tian, and L. Xie, ‚ÄúAutoregressive Speech Synthesis with\n[229] D. Xin, X. Tan, K. Shen et al., ‚ÄúRALL-E: Robust Codec Language\nNext-DistributionPrediction,‚ÄùarXivpreprintarXiv:2412.16846,2024.\nModellingwithChain-of-ThoughtPromptingforText-to-SpeechSyn-\n[257] A. Turetzky, N. Shabtay, S. Shechtman et al., ‚ÄúContinuous\nthesis,‚ÄùarXivpreprintarXiv:2404.03204,2024.\nSpeech Synthesis Using Per-Token Latent Diffusion,‚Äù arXiv preprint\n[230] B. Han, L. Zhou, S. Liu et al., ‚ÄúVALL-E R: Robust and Efficient\narXiv:2410.16048,2024.\nZero-ShotText-to-SpeechSynthesisviaMonotonicAlignment,‚ÄùarXiv\n[258] D. Jia, Z. Chen, J. Chen, C. Du, J. Wu, J. Cong, X. Zhuang,\npreprintarXiv:2406.07855,2024.\nC.Li,Z.Wei,Y.Wang,andY.Wang,‚ÄúDiTAR:Diffusiontransformer\n[231] C. Du, Y. Guo, H. Wang et al., ‚ÄúVALL-T: Decoder-Only Generative\nautoregressivemodelingforspeechgeneration,‚ÄùinProc.ICML,2025.\nTransducerforRobustandDecoding-ControllableText-to-Speech,‚Äùin\n[259] S. Cuervo and R. Marxer, ‚ÄúScaling Properties of Speech Language\nProc.IEEEICASSP,2025.\nModels,‚ÄùinProc.EMNLP,Nov.2024,pp.351‚Äì361.\n[232] H.Wang,C.Du,Y.Guoetal.,‚ÄúAttention-ConstrainedInferenceFor\n[260] G.-T. Lin, P. G. Shivakumar, A. Gourav et al., ‚ÄúAlign-SLM: Textless\nRobust Decoder-Only Text-to-Speech,‚Äù in Proc. IEEE SLT, 2024, pp.\nspoken language models with reinforcement learning from AI feed-\n630‚Äì637.\nback,‚ÄùinProc.ACL,Jul.2025,pp.20395‚Äì20411.\n[233] S.Chen,S.Liu,L.Zhouetal.,‚ÄúVALL-E2:NeuralCodecLanguage\n[261] T.A.Nguyen,B.Muller,B.Yuetal.,‚ÄúSpiRit-LM:Interleavedspoken\nModels are Human Parity Zero-Shot Text to Speech Synthesizers,‚Äù\nandwrittenlanguagemodel,‚ÄùTrans.ACL,vol.13,pp.30‚Äì52,012025.\narXivpreprintarXiv:2406.05370,2024.\n[234] Y.Song,Z.Chen,X.Wangetal.,‚ÄúTacoLM:GaTedAttentionEquipped [262] S.J.Park,J.Salazar,A.Jansenetal.,‚ÄúLong-FormSpeechGeneration\nCodec Language Model are Efficient Zero-Shot Text to Speech Syn- with Spoken Language Models,‚Äù arXiv preprint arXiv:2412.18603,\nthesizers,‚ÄùinProc.ISCAInterspeech,2024,pp.4433‚Äì4437. 2024.\n[235] D. Kim, S. Hong, and Y.-H. Choi, ‚ÄúSC VALL-E: Style- [263] B.Veluri,B.N.Peloquin,B.Yuetal.,‚ÄúBeyondTurn-BasedInterfaces:\nControllable Zero-Shot Text to Speech Synthesizer,‚Äù arXiv preprint Synchronous LLMs as Full-Duplex Dialogue Agents,‚Äù arXiv preprint\narXiv:2307.10550,2023. arXiv:2409.15594,2024.\n[236] D. Lyth and S. King, ‚ÄúNatural Language Guidance of High- [264] X. Zhang, X. Lyu, Z. Du et al., ‚ÄúIntrinsicVoice: Empowering LLMs\nFidelity Text-to-Speech with Synthetic Annotations,‚Äù arXiv preprint with Intrinsic Real-Time Voice Interaction Abilities,‚Äù arXiv preprint\narXiv:2402.01912,2024. arXiv:2410.08035,2024.\n[237] S. Ji, J. Zuo, M. Fang et al., ‚ÄúTextrolSpeech: A Text Style Control [265] Z. Ma, Y. Song, C. Du et al., ‚ÄúLanguage Model Can Listen While\nSpeechCorpuswithCodecLanguageText-to-SpeechModels,‚ÄùinProc. Speaking,‚ÄùarXivpreprintarXiv:2408.02622,2024.\nIEEEICASSP,2024,pp.10301‚Äì10305. [266] D.Zhang,X.Zhang,J.Zhanetal.,‚ÄúSpeechGPT-Gen:ScalingChain-\n[238] H. Hao, L. Zhou, S. Liu et al., ‚ÄúBoosting large language model for of-InformationSpeechGeneration,‚ÄùarXivpreprintarXiv:2401.13527,\nspeechsynthesis:Anempiricalstudy,‚ÄùinProc.IEEEICASSP,2025. 2024.\n[239] M.Shen,S.Zhang,J.Wuetal.,‚ÄúGetlargelanguagemodelsreadyto [267] C.Fu,H.Lin,Z.Longetal.,‚ÄúVita:TowardsOpen-SourceInteractive\nspeak: A late-fusion approach for speech generation,‚Äù in Proc. IEEE OmniMultimodalLLM,‚ÄùarXivpreprintarXiv:2408.05211,2024.\nICASSP,2025. [268] Z. Xie and C. Wu, ‚ÄúMini-Omni: Language Models Can Hear, Talk\n[240] D.Yang,J.Tian,X.Tanetal.,‚ÄúUniAudio:TowardsUniversalAudio WhileThinkinginStreaming,‚ÄùarXivpreprintarXiv:2408.16725,2024.\nGenerationwithLargeLanguageModels,‚ÄùinProc.ICML,2024. [269] ‚Äî‚Äî, ‚ÄúMini-Omni2: Towards Open-Source GPT-4o with Vision,\n[241] J. Copet, F. Kreuk, I. Gat et al., ‚ÄúSimple and Controllable Music Speech and Duplex Capabilities,‚Äù arXiv preprint arXiv:2410.11190,\nGeneration,‚ÄùProc.NeurIPS,vol.36,2024. 2024.\n[242] P. Peng, P.-Y. Huang, S.-W. Li et al., ‚ÄúVoiceCraft: Zero-Shot Speech [270] W. Yu, S. Wang, X. Yang et al., ‚ÄúSALMONN-omni: A Codec-Free\nEditingandText-to-SpeechintheWild,‚ÄùinProc.ACL,Aug.2024,pp. LLM for Full-Duplex Speech Understanding and Generation,‚Äù arXiv\n12442‚Äì12462. preprintarXiv:2411.18138,2024.\n[243] Y. Yang, Z. Ma, S. Liu et al., ‚ÄúInterleaved Speech-Text Language [271] Z. Zhong, C. Wang, Y. Liu et al., ‚ÄúLyra: An Efficient and\nModels are Simple Streaming Text to Speech Synthesizers,‚Äù arXiv Speech-Centric Framework for Omni-Cognition,‚Äù arXiv preprint\npreprintarXiv:2412.16102,2024. arXiv:2412.09501,2024.\n\n23\n[272] W. Chen, Z. Ma, R. Yan et al., ‚ÄúSLAM-Omni: Timbre-controllable\nvoiceinteractionsystemwithsingle-stagetraining,‚ÄùinFindingsofACL.\nAssociationforComputationalLinguistics,Jul.2025,pp.2262‚Äì2282.\n[273] K.Chen,Y.Gou,R.Huangetal.,‚ÄúEMOVA:EmpoweringLanguage\nModelstoSee,HearandSpeakwithVividEmotions,‚ÄùarXivpreprint\narXiv:2409.18042,2024.\n[274] X.Wang,Y.Li,C.Fuetal.,‚ÄúFreeze-omni:aSmartandLowLatency\nSpeech-To-SpeechDialogueModelwithFrozenLLM,‚ÄùarXivpreprint\narXiv:2411.00774,2024.\n[275] P.Zhang,X.Dong,Y.Caoetal.,‚ÄúInternLM-XComposer2.5-OmniLive:\nAComprehensiveMultimodalSystemforLong-TermStreamingVideo\nandAudioInteractions,‚ÄùarXivpreprintarXiv:2412.09596,2024.\n[276] C.Fu,H.Lin,X.Wangetal.,‚ÄúVITA-1.5:TowardsGPT-4oLevelReal-\nTimeVisionandSpeechInteraction,‚ÄùarXivpreprintarXiv:2501.01957,\n2025.\n[277] R.Luo,T.-E.Lin,H.Zhangetal.,‚ÄúOpenOmni:Largelanguagemodels\npivot zero-shot omnimodal alignment across language with real-time\nself-awareemotionalspeechsynthesis,‚ÄùinProc.NeurIPS,2025.\n[278] Q. Chen, Y. Chen, Y. Chen et al., ‚ÄúMinMo: A Multimodal Large\nLanguage Model for Seamless Voice Interaction,‚Äù arXiv preprint\narXiv:2501.06282,2025.\n[279] Y. Wang, D. Chen, X. Zhang et al., ‚ÄúTaDiCodec: Text-aware diffu-\nsion speech tokenizer for speech language modeling,‚Äù arXiv preprint\narXiv:2508.16790,2025.\n[280] Y. Zheng, W. Tu, L. Xiao et al., ‚ÄúSRCodec: Split-Residual Vector\nQuantizationforNeuralSpeechCodec,‚ÄùinProc.IEEEICASSP,2024,\npp.451‚Äì455.\n[281] E.Casanova,R.Langman,P.Neekharaetal.,‚ÄúLowFrame-RateSpeech\nCodec:aCodecDesignedforFastHigh-qualitySpeechLLMTraining\nandInference,‚ÄùarXivpreprintarXiv:2409.12117,2024.\n[282] L. Della Libera, F. Paissan, C. Subakan, and M. Ravanelli, ‚ÄúFocal-\nCodec: Low-bitrate speech coding via focal modulation networks,‚Äù\narXivpreprintarXiv:2502.04465,2025.\n[283] D. Yang, S. Liu, H. Guo et al., ‚ÄúALMTokenizer: A low-bitrate and\nsemantic-richaudiocodectokenizerforaudiolanguagemodeling,‚Äùin\nProc.ICML,2025.\n[284] J. Li, X. Lin, Z. Li et al., ‚ÄúDualCodec: A Low-Frame-Rate,\nSemantically-Enhanced Neural Audio Codec for Speech Generation,‚Äù\ninProc.ISCAInterspeech,2025,pp.4883‚Äì4887.\n[285] Y. Pan, L. Ma, and J. Zhao, ‚ÄúPromptCodec: High-Fidelity Neu-\nral Speech Codec using Disentangled Representation Learning\nbased Adaptive Feature-aware Prompt Encoders,‚Äù arXiv preprint\narXiv:2404.02702,2024.\n[286] Y.-A.Chung,Y.Zhang,W.Hanetal.,‚ÄúW2v-BERT:CombiningCon-\ntrastiveLearningandMaskedLanguageModellingforSelf-Supervised\nSpeechPre-Training,‚ÄùinProc.IEEEASRU,2021,pp.244‚Äì250.\n[287] A.H.Liu,H.-J.Chang,M.Aulietal.,‚ÄúDinoSR:Self-Distillationand\nOnline Clustering for Self-Supervised Speech Representation Learn-\ning,‚ÄùProc.NeurIPS,vol.36,2024.\n[288] M.Baas,B.vanNiekerk,andH.Kamper,‚ÄúVoiceConversionWithJust\nNearestNeighbors,‚ÄùinProc.ISCAInterspeech,2023,pp.2053‚Äì2057.\n[289] A. Graves, S. Ferna¬¥ndez, F. Gomez et al., ‚ÄúConnectionist Temporal\nClassification:LabellingUnsegmentedSequenceDatawithRecurrent\nNeuralNetworks,‚ÄùinProc.ICML,2006,p.369‚Äì376.\n[290] I.LoshchilovandF.Hutter,‚ÄúDecoupledweightdecayregularization,‚Äù\ninProc.ICLR,2019.\n[291] D.P.KingmaandJ.Ba,‚ÄúAdam:Amethodforstochasticoptimization,‚Äù\ninProc.ICLR,2015.\n\n24\nAPPENDIXI. LISTOFWIDELY-USEDDISCRETESPEECH ‚Ä¢ For WER, we transcribe the synthesized speech using\nTOKENS NeMo-ASR9, normalize the text using Whisper normal-\nizer10, and use jiwer11 to measure the total WER.\nTo give a high-level overview of existing discrete speech ‚Ä¢ For GPE and P.Corr, we use the YIN algorithm in\ntokens,wesummarizethewidely-useddiscreteacoustictokens PyWorld12 to extract pitch contours. Pitch errors and\nin Table II, and semantic tokens in Table III. correlation are only computed on frames where both the\nreference and the synthesized waveforms are voiced.\n‚Ä¢ For PESQ, we compute wide-band PESQ using this\ntool13. For STOI, we use this tool14.\nAPPENDIXII. EXPERIMENTALDETAILS\n‚Ä¢ For SECS, we use Resemblyzer15 to extract embeddings\nfor both the reference waveform and the synthesized\nA. Vocoder for Reconstruction and Voice Conversion\nwaveform. Then, SECS is computed as the cosine simi-\nlarity between the two embeddings.\nIn the experiments of reconstruction and voice conver-\nsion, we train a speech token vocoder for each of the The test metadata for reconstruction and voice conversion\nsemantic tokens. This speech token vocoder is chosen as can be found here16 and here17, respectively.\nCTX-vec2wavŒ± [29] as the improved version of CTX-\nvec2wav [108]. This vocoder is a timbre-controllable speech\nB. Downstream Semantic Modeling\ntoken vocoder. The input speech tokens are converted to\ntheir corresponding code-vectors first, where code-vectors 1) Motivation: Giventhatalargebranchofdiscretespeech\nare concatenated along channel dimension when there are tokens is semantic tokens, it is also important to measure the\nmultiple codebooks. Then, these code-vectors are passed to semantic modeling abilities of speech tokens in downstream\na Conformer [73] frontend before a HifiGAN [153] generator tasks, without relying on a codec decoder or vocoder to\nmodule. The timbre information is provided to the vocoder reconstruct into waveforms. There are mainly two approaches\nmodel from a reference waveform. This reference is passed to to probe the semantic modeling abilities in discrete speech\naWavLM-Large[35]modeltoextractstrongtimbre-dependent tokens:\nembeddings. These embeddings are extracted from the 6-th\n‚Ä¢ Index-based: Like DASB [147], discrete indices are di-\nlayer of this WavLM model, following previous work [288].\nrectly used as inputs and passed to a set of learnable\nThen, they are fed to the Conformer frontend via position-\nembedding lookup modules before subsequent networks.\nagnostic cross-attention mechanism [108].\nWhentherearemultiplecodestreams(forexample,when\nThe training of these CTX-vec2wavŒ± vocoders follows the using GVQ or RVQ), a learnable attention weight will\nGAN-based vocoder paradigm where a set of discriminators aggregate the embeddings from different code streams.\nare employed. We follow the detailed training setup in this ‚Ä¢ Vector-based: The code-vectors corresponding to the dis-\nrepository8. The generator of this vocoder consists of ‚âà32M crete indices are considered as inputs to the probing\nparameters.Wetrainthevocoderforeachtokenupto1million network. This is a direct extension to the SUPERB [114]\nstepson4GPUs,withadynamicbatchsizeof‚âà36sofspeech benchmark that mainly considers continuous speech rep-\nper GPU. The training data is all LibriTTS training splits, resentations.\nwhich amount to about 585 hours.\nThe vector-based approach, in other words, probes the infor-\nAfter obtaining a vocoder for each semantic token, we can\nmation from the established latent space where speech tok-\nperform reconstruction comparisons on all tokens, and voice\nenizers perform quantization on. Meanwhile, the index-based\nconversioncomparisonsonsemantictokensandsomeacoustic\napproach concentrates on the abstract discrete indices and\ntokens.Forreconstruction,weprovidetheoriginalutteranceas\nanticipates that indices themselves carry important semantic\nthesourceoftimbreinformation.Onemayarguethatthissetup\ninformationratherthantheunderlyinghigh-dimensionalspace.\ncarriesariskofinformationleakage,sincecontentinformation\nSince the most of the discrete token-based speech generation\nmay also be implicitly encoded in timbre representations. An\nmodels use token indices instead of code-vectors as modeling\nalternative strategy is to use another reference prompt from\ntargets,weopttousetheindex-based approachinthissurvey.\nthe same speaker to supply timbre information. However,\nTo better align with the results in previous works, we use\nthis essentially becomes a ‚Äúsame-speaker conversion‚Äù task the DASB [147] framework and its official implementation18.\nrather than reconstruction, and the resulting metrics are not\nstrictly comparable to those of ordinary acoustic tokens that\n9https://huggingface.co/nvidia/stt en fastconformer transducer large\ndo not require additional speaker inputs. Given that the CTX- 10https://github.com/openai/whisper/tree/main/whisper/normalizers\nvec2wavŒ± vocoderhasneverbeentrainedoncaseswheretim- 11https://github.com/jitsi/jiwer\nbrerepresentationsandcontenttokensoriginatefromthesame\n12https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder\n13https://github.com/ludlows/PESQ\nsegment, we consider this strategy to still provide sufficiently\n14https://github.com/mpariente/pystoi\nfair comparisons. 15https://github.com/resemble-ai/Resemblyzer\nThe details in evaluation metrics include: 16https://cpdu.github.io/unicats/resources/testsetB utt2prompt\n17https://cantabile-kwok.github.io/LSCodec/audio ready/VC/utt2prompt\n18https://github.com/speechbrain/benchmarks/tree/DASB/benchmarks/\n8https://github.com/cantabile-kwok/vec2wav2.0 DASB\n\n25\nTABLE II: A summary of widely-used acoustic speech tokens (neural speech codecs). Note that some codecs in this list are\nalso applicable to general audio. Italic ‚ÄúC,T,U‚Äù denote CNN, Transformer or U-Net-based generator architecture. Symbols ‚Äò/‚Äô\nand ‚Äò-‚Äô denote ‚Äúor‚Äù and ‚Äúto‚Äù for different model versions, ‚Äú+‚Äù means different configurations in different VQ streams in a\nsingle model, and ‚Äú‚âà‚Äù means the average value. Q,F,V mean number of quantizers, frame rate and vocabulary size of each\nquantizer respectively. For example, ‚ÄúQ = 2, V=8192+(4096-32768)‚Äù in SemantiCodec means one of the two VQ streams\nhas 8192 possible codes, and the other can vary from 4096 to 32768 in different configurations. For FSQ, V is presented\nas Ld where L is the quantization levels for each dimension, and d is the number of dimensions. When L is different for\neach dimension, like LFSC, we use L √óL √ó¬∑¬∑¬∑ to represent each dimension. ‚ÄúDynamic‚Äù means the number for each token\n1 2\nframe is variable, like in variable-bitrate and variable frame rate setups. Bitrates are computed by 1 (cid:80)Q F ‚åàlog V ‚åâ kbps,\n1000 i=1 i 2 i\nwithout entropy coding.\nSampling Quantization\nAcousticSpeechTokens ModelFramework Q F (Hz) V Bitrate(kbps)\nRate(kHz) Method\nGeneral-purposeacoustictokens\nSoundStream[69] VQ-GAN(C) 24 RVQ max24 75 1024 max18.00\nEnCodec[70] VQ-GAN(C) 24 RVQ max32 75 1024 max24.00\nTF-Codec[55] VQ-GAN(C) 16 GVQ 3-32 25 512/1024 0.68-8.00\nDisen-TF-Codec[21] VQ-GAN(C) 16 GVQ 2/6 25 256/1024 0.40/1.50\nAudioDec[76] VQ-VAE(C)+GAN 48 RVQ 8 160 1024 12.80\nHiFi-Codec[59] VQ-GAN(C) 16/24 GRVQ 4 50-100 1024 2.00-4.00\nDAC[19] VQ-GAN(C) 44.1 RVQ 9 86 1024 7.74\nLaDiffCodec[17] Diffusion 16 RVQ 3/6 50 1024 1.50/3.00\nFreqCodec[71] VQ-GAN(C) 16 RVQ max32 50 1024 max16.00\nTiCodec[22] VQ-GAN(C) 24 RVQ,GVQ 1-4 75 1024 0.75-3.00\nAPCodec[82] VQ-GAN(C) 48 RVQ 4 150 1024 6.00\nSRCodec[280] VQ-GAN(C) 16 GRVQ 2-8 50 512+1024 0.95-3.80\nSQ-Codec[89] VQ-GAN(C) 16 FSQ 1 50 1932 8.00\nSingle-Codec[72] VQ-GAN(T+C) 24 VQ 1 23 8192 0.30\nESC[75] VQ-GAN(U) 16 GVQ max18 50 1024 max9.00\nCoFi-Codec[74] VQ-GAN(U) 16 GVQ 3 8.33+25+50 16384 1.17\nHILCodec[84] VQ-GAN(C) 24 RVQ 2-12 75 1024 1.50-9.00\nSuperCodec[85] VQ-GAN(C) 16 RVQ 2-12 50 1024 1.00-6.00\nSNAC[23] VQ-GAN(C) 24 RVQ 3 12+23+47 4096 0.98\nWavTokenizer[90] VQ-GAN(C) 24 VQ 1 40/75 4096 0.48/0.90\nBigCodec[91] VQ-GAN(C) 16 VQ 1 80 8192 1.04\nLFSC[281] VQ-GAN(C) 22.05 FSQ 8 21.5 8√ó7√ó6√ó6 1.89\nNDVQ[88] VQ-GAN(C) 24 RVQ max32 75 1024 max24.00\nVRVQ[24] VQ-GAN(C) 44.1 RVQ dynamic,max.8 86 1024 0.26+max6.89\nTS3-Codec[20] VQ-GAN(T) 16 VQ 1 40/50 65536/131072 0.64-0.85\nStable-Codec[60] VQ-GAN(T) 16 FSQ 1/2 25 56/66 0.40/0.70\nFreeCodec[94] VQ-GAN(C+T) 16 VQ 1+1 50+7 256 0.45\nFocalCodec[282] VQ-GAN(T+C) 16 FSQ 1 12.5/25/50 213 0.16/0.33/0.65\nALMTokenizer[283] VQ-GAN(T) 24 RVQ 3 12.5 2048 0.41\nTFC[172] VQ-GAN(C) 24 RVQ 8 dynamic,18.75-75 1024 1.50-6.00\nCodecSlime[173] VQ-GAN(C) 16 FSQ 1 dynamic,‚âà40 52√ó36 0.08+(‚âà0.60)\nMixed-objectiveacoustictokens:semanticdistillation\nSiahkoohietal.[100] VQ-GAN(C) 16 RVQ 2+1/2+2/6 25+50 64 0.60/0.90/1.80\nSpeechTokenizer[13] VQ-GAN(C) 16 RVQ 8 50 1024 4.00\nSemantiCodec[18] Diffusion 16 VQ 2 12.5-50 8192+(4096-32768) 0.31-1.40\nLLM-Codec[25] VQ-GAN(C) 16 RVQ 3 8.33+16.67+33.33 3248+32000+32000 0.85\nX-Codec[26] VQ-GAN(C) 16 RVQ max8 50 1024 max4.00\nSoCodec[92] VQ-GAN(C) 16 GVQ 1/4/8 25/8.3/4.2 16384 0.35/0.47\nMimi[14] VQ-GAN(C+T) 24 RVQ 8 12.5 2048 1.10\nX-Codec2.0[102] VQ-GAN(C+T) 16 FSQ 1 50 48 0.80\nBiCodec[93] VQ-GAN(C) 16 VQ 1 50 8192 0.65\nDualCodec[284] VQ-GAN(C) 24 RVQ 3/6 12.5/25 (1024/16384)+(1024/4096) 0.75-0.93\nXY-Tokenizer[104] VQ-GAN(T) 16 RVQ 8 12.5 1024 1.00\nTaDiCodec[279] Diffusion 24 FSQ 1 6.25 214 0.0875\nMixed-objectiveacoustictokens:disentanglement\nSSVC[27] VQ-GAN(C) 24 RVQ 4 50 512 1.80\nPromptCodec[285] VQ-GAN(C) 24 GRVQ 1-4 75 1024 0.75-3.00\nFACodec[28] VQ-GAN(C) 16 RVQ 1+2+3 80 1024 4.80\nLSCodec[29] VQ-VAE(C+T)+GAN 24 VQ 1 25/50 1024/300 0.25/0.45\nSD-Codec[30] VQ-GAN(C) 16 RVQ 12 50 1024 6.00\nDeCodec[31] VQ-GAN(C) 16 RVQ 8+8 50 1024 8.00\n\n26\nTABLE III: A high-level summary of widely-used semantic speech tokens. Notations follow Table.II. Symbol ‚Äò/‚Äô denotes\ndifferentversions.‚ÄúInnerQuantizer‚Äùreferstowhetherthemodelhasaquantizer,orexternalquantization(e.g.clustering)must\nbe performed. F denotes frame rate. In case there are inner quantizers, Q,V denote number of quantizers and vocabulary size\nfor each quantizer, respectively. ‚ÄúNR.‚Äù means not reported.\nSemanticSpeechTokens Criterion/Objective TrainingData(h) F (Hz) InnerQuantizer\nFromself-supervisedlearning(SSL)models\nvq-wav2vec[32] Contrastive 0.96k 100 GVQ,Q=2,V =320\nwav2vec2.0[33] Contrastive 60k 50 GVQ,Q=2,V =320\nXLSR-53[136] Contrastive 50k 50 GVQ,Q=2,V =320\nHuBERT[34] Predictive 60k 50 No\nWavLM[35] Predictive 94k 50 No\nBEST-RQ[134] Predictive 60k 25 No\nw2v-BERT[286] Predictive+Contrastive 60k 50 VQ,Q=1,V =1024\nw2v-BERT2.0[103] Predictive+Contrastive 4500k 50 GVQ,Q=2,V =320\nDinoSR[287] Predictive 0.96k 50 VQ,Q=8,V =256\nNEST-RQ[139] Predictive 300k 25 No\nLAST[138] Predictive NR. 50 VQ,Q=1,V =500\nGatetal.[140] NoiseInvariance 0.10k 50 VQ,G=1,V =50-500\nContentVec[36] SpeakerInvariance 0.96k 50 No\nSPIRAL[142] NoiseInvariance 60k 12.5Hz No\nCCC-wav2vec2.0[141] NoiseInvariance 0.36k 50 GVQ,G=2,V =320\nSpin[143] SpeakerInvariance 0.10k 50 VQ,Q=1,V =128-2048\nNAST[37] NoiseInvariance 0.96k 50 VQ,Q=1,V =50-200\nDC-Spin[144] SpeakerInvariance 0.96k 50 VQ,Q=2,V =(50-500)+4096\nSupervisedsemantictokens\nS3 Tokenizer[39] SupervisedASR 172k 25/50 VQ,Q=1,V =4096\nZengetal.[148] SupervisedASR 90k 12.5 VQ,Q=1,Q=16384\nDuetal.(CosyVoice2)[150] SupervisedASR 200k 12.5 FSQ,Q=1,V =38\nDuetal.(CosyVoice3)[151] SupervisedMulti-Task 530k 25 FSQ,DetailsNR.\n2) TasksandDatasets: Weconsidertwoofthemostrepre- around 43M parameters.\nsentative speech semantic tasks: automatic speech recognition It is important to note that some speech tokens have a low\n(ASR) and intent classification (IC). In ASR, we use the framerate,suchas12.5HzforMimi[14].Asthecharacterrate\ncharacters as text units for simplicity, and the connectionist is about 14Hz, the original low frame rates make these tokens\ntemporal classification (CTC) [289] criterion for training. We not suitable for character-based CTC ASR. Hence, when the\nuseallthe960hoursinLibriSpeech[195]asthetrainingdata, tokenframerateislessthan50Hz,wealwaysrepeattheinput\ndev-clean split as the validation set, and test-clean as the test tokens to at least 50Hz for ASR task. For the IC task, no\nset. IC is the process of understanding the underlying goal or repeating is performed.\npurposeofauser‚Äôsspokenutterance.WeuseSLURP[194]for 4) Training and Decoding: For the ASR task, we use\nthe IC task, which includes 18 classification categories, such AdamW [290] optimizer with initial learning rate 10‚àí4 and\naseventname,date,etc.Weusethetrain-realsplitfortraining weight decay 5√ó10‚àí4. For the IC task, we use Adam [291]\nand the official dev and test sets, which contains about 40, 7 optimizerwithinitiallearningrate2√ó10‚àí4.Forbothtasks,the\nand 10 hours of speech data, respectively. learning rate is reduced to 80% of its original value whenever\nAll the original waveforms are in 16kHz. If a speech tok- the improvement in validation loss after an epoch is less\nenizerrequiresinputatadifferentsamplingrate,weresample than 0.25% relative to the previous epoch. For each token,\nthe waveform before feeding them accordingly. training is conducted for 20 epochs on a single GPU, and\nthe checkpoint with the lowest validation loss is selected for\n3) Model: FollowingDASB,weuseLSTM-basednetworks\ntesting.\nfor both tasks. For each token, the input layer consists of Q\nIn ASR task, we use beam search decoding with beam\nembedding lookup tables of shape V √ó 1024, where Q is\nsize100.Otherhyperparametersallremainconsistentwiththe\nthe number of codebooks and V is the vocabulary size of\nDASBofficialconfiguration.Forsometokens,trainingsuffers\neach codebook. For each frame, the Q embeddings are fed\nfromcollapse,leadingtorepetitiveandmeaninglesscharacters\ninto an multi-layer perceptron (MLP) with a hidden layer of\nin decoding. This is usually observed when the token has a\n1024dimensions,toproducesoftmaxprobabilitiesthatweight-\nlarge vocabulary size (like 2 groups of 15625 FSQ indices\nsum the Q embeddings. Then, an bidirectional LSTM with 2\nfor StableCodec [60], and 2 groups of 32000 VQ indices for\nlayersand1024hiddenneuronstakestheembeddingsequence\nLLM-Codec[25]).FollowingDASB,wedenotethesecasesas\nas input, and produces 2048-dimensional outputs. For the\n‚Äúnot converged‚Äù (NC), since the metrics are not meaningful\nASR task, another linear layer is applied to produce emission\nthere.\nprobabilities on the 31 text units (26 English characters and\nspecial tokens). For the IC task, mean pooling over the\nsequenceaxisisappliedbeforealinearprojectiontothelogits\non18outputclasses.Besidestheinputembeddinglayerwhich\nisdependentonthespecifictokenconfiguration,themodelhas\nmcp: sandbox-mcp starting\nmcp: sequential-thinking starting\nmcp: MCP_DOCKER starting\nmcp: kit starting\nmcp: context7 starting\nmcp: memory starting\nmcp: microsoft_docs starting\nmcp: sandbox-mcp ready\n2026-01-12T13:10:05.382622Z ERROR rmcp::transport::streamable_http_client: fail to get common stream: Unexpected content type: None\n2026-01-12T13:10:05.493705Z  WARN rmcp::transport::streamable_http_client: sse client event stream terminated with error: Err(UnexpectedContentType(None))\nmcp: microsoft_docs ready\nmcp: sequential-thinking ready\nmcp: memory ready\nmcp: context7 ready\nmcp: kit ready\nmcp: MCP_DOCKER failed: MCP client for `MCP_DOCKER` failed to start: MCP startup failed: handshaking with MCP server failed: connection closed: initialize response\nmcp startup: ready: sandbox-mcp, microsoft_docs, sequential-thinking, memory, context7, kit; failed: MCP_DOCKER\n2026-01-12T13:10:16.002484Z ERROR codex_api::endpoint::responses: error=http 400 Bad Request: Some(\"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Invalid schema for response_format 'codex_output_schema': In context=(), 'required' is required to be supplied and to be an array including every key in properties. Extra required key 'search_terms' supplied.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": \\\"text.format.schema\\\",\\n    \\\"code\\\": \\\"invalid_json_schema\\\"\\n  }\\n}\")\nERROR: {\n  \"error\": {\n    \"message\": \"Invalid schema for response_format 'codex_output_schema': In context=(), 'required' is required to be supplied and to be an array including every key in properties. Extra required key 'search_terms' supplied.\",\n    \"type\": \"invalid_request_error\",\n    \"param\": \"text.format.schema\",\n    \"code\": \"invalid_json_schema\"\n  }\n}"
  ]
}